{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# H&M Data Preprocessing and Cleaning with Polars\n",
    "\n",
    "This notebook provides a data preprocessing and cleaning pipeline for the H&M dataset using Polars for high-performance data processing. The pipeline includes duplicate removal, missing value imputation, outlier handling, and data validation.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The preprocessing pipeline handles:\n",
    "\n",
    "- **Duplicate Removal**: Identifying and removing duplicate records efficiently\n",
    "- **Missing Value Imputation**: Filling missing values using statistical methods\n",
    "- **Outlier Handling**: Detecting and capping outliers using IQR method\n",
    "- **Data Validation**: Ensuring data integrity after cleaning\n",
    "- **Performance Optimisation**: Leveraging Polars' speed and memory efficiency\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Setup\n",
    "\n",
    "Import Polars and other necessary libraries for data processing, cleaning, and validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import time\n",
    "from typing import Tuple, List, Dict, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure Polars for optimal performance\n",
    "try:\n",
    "    pl.Config.set_streaming_chunk_size(10000)\n",
    "    # Some Polars versions may not have all config options\n",
    "    if hasattr(pl.Config, 'set_table_width'):\n",
    "        pl.Config.set_table_width(120)\n",
    "except AttributeError:\n",
    "    # Gracefully handle missing config options in older versions\n",
    "    pass\n",
    "\n",
    "print(f\"Polars version: {pl.__version__}\")\n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Integrated Dataset\n",
    "\n",
    "Load the integrated dataset from the EDA phase. This assumes you have already run the data exploration notebook and have an integrated dataset available.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load integrated dataset (assumes previous EDA notebook has been run)\n",
    "# Configuration parameters\n",
    "data_dir = '../data'  # Path relative to the notebook's location in notebooks/\n",
    "use_saved_parquet = True  # Prefer saved Parquet files for speed\n",
    "\n",
    "# Option 1: Load from saved Parquet file (recommended and fastest)\n",
    "integrated_path = os.path.join(data_dir, 'processed', 'hm_integrated_dataset.parquet')\n",
    "\n",
    "print(\"Loading integrated dataset...\")\n",
    "start_time = time.time()\n",
    "\n",
    "if use_saved_parquet and os.path.exists(integrated_path):\n",
    "    print(\"✓ Found saved integrated dataset - loading from Parquet...\")\n",
    "    df_integrated = pl.read_parquet(integrated_path)\n",
    "    load_time = time.time() - start_time\n",
    "    print(f\"✓ Loaded {df_integrated.height:,} records in {load_time:.2f} seconds\")\n",
    "    print(f\"✓ Memory usage: {df_integrated.estimated_size('mb'):.1f} MB\")\n",
    "else:\n",
    "    print(\"Saved integrated dataset not found. Creating from raw files...\")\n",
    "    \n",
    "    # Set up individual file paths\n",
    "    transactions_path = os.path.join(data_dir, 'raw', 'transactions_train.csv')\n",
    "    customers_path = os.path.join(data_dir, 'raw', 'customers.csv')\n",
    "    articles_path = os.path.join(data_dir, 'raw', 'articles.csv')\n",
    "    \n",
    "    # Verify all files exist\n",
    "    print(f\"Path verification:\")\n",
    "    print(f\"• Transactions file exists: {os.path.exists(transactions_path)}\")\n",
    "    print(f\"• Customers file exists: {os.path.exists(customers_path)}\")\n",
    "    print(f\"• Articles file exists: {os.path.exists(articles_path)}\")\n",
    "    \n",
    "    if not all(os.path.exists(p) for p in [transactions_path, customers_path, articles_path]):\n",
    "        raise FileNotFoundError(\"Required data files not found. Please check file paths.\")\n",
    "    \n",
    "    # Load individual datasets\n",
    "    print(\"Loading individual datasets...\")\n",
    "    \n",
    "    # Load with sample for memory efficiency\n",
    "    sample_fraction = 0.1\n",
    "    print(f\"Using {sample_fraction*100:.0f}% sample for processing...\")\n",
    "    \n",
    "    df_transactions = pl.read_csv(transactions_path).sample(fraction=sample_fraction, seed=42)\n",
    "    df_customers = pl.read_csv(customers_path)\n",
    "    df_articles = pl.read_csv(articles_path)\n",
    "    \n",
    "    print(f\"✓ Loaded transactions: {df_transactions.height:,} records\")\n",
    "    print(f\"✓ Loaded customers: {df_customers.height:,} records\")\n",
    "    print(f\"✓ Loaded articles: {df_articles.height:,} records\")\n",
    "    \n",
    "    # Create integrated dataset\n",
    "    print(\"Creating integrated dataset...\")\n",
    "    df_integrated = (\n",
    "        df_transactions\n",
    "        .join(df_customers, on=\"customer_id\", how=\"left\")\n",
    "        .join(df_articles, on=\"article_id\", how=\"left\")\n",
    "    )\n",
    "    \n",
    "    load_time = time.time() - start_time\n",
    "    print(f\"✓ Created integrated dataset with {df_integrated.height:,} records\")\n",
    "    print(f\"✓ Integration completed in {load_time:.2f} seconds\")\n",
    "    \n",
    "    # Optionally save for future use\n",
    "    os.makedirs(os.path.join(data_dir, 'processed'), exist_ok=True)\n",
    "    df_integrated.write_parquet(integrated_path)\n",
    "    print(f\"✓ Saved integrated dataset for future use\")\n",
    "\n",
    "print(f\"\\nDataset ready for preprocessing:\")\n",
    "print(f\"• Records: {df_integrated.height:,}\")\n",
    "print(f\"• Columns: {len(df_integrated.columns)}\")\n",
    "print(f\"• Memory usage: {df_integrated.estimated_size('mb'):.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Quality Assessment\n",
    "\n",
    "Assess the current data quality using Polars' efficient operations to understand issues that need to be addressed in preprocessing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Analysing existing data quality issues in H&M dataset...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Efficient quality assessment with Polars\n",
    "print(f\"\\n• Missing Values Analysis:\")\n",
    "\n",
    "# Get null counts for all columns in one operation\n",
    "null_counts = df_integrated.null_count()\n",
    "total_records = df_integrated.height\n",
    "\n",
    "missing_summary = []\n",
    "for col_name in df_integrated.columns:\n",
    "    missing_count = null_counts[col_name][0]\n",
    "    if missing_count > 0:\n",
    "        missing_percentage = (missing_count / total_records) * 100\n",
    "        missing_summary.append((col_name, missing_count, missing_percentage))\n",
    "        print(f\"  {col_name}: {missing_count:,} ({missing_percentage:.2f}%)\")\n",
    "\n",
    "if not missing_summary:\n",
    "    print(\"  ✓ No missing values found in dataset\")\n",
    "\n",
    "# Analyse duplicates efficiently\n",
    "print(f\"\\n• Duplicate Analysis:\")\n",
    "unique_records = df_integrated.unique().height\n",
    "duplicate_count = total_records - unique_records\n",
    "print(f\"  Total records: {total_records:,}\")\n",
    "print(f\"  Unique records: {unique_records:,}\")\n",
    "print(f\"  Duplicate records: {duplicate_count:,}\")\n",
    "\n",
    "# Price distribution analysis\n",
    "if 'price' in df_integrated.columns:\n",
    "    print(f\"\\n• Price Distribution Analysis:\")\n",
    "    price_stats = df_integrated.select([\n",
    "        pl.col('price').count().alias('count'),\n",
    "        pl.col('price').mean().alias('mean'),\n",
    "        pl.col('price').std().alias('std'),\n",
    "        pl.col('price').min().alias('min'),\n",
    "        pl.col('price').quantile(0.25).alias('25%'),\n",
    "        pl.col('price').quantile(0.5).alias('50%'),\n",
    "        pl.col('price').quantile(0.75).alias('75%'),\n",
    "        pl.col('price').max().alias('max')\n",
    "    ]).to_pandas().iloc[0]\n",
    "    \n",
    "    for stat_name, value in price_stats.items():\n",
    "        print(f\"  {stat_name}: {value:.2f}\")\n",
    "else:\n",
    "    print(f\"\\n• No price column found - skipping price analysis\")\n",
    "\n",
    "assessment_time = time.time() - start_time\n",
    "print(f\"\\n✓ Quality assessment completed in {assessment_time:.2f} seconds\")\n",
    "\n",
    "# Store results for use in processing steps\n",
    "quality_report = {\n",
    "    'missing_summary': missing_summary,\n",
    "    'duplicate_count': duplicate_count,\n",
    "    'total_records': total_records\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Duplicate Removal\n",
    "\n",
    "Remove duplicate records efficiently using Polars' optimised duplicate removal to ensure data integrity and prevent bias in analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Starting data preprocessing and cleaning pipeline on {total_records:,} records...\")\n",
    "\n",
    "# Step 1: Remove duplicates if any exist\n",
    "if duplicate_count > 0:\n",
    "    print(f\"\\n• Removing Duplicates:\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    df_no_duplicates = df_integrated.unique()\n",
    "    final_count = df_no_duplicates.height\n",
    "    \n",
    "    dedup_time = time.time() - start_time\n",
    "    print(f\"  - Removed {duplicate_count:,} duplicate records in {dedup_time:.2f} seconds\")\n",
    "    print(f\"  - Remaining records: {final_count:,}\")\n",
    "    print(f\"  - Memory saved: {(df_integrated.estimated_size('mb') - df_no_duplicates.estimated_size('mb')):.1f} MB\")\n",
    "else:\n",
    "    df_no_duplicates = df_integrated\n",
    "    print(f\"\\n• No duplicates found - proceeding with original dataset for preprocessing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Missing Value Imputation\n",
    "\n",
    "Handle missing values using Polars' efficient statistical functions:\n",
    "\n",
    "- **Numerical columns**: Median imputation (robust to outliers)\n",
    "- **Categorical columns**: Mode imputation or default values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Handle missing values if any exist\n",
    "if missing_summary:\n",
    "    print(f\"\\n• Handling Missing Values:\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Identify column types efficiently\n",
    "    schema_info = df_no_duplicates.schema\n",
    "    \n",
    "    # Handle numerical columns with median imputation\n",
    "    numerical_cols = [col_name for col_name, _, _ in missing_summary \n",
    "                     if schema_info[col_name] in [pl.Int64, pl.Int32, pl.Float64, pl.Float32, pl.UInt32, pl.UInt64]]\n",
    "    \n",
    "    if numerical_cols:\n",
    "        print(f\"  Processing numerical columns: {numerical_cols}\")\n",
    "        \n",
    "        # Calculate medians for all numerical columns at once\n",
    "        median_exprs = [pl.col(col).median().alias(f\"{col}_median\") for col in numerical_cols]\n",
    "        median_values = df_no_duplicates.select(median_exprs).to_pandas().iloc[0] if median_exprs else {}\n",
    "        \n",
    "        # Fill missing values with medians\n",
    "        for col_name in numerical_cols:\n",
    "            median_val = median_values.get(f\"{col_name}_median\", 0)\n",
    "            df_no_duplicates = df_no_duplicates.with_columns(\n",
    "                pl.col(col_name).fill_null(median_val)\n",
    "            )\n",
    "            print(f\"    - {col_name}: filled with median value {median_val:.2f}\")\n",
    "    \n",
    "    # Handle categorical columns with mode imputation\n",
    "    categorical_cols = [col_name for col_name, _, _ in missing_summary \n",
    "                       if schema_info[col_name] in [pl.Utf8, pl.String]]\n",
    "    \n",
    "    if categorical_cols:\n",
    "        print(f\"  Processing categorical columns: {categorical_cols}\")\n",
    "        \n",
    "        for col_name in categorical_cols:\n",
    "            try:\n",
    "                # Get mode (most frequent value) efficiently\n",
    "                mode_result = (\n",
    "                    df_no_duplicates\n",
    "                    .select(col_name)\n",
    "                    .filter(pl.col(col_name).is_not_null())\n",
    "                    .group_by(col_name)\n",
    "                    .count()\n",
    "                    .sort('count', descending=True)\n",
    "                    .limit(1)\n",
    "                )\n",
    "                \n",
    "                if mode_result.height > 0:\n",
    "                    mode_val = mode_result[col_name][0]\n",
    "                    df_no_duplicates = df_no_duplicates.with_columns(\n",
    "                        pl.col(col_name).fill_null(mode_val)\n",
    "                    )\n",
    "                    print(f\"    - {col_name}: filled with mode value '{mode_val}'\")\n",
    "                else:\n",
    "                    # Use default value if no mode available\n",
    "                    default_val = \"Unknown\"\n",
    "                    df_no_duplicates = df_no_duplicates.with_columns(\n",
    "                        pl.col(col_name).fill_null(default_val)\n",
    "                    )\n",
    "                    print(f\"    - {col_name}: filled with default value '{default_val}'\")\n",
    "            except Exception as e:\n",
    "                print(f\"    - {col_name}: Error during imputation - {e}\")\n",
    "                # Skip this column or use a simple default\n",
    "                continue\n",
    "    \n",
    "    imputation_time = time.time() - start_time\n",
    "    print(f\"  ✓ Missing value imputation completed in {imputation_time:.2f} seconds\")\n",
    "else:\n",
    "    print(f\"\\n• No missing values detected - proceeding to outlier handling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Outlier Handling\n",
    "\n",
    "Handle outliers in numerical columns using the Interquartile Range (IQR) method with Polars' efficient statistical functions. This approach caps outliers at statistically reasonable bounds rather than removing records entirely.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Handle outliers in price column using IQR method\n",
    "if 'price' in df_no_duplicates.columns:\n",
    "    print(f\"\\n• Handling Outliers (IQR Method):\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Calculate quartiles efficiently with Polars\n",
    "    quartiles = df_no_duplicates.select([\n",
    "        pl.col('price').quantile(0.25).alias('Q1'),\n",
    "        pl.col('price').quantile(0.75).alias('Q3')\n",
    "    ]).to_pandas().iloc[0]\n",
    "    \n",
    "    Q1, Q3 = quartiles['Q1'], quartiles['Q3']\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    # Define outlier bounds\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    # Count outliers before capping\n",
    "    outlier_count = df_no_duplicates.filter(\n",
    "        (pl.col('price') < lower_bound) | (pl.col('price') > upper_bound)\n",
    "    ).height\n",
    "    \n",
    "    # Cap outliers at bounds efficiently\n",
    "    df_no_duplicates = df_no_duplicates.with_columns(\n",
    "        pl.col('price').clip(lower_bound, upper_bound)\n",
    "    )\n",
    "    \n",
    "    outlier_time = time.time() - start_time\n",
    "    print(f\"  - price: {outlier_count:,} outliers capped (bounds: {lower_bound:.1f} - {upper_bound:.1f})\")\n",
    "    print(f\"  ✓ Outlier handling completed in {outlier_time:.2f} seconds\")\n",
    "else:\n",
    "    outlier_count = 0\n",
    "    print(f\"\\n• No price column found - skipping outlier handling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Data Validation and Quality Check\n",
    "\n",
    "Validate the cleaned dataset using Polars' efficient operations to ensure all preprocessing steps were successful and data integrity is maintained.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data validation and final quality check\n",
    "print(f\"\\n• Final Data Quality Check:\")\n",
    "start_time = time.time()\n",
    "\n",
    "final_record_count = df_no_duplicates.height\n",
    "print(f\"  - Final dataset size: {final_record_count:,} records\")\n",
    "\n",
    "# Check for remaining missing values efficiently\n",
    "try:\n",
    "    # Use the null_count method we know works\n",
    "    final_null_counts = df_no_duplicates.null_count()\n",
    "    remaining_nulls_total = sum(final_null_counts.row(0))\n",
    "except Exception as e:\n",
    "    print(f\"  Warning: Could not calculate null counts - {e}\")\n",
    "    remaining_nulls_total = 0\n",
    "\n",
    "print(f\"  - Remaining missing values: {remaining_nulls_total}\")\n",
    "print(f\"  - Data integrity: {'✓ PASSED' if remaining_nulls_total == 0 else '✗ FAILED'}\")\n",
    "print(f\"  - Memory usage: {df_no_duplicates.estimated_size('mb'):.1f} MB\")\n",
    "\n",
    "validation_time = time.time() - start_time\n",
    "print(f\"  ✓ Validation completed in {validation_time:.2f} seconds\")\n",
    "\n",
    "# Store cleaned dataset reference\n",
    "df_cleaned = df_no_duplicates\n",
    "print(f\"✓ Dataset ready for downstream processing\")\n",
    "\n",
    "# Update processing summary with actual values\n",
    "processing_summary = {\n",
    "    'original_record_count': total_records,\n",
    "    'final_record_count': final_record_count,\n",
    "    'duplicates_removed': duplicate_count,\n",
    "    'missing_values_imputed': len(missing_summary),\n",
    "    'outliers_handled': outlier_count if 'outlier_count' in locals() else 0,\n",
    "    'remaining_nulls': remaining_nulls_total,\n",
    "    'data_integrity_passed': remaining_nulls_total == 0,\n",
    "    'memory_usage_mb': df_cleaned.estimated_size('mb'),\n",
    "    'processing_framework': f'Polars {pl.__version__}'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Cleaned Dataset\n",
    "\n",
    "Save the cleaned dataset in Parquet format using Polars' efficient I/O for optimal storage and future analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save cleaned dataset efficiently\n",
    "print(f\"\\n• Saving Cleaned Dataset:\")\n",
    "start_time = time.time()\n",
    "\n",
    "output_dir = os.path.join(data_dir, 'processed')\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "output_path = os.path.join(output_dir, 'hm_customer_data_cleaned.parquet')\n",
    "\n",
    "# Polars writes Parquet files very efficiently\n",
    "df_cleaned.write_parquet(output_path)\n",
    "\n",
    "save_time = time.time() - start_time\n",
    "file_size_mb = os.path.getsize(output_path) / (1024 * 1024)\n",
    "\n",
    "print(f\"  ✓ Saved preprocessed and cleaned dataset as Parquet file\")\n",
    "print(f\"  - Location: {output_path}\")\n",
    "print(f\"  - File size: {file_size_mb:.1f} MB\")\n",
    "print(f\"  - Save time: {save_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary Statistics\n",
    "\n",
    "Generate comprehensive summary statistics for the cleaned dataset using Polars' efficient statistical functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive summary statistics\n",
    "print(f\"\\n• Dataset Summary Statistics:\")\n",
    "\n",
    "# Get schema information\n",
    "schema_info = df_cleaned.schema\n",
    "numerical_columns = [col for col, dtype in schema_info.items() \n",
    "                    if dtype in [pl.Int64, pl.Int32, pl.Float64, pl.Float32]]\n",
    "\n",
    "if numerical_columns:\n",
    "    print(f\"\\nSummary statistics for numerical columns:\")\n",
    "    \n",
    "    # Generate comprehensive statistics efficiently\n",
    "    stats_expr = []\n",
    "    for col in numerical_columns[:5]:  # Limit to first 5 for readability\n",
    "        stats_expr.extend([\n",
    "            pl.col(col).count().alias(f\"{col}_count\"),\n",
    "            pl.col(col).mean().alias(f\"{col}_mean\"),\n",
    "            pl.col(col).std().alias(f\"{col}_std\"),\n",
    "            pl.col(col).min().alias(f\"{col}_min\"),\n",
    "            pl.col(col).max().alias(f\"{col}_max\")\n",
    "        ])\n",
    "    \n",
    "    summary_stats = df_cleaned.select(stats_expr)\n",
    "    \n",
    "    # Display statistics in a readable format\n",
    "    for col in numerical_columns[:5]:\n",
    "        stats = summary_stats.select([f\"{col}_count\", f\"{col}_mean\", f\"{col}_std\", f\"{col}_min\", f\"{col}_max\"]).to_pandas().iloc[0]\n",
    "        print(f\"\\n{col}:\")\n",
    "        print(f\"  Count: {stats[f'{col}_count']:,.0f}\")\n",
    "        print(f\"  Mean:  {stats[f'{col}_mean']:.2f}\")\n",
    "        print(f\"  Std:   {stats[f'{col}_std']:.2f}\")\n",
    "        print(f\"  Min:   {stats[f'{col}_min']:.2f}\")\n",
    "        print(f\"  Max:   {stats[f'{col}_max']:.2f}\")\n",
    "else:\n",
    "    print(\"No numerical columns found for summary statistics\")\n",
    "\n",
    "# Data type summary\n",
    "print(f\"\\nData types summary:\")\n",
    "for col, dtype in schema_info.items():\n",
    "    print(f\"  {col}: {dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Processing Summary and Performance Metrics\n",
    "\n",
    "Compile and display comprehensive preprocessing results with performance metrics showcasing Polars' efficiency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile comprehensive processing summary\n",
    "processing_summary = {\n",
    "    'original_record_count': total_records,\n",
    "    'final_record_count': final_record_count,\n",
    "    'duplicates_removed': duplicate_count,\n",
    "    'missing_values_imputed': len(missing_summary),\n",
    "    'outliers_handled': outlier_count if 'price' in df_cleaned.columns else 0,\n",
    "    'remaining_nulls': remaining_nulls_total,\n",
    "    'data_integrity_passed': remaining_nulls_total == 0,\n",
    "    'memory_usage_mb': df_cleaned.estimated_size('mb'),\n",
    "    'processing_framework': f'Polars {pl.__version__}'\n",
    "}\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(\"PREPROCESSING PIPELINE COMPLETED SUCCESSFULLY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\n✓ Processing Summary:\")\n",
    "print(f\"  - Original records: {processing_summary['original_record_count']:,}\")\n",
    "print(f\"  - Final records: {processing_summary['final_record_count']:,}\")\n",
    "print(f\"  - Duplicates removed: {processing_summary['duplicates_removed']:,}\")\n",
    "print(f\"  - Missing value columns handled: {processing_summary['missing_values_imputed']}\")\n",
    "print(f\"  - Outliers capped: {processing_summary['outliers_handled']:,}\")\n",
    "print(f\"  - Data quality: {'High' if processing_summary['data_integrity_passed'] else 'Needs attention'}\")\n",
    "print(f\"  - Memory usage: {processing_summary['memory_usage_mb']:.1f} MB\")\n",
    "print(f\"  - Framework: {processing_summary['processing_framework']}\")\n",
    "\n",
    "print(f\"\\n✓ Dataset ready for downstream analysis and modelling\")\n",
    "print(f\"✓ Cleaned dataset available as 'df_cleaned'\")\n",
    "print(f\"✓ Processing summary available as 'processing_summary'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Data Preview and Validation\n",
    "\n",
    "Display a sample of the cleaned dataset to verify preprocessing results and demonstrate data quality improvements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show sample of cleaned data\n",
    "print(f\"Sample of cleaned dataset:\")\n",
    "sample_cols = [\"customer_id\", \"article_id\", \"price\", \"sales_channel_id\", \"age\", \"club_member_status\", \"product_type_name\"]\n",
    "available_cols = [col for col in sample_cols if col in df_cleaned.columns]\n",
    "\n",
    "print(f\"\\nFirst 10 records:\")\n",
    "print(df_cleaned.select(available_cols).head(10))\n",
    "\n",
    "# Verify data quality improvements\n",
    "print(f\"\\n• Data Quality Verification:\")\n",
    "final_null_check = df_cleaned.null_count()\n",
    "total_nulls = sum(final_null_check.row(0))\n",
    "print(f\"  - Total null values: {total_nulls}\")\n",
    "print(f\"  - Data completeness: {((df_cleaned.height * len(df_cleaned.columns) - total_nulls) / (df_cleaned.height * len(df_cleaned.columns)) * 100):.2f}%\")\n",
    "print(f\"  - Dataset shape: {df_cleaned.height:,} rows × {len(df_cleaned.columns)} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Advanced Analytics Preparation\n",
    "\n",
    "Prepare the cleaned dataset for advanced analytics by creating derived features and performing initial segmentation preparation using Polars' powerful expression API.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Create derived features for analytics\n",
    "print(f\"\\n• Preparing for Advanced Analytics:\")\n",
    "\n",
    "# Customer transaction summary (useful for segmentation)\n",
    "if all(col in df_cleaned.columns for col in ['customer_id', 'price']):\n",
    "    customer_summary = (\n",
    "        df_cleaned\n",
    "        .group_by('customer_id')\n",
    "        .agg([\n",
    "            pl.col('price').count().alias('transaction_count'),\n",
    "            pl.col('price').sum().alias('total_spent'),\n",
    "            pl.col('price').mean().alias('avg_transaction_value'),\n",
    "            pl.col('article_id').n_unique().alias('unique_items_purchased')\n",
    "        ])\n",
    "        .sort('total_spent', descending=True)\n",
    "    )\n",
    "    \n",
    "    print(f\"  ✓ Customer transaction summary created ({customer_summary.height:,} customers)\")\n",
    "    print(f\"  ✓ Ready for RFM analysis and customer segmentation\")\n",
    "\n",
    "# Product performance summary\n",
    "if 'product_type_name' in df_cleaned.columns:\n",
    "    product_summary = (\n",
    "        df_cleaned\n",
    "        .group_by('product_type_name')\n",
    "        .agg([\n",
    "            pl.col('customer_id').count().alias('sales_volume'),\n",
    "            pl.col('price').mean().alias('avg_price'),\n",
    "            pl.col('customer_id').n_unique().alias('unique_customers')\n",
    "        ])\n",
    "        .sort('sales_volume', descending=True)\n",
    "    )\n",
    "    \n",
    "    print(f\"  ✓ Product performance summary created ({product_summary.height:,} product types)\")\n",
    "    print(f\"  ✓ Ready for product recommendation analysis\")\n",
    "\n",
    "print(f\"\\n✓ Dataset optimised for machine learning and advanced analytics\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook provides a comprehensive preprocessing and cleaning pipeline for the H&M dataset using Polars for high-performance data processing:\n",
    "\n",
    "1. **Quality Assessment**: Efficiently identified data quality issues using Polars' optimised operations\n",
    "2. **Duplicate Removal**: Removed duplicate records with minimal processing time\n",
    "3. **Missing Value Imputation**: Applied statistical methods using Polars' efficient aggregation functions\n",
    "4. **Outlier Handling**: Used IQR method with Polars' statistical functions for robust outlier capping\n",
    "5. **Data Validation**: Verified final dataset quality with comprehensive checks\n",
    "6. **Performance Optimisation**: Leveraged Polars' speed and memory efficiency throughout\n",
    "\n",
    "### Polars Advantages Demonstrated\n",
    "\n",
    "- ✓ **Speed**: Significantly faster processing compared to pandas-based workflows\n",
    "- ✓ **Memory Efficiency**: Lower memory footprint for large datasets\n",
    "- ✓ **Expressive API**: Clean, readable code for complex data transformations\n",
    "- ✓ **Built-in Optimisation**: Automatic query optimisation and parallel processing\n",
    "- ✓ **Type Safety**: Better handling of data types and null values\n",
    "\n",
    "### Key Quality Improvements\n",
    "\n",
    "- ✓ Removed duplicate records efficiently\n",
    "- ✓ Handled missing values using appropriate statistical methods\n",
    "- ✓ Capped outliers to reduce bias whilst preserving data structure\n",
    "- ✓ Validated data integrity with comprehensive quality checks\n",
    "- ✓ Optimised dataset for downstream processing and analytics\n",
    "\n",
    "### Ready for Advanced Analytics\n",
    "\n",
    "The cleaned dataset is now optimised for:\n",
    "\n",
    "- **Customer Segmentation**: RFM analysis and behavioural clustering\n",
    "- **Machine Learning**: Feature engineering and model development\n",
    "- **Statistical Analysis**: Reliable insights with high-quality data\n",
    "- **Recommendation Systems**: Customer-product interaction analysis\n",
    "- **Time Series Analysis**: Temporal pattern exploration\n",
    "\n",
    "The combination of Polars' performance and comprehensive data cleaning ensures a robust foundation for advanced H&M customer analytics and machine learning workflows.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
