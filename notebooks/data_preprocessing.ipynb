{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# H&M Data Preprocessing and Cleaning with Polars\n",
    "\n",
    "This notebook provides a data preprocessing and cleaning pipeline for the H&M dataset using Polars for high-performance data processing. The pipeline includes duplicate removal, missing value imputation, outlier handling, and data validation.\n",
    "\n",
    "## Overview\n",
    "\n",
    "The preprocessing pipeline handles:\n",
    "\n",
    "- **Duplicate Removal**: Identifying and removing duplicate records efficiently\n",
    "- **Missing Value Imputation**: Filling missing values using statistical methods\n",
    "- **Outlier Handling**: Detecting and capping outliers using IQR method\n",
    "- **Data Validation**: Ensuring data integrity after cleaning\n",
    "- **Performance Optimisation**: Leveraging Polars' speed and memory efficiency\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Setup\n",
    "\n",
    "Import Polars and other necessary libraries for data processing, cleaning, and validation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polars version: 1.32.0\n",
      "Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import time\n",
    "from typing import Tuple, List, Dict, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure Polars for optimal performance\n",
    "try:\n",
    "    pl.Config.set_streaming_chunk_size(10000)\n",
    "    # Some Polars versions may not have all config options\n",
    "    if hasattr(pl.Config, 'set_table_width'):\n",
    "        pl.Config.set_table_width(120)\n",
    "except AttributeError:\n",
    "    # Gracefully handle missing config options in older versions\n",
    "    pass\n",
    "\n",
    "print(f\"Polars version: {pl.__version__}\")\n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Integrated Dataset\n",
    "\n",
    "Load the integrated dataset from the EDA phase. This assumes you have already run the data exploration notebook and have an integrated dataset available.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading integrated dataset...\n",
      "✓ Found saved integrated dataset - loading from Parquet...\n",
      "✓ Loaded 3,178,832 records in 0.55 seconds\n",
      "✓ Memory usage: 1636.3 MB\n",
      "\n",
      "Dataset ready for preprocessing:\n",
      "• Records: 3,178,832\n",
      "• Columns: 35\n",
      "• Memory usage: 1636.3 MB\n"
     ]
    }
   ],
   "source": [
    "# Load integrated dataset (assumes previous EDA notebook has been run)\n",
    "# Configuration parameters\n",
    "data_dir = '../data'  # Path relative to the notebook's location in notebooks/\n",
    "use_saved_parquet = True  # Prefer saved Parquet files for speed\n",
    "\n",
    "# Option 1: Load from saved Parquet file (recommended and fastest)\n",
    "integrated_path = os.path.join(data_dir, 'processed', 'hm_integrated_dataset.parquet')\n",
    "\n",
    "print(\"Loading integrated dataset...\")\n",
    "start_time = time.time()\n",
    "\n",
    "if use_saved_parquet and os.path.exists(integrated_path):\n",
    "    print(\"✓ Found saved integrated dataset - loading from Parquet...\")\n",
    "    df_integrated = pl.read_parquet(integrated_path)\n",
    "    load_time = time.time() - start_time\n",
    "    print(f\"✓ Loaded {df_integrated.height:,} records in {load_time:.2f} seconds\")\n",
    "    print(f\"✓ Memory usage: {df_integrated.estimated_size('mb'):.1f} MB\")\n",
    "else:\n",
    "    print(\"Saved integrated dataset not found. Creating from raw files...\")\n",
    "    \n",
    "    # Set up individual file paths\n",
    "    transactions_path = os.path.join(data_dir, 'raw', 'transactions_train.csv')\n",
    "    customers_path = os.path.join(data_dir, 'raw', 'customers.csv')\n",
    "    articles_path = os.path.join(data_dir, 'raw', 'articles.csv')\n",
    "    \n",
    "    # Verify all files exist\n",
    "    print(f\"Path verification:\")\n",
    "    print(f\"• Transactions file exists: {os.path.exists(transactions_path)}\")\n",
    "    print(f\"• Customers file exists: {os.path.exists(customers_path)}\")\n",
    "    print(f\"• Articles file exists: {os.path.exists(articles_path)}\")\n",
    "    \n",
    "    if not all(os.path.exists(p) for p in [transactions_path, customers_path, articles_path]):\n",
    "        raise FileNotFoundError(\"Required data files not found. Please check file paths.\")\n",
    "    \n",
    "    # Load individual datasets\n",
    "    print(\"Loading individual datasets...\")\n",
    "    \n",
    "    # Load with sample for memory efficiency\n",
    "    sample_fraction = 0.1\n",
    "    print(f\"Using {sample_fraction*100:.0f}% sample for processing...\")\n",
    "    \n",
    "    df_transactions = pl.read_csv(transactions_path).sample(fraction=sample_fraction, seed=42)\n",
    "    df_customers = pl.read_csv(customers_path)\n",
    "    df_articles = pl.read_csv(articles_path)\n",
    "    \n",
    "    print(f\"✓ Loaded transactions: {df_transactions.height:,} records\")\n",
    "    print(f\"✓ Loaded customers: {df_customers.height:,} records\")\n",
    "    print(f\"✓ Loaded articles: {df_articles.height:,} records\")\n",
    "    \n",
    "    # Create integrated dataset\n",
    "    print(\"Creating integrated dataset...\")\n",
    "    df_integrated = (\n",
    "        df_transactions\n",
    "        .join(df_customers, on=\"customer_id\", how=\"left\")\n",
    "        .join(df_articles, on=\"article_id\", how=\"left\")\n",
    "    )\n",
    "    \n",
    "    load_time = time.time() - start_time\n",
    "    print(f\"✓ Created integrated dataset with {df_integrated.height:,} records\")\n",
    "    print(f\"✓ Integration completed in {load_time:.2f} seconds\")\n",
    "    \n",
    "    # Optionally save for future use\n",
    "    os.makedirs(os.path.join(data_dir, 'processed'), exist_ok=True)\n",
    "    df_integrated.write_parquet(integrated_path)\n",
    "    print(f\"✓ Saved integrated dataset for future use\")\n",
    "\n",
    "print(f\"\\nDataset ready for preprocessing:\")\n",
    "print(f\"• Records: {df_integrated.height:,}\")\n",
    "print(f\"• Columns: {len(df_integrated.columns)}\")\n",
    "print(f\"• Memory usage: {df_integrated.estimated_size('mb'):.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Quality Assessment\n",
    "\n",
    "Assess the current data quality using Polars' efficient operations to understand issues that need to be addressed in preprocessing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysing existing data quality issues in H&M dataset...\n",
      "\n",
      "• Missing Values Analysis:\n",
      "  FN: 1,821,934 (57.31%)\n",
      "  Active: 1,842,236 (57.95%)\n",
      "  club_member_status: 6,226 (0.20%)\n",
      "  fashion_news_frequency: 14,299 (0.45%)\n",
      "  age: 14,156 (0.45%)\n",
      "  detail_desc: 11,458 (0.36%)\n",
      "\n",
      "• Duplicate Analysis:\n",
      "  Total records: 3,178,832\n",
      "  Unique records: 3,141,366\n",
      "  Duplicate records: 37,466\n",
      "\n",
      "• Price Distribution Analysis:\n",
      "  count: 3178832.00\n",
      "  mean: 0.03\n",
      "  std: 0.02\n",
      "  min: 0.00\n",
      "  25%: 0.02\n",
      "  50%: 0.03\n",
      "  75%: 0.03\n",
      "  max: 0.59\n",
      "\n",
      "✓ Quality assessment completed in 1.76 seconds\n"
     ]
    }
   ],
   "source": [
    "print(\"Analysing existing data quality issues in H&M dataset...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Efficient quality assessment with Polars\n",
    "print(f\"\\n• Missing Values Analysis:\")\n",
    "\n",
    "# Get null counts for all columns in one operation\n",
    "null_counts = df_integrated.null_count()\n",
    "total_records = df_integrated.height\n",
    "\n",
    "missing_summary = []\n",
    "for col_name in df_integrated.columns:\n",
    "    missing_count = null_counts[col_name][0]\n",
    "    if missing_count > 0:\n",
    "        missing_percentage = (missing_count / total_records) * 100\n",
    "        missing_summary.append((col_name, missing_count, missing_percentage))\n",
    "        print(f\"  {col_name}: {missing_count:,} ({missing_percentage:.2f}%)\")\n",
    "\n",
    "if not missing_summary:\n",
    "    print(\"  ✓ No missing values found in dataset\")\n",
    "\n",
    "# Analyse duplicates efficiently\n",
    "print(f\"\\n• Duplicate Analysis:\")\n",
    "unique_records = df_integrated.unique().height\n",
    "duplicate_count = total_records - unique_records\n",
    "print(f\"  Total records: {total_records:,}\")\n",
    "print(f\"  Unique records: {unique_records:,}\")\n",
    "print(f\"  Duplicate records: {duplicate_count:,}\")\n",
    "\n",
    "# Price distribution analysis\n",
    "if 'price' in df_integrated.columns:\n",
    "    print(f\"\\n• Price Distribution Analysis:\")\n",
    "    price_stats = df_integrated.select([\n",
    "        pl.col('price').count().alias('count'),\n",
    "        pl.col('price').mean().alias('mean'),\n",
    "        pl.col('price').std().alias('std'),\n",
    "        pl.col('price').min().alias('min'),\n",
    "        pl.col('price').quantile(0.25).alias('25%'),\n",
    "        pl.col('price').quantile(0.5).alias('50%'),\n",
    "        pl.col('price').quantile(0.75).alias('75%'),\n",
    "        pl.col('price').max().alias('max')\n",
    "    ]).to_pandas().iloc[0]\n",
    "    \n",
    "    for stat_name, value in price_stats.items():\n",
    "        print(f\"  {stat_name}: {value:.2f}\")\n",
    "else:\n",
    "    print(f\"\\n• No price column found - skipping price analysis\")\n",
    "\n",
    "assessment_time = time.time() - start_time\n",
    "print(f\"\\n✓ Quality assessment completed in {assessment_time:.2f} seconds\")\n",
    "\n",
    "# Store results for use in processing steps\n",
    "quality_report = {\n",
    "    'missing_summary': missing_summary,\n",
    "    'duplicate_count': duplicate_count,\n",
    "    'total_records': total_records\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Duplicate Removal\n",
    "\n",
    "Remove duplicate records efficiently using Polars' optimised duplicate removal to ensure data integrity and prevent bias in analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting data preprocessing and cleaning pipeline on 3,178,832 records...\n",
      "\n",
      "• Removing Duplicates:\n",
      "  - Removed 37,466 duplicate records in 1.27 seconds\n",
      "  - Remaining records: 3,141,366\n",
      "  - Memory saved: 19.2 MB\n"
     ]
    }
   ],
   "source": [
    "print(f\"Starting data preprocessing and cleaning pipeline on {total_records:,} records...\")\n",
    "\n",
    "# Step 1: Remove duplicates if any exist\n",
    "if duplicate_count > 0:\n",
    "    print(f\"\\n• Removing Duplicates:\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    df_no_duplicates = df_integrated.unique()\n",
    "    final_count = df_no_duplicates.height\n",
    "    \n",
    "    dedup_time = time.time() - start_time\n",
    "    print(f\"  - Removed {duplicate_count:,} duplicate records in {dedup_time:.2f} seconds\")\n",
    "    print(f\"  - Remaining records: {final_count:,}\")\n",
    "    print(f\"  - Memory saved: {(df_integrated.estimated_size('mb') - df_no_duplicates.estimated_size('mb')):.1f} MB\")\n",
    "else:\n",
    "    df_no_duplicates = df_integrated\n",
    "    print(f\"\\n• No duplicates found - proceeding with original dataset for preprocessing\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Missing Value Imputation\n",
    "\n",
    "Handle missing values using Polars' efficient statistical functions:\n",
    "\n",
    "- **Numerical columns**: Median imputation (robust to outliers)\n",
    "- **Categorical columns**: Mode imputation or default values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "• Handling Missing Values:\n",
      "  Processing numerical columns: ['FN', 'Active', 'age']\n",
      "    - FN: filled with median value 1.00\n",
      "    - Active: filled with median value 1.00\n",
      "    - age: filled with median value 31.00\n",
      "  Processing categorical columns: ['club_member_status', 'fashion_news_frequency', 'detail_desc']\n",
      "    - club_member_status: filled with mode value 'ACTIVE'\n",
      "    - fashion_news_frequency: filled with mode value 'NONE'\n",
      "    - detail_desc: filled with mode value 'High-waisted jeans in washed superstretch denim with a zip fly and button, fake front pockets, real back pockets and super-skinny legs.'\n",
      "  ✓ Missing value imputation completed in 0.31 seconds\n"
     ]
    }
   ],
   "source": [
    "# Step 2: Handle missing values if any exist\n",
    "if missing_summary:\n",
    "    print(f\"\\n• Handling Missing Values:\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Identify column types efficiently\n",
    "    schema_info = df_no_duplicates.schema\n",
    "    \n",
    "    # Handle numerical columns with median imputation\n",
    "    numerical_cols = [col_name for col_name, _, _ in missing_summary \n",
    "                     if schema_info[col_name] in [pl.Int64, pl.Int32, pl.Float64, pl.Float32, pl.UInt32, pl.UInt64]]\n",
    "    \n",
    "    if numerical_cols:\n",
    "        print(f\"  Processing numerical columns: {numerical_cols}\")\n",
    "        \n",
    "        # Calculate medians for all numerical columns at once\n",
    "        median_exprs = [pl.col(col).median().alias(f\"{col}_median\") for col in numerical_cols]\n",
    "        median_values = df_no_duplicates.select(median_exprs).to_pandas().iloc[0] if median_exprs else {}\n",
    "        \n",
    "        # Fill missing values with medians\n",
    "        for col_name in numerical_cols:\n",
    "            median_val = median_values.get(f\"{col_name}_median\", 0)\n",
    "            df_no_duplicates = df_no_duplicates.with_columns(\n",
    "                pl.col(col_name).fill_null(median_val)\n",
    "            )\n",
    "            print(f\"    - {col_name}: filled with median value {median_val:.2f}\")\n",
    "    \n",
    "    # Handle categorical columns with mode imputation\n",
    "    categorical_cols = [col_name for col_name, _, _ in missing_summary \n",
    "                       if schema_info[col_name] in [pl.Utf8, pl.String]]\n",
    "    \n",
    "    if categorical_cols:\n",
    "        print(f\"  Processing categorical columns: {categorical_cols}\")\n",
    "        \n",
    "        for col_name in categorical_cols:\n",
    "            try:\n",
    "                # Get mode (most frequent value) efficiently\n",
    "                mode_result = (\n",
    "                    df_no_duplicates\n",
    "                    .select(col_name)\n",
    "                    .filter(pl.col(col_name).is_not_null())\n",
    "                    .group_by(col_name)\n",
    "                    .count()\n",
    "                    .sort('count', descending=True)\n",
    "                    .limit(1)\n",
    "                )\n",
    "                \n",
    "                if mode_result.height > 0:\n",
    "                    mode_val = mode_result[col_name][0]\n",
    "                    df_no_duplicates = df_no_duplicates.with_columns(\n",
    "                        pl.col(col_name).fill_null(mode_val)\n",
    "                    )\n",
    "                    print(f\"    - {col_name}: filled with mode value '{mode_val}'\")\n",
    "                else:\n",
    "                    # Use default value if no mode available\n",
    "                    default_val = \"Unknown\"\n",
    "                    df_no_duplicates = df_no_duplicates.with_columns(\n",
    "                        pl.col(col_name).fill_null(default_val)\n",
    "                    )\n",
    "                    print(f\"    - {col_name}: filled with default value '{default_val}'\")\n",
    "            except Exception as e:\n",
    "                print(f\"    - {col_name}: Error during imputation - {e}\")\n",
    "                # Skip this column or use a simple default\n",
    "                continue\n",
    "    \n",
    "    imputation_time = time.time() - start_time\n",
    "    print(f\"  ✓ Missing value imputation completed in {imputation_time:.2f} seconds\")\n",
    "else:\n",
    "    print(f\"\\n• No missing values detected - proceeding to outlier handling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Outlier Handling\n",
    "\n",
    "Handle outliers in numerical columns using the Interquartile Range (IQR) method with Polars' efficient statistical functions. This approach caps outliers at statistically reasonable bounds rather than removing records entirely.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "• Handling Outliers (IQR Method):\n",
      "  - price: 154,382 outliers capped (bounds: -0.0 - 0.1)\n",
      "  ✓ Outlier handling completed in 0.03 seconds\n"
     ]
    }
   ],
   "source": [
    "# Step 3: Handle outliers in price column using IQR method\n",
    "if 'price' in df_no_duplicates.columns:\n",
    "    print(f\"\\n• Handling Outliers (IQR Method):\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    # Calculate quartiles efficiently with Polars\n",
    "    quartiles = df_no_duplicates.select([\n",
    "        pl.col('price').quantile(0.25).alias('Q1'),\n",
    "        pl.col('price').quantile(0.75).alias('Q3')\n",
    "    ]).to_pandas().iloc[0]\n",
    "    \n",
    "    Q1, Q3 = quartiles['Q1'], quartiles['Q3']\n",
    "    IQR = Q3 - Q1\n",
    "    \n",
    "    # Define outlier bounds\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    \n",
    "    # Count outliers before capping\n",
    "    outlier_count = df_no_duplicates.filter(\n",
    "        (pl.col('price') < lower_bound) | (pl.col('price') > upper_bound)\n",
    "    ).height\n",
    "    \n",
    "    # Cap outliers at bounds efficiently\n",
    "    df_no_duplicates = df_no_duplicates.with_columns(\n",
    "        pl.col('price').clip(lower_bound, upper_bound)\n",
    "    )\n",
    "    \n",
    "    outlier_time = time.time() - start_time\n",
    "    print(f\"  - price: {outlier_count:,} outliers capped (bounds: {lower_bound:.1f} - {upper_bound:.1f})\")\n",
    "    print(f\"  ✓ Outlier handling completed in {outlier_time:.2f} seconds\")\n",
    "else:\n",
    "    outlier_count = 0\n",
    "    print(f\"\\n• No price column found - skipping outlier handling\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Data Validation and Quality Check\n",
    "\n",
    "Validate the cleaned dataset using Polars' efficient operations to ensure all preprocessing steps were successful and data integrity is maintained.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "• Final Data Quality Check:\n",
      "  - Final dataset size: 3,141,366 records\n",
      "  - Remaining missing values: 0\n",
      "  - Data integrity: ✓ PASSED\n",
      "  - Memory usage: 2023.9 MB\n",
      "  ✓ Validation completed in 0.00 seconds\n",
      "✓ Dataset ready for downstream processing\n"
     ]
    }
   ],
   "source": [
    "# Data validation and final quality check\n",
    "print(f\"\\n• Final Data Quality Check:\")\n",
    "start_time = time.time()\n",
    "\n",
    "final_record_count = df_no_duplicates.height\n",
    "print(f\"  - Final dataset size: {final_record_count:,} records\")\n",
    "\n",
    "# Check for remaining missing values efficiently\n",
    "try:\n",
    "    # Use the null_count method we know works\n",
    "    final_null_counts = df_no_duplicates.null_count()\n",
    "    remaining_nulls_total = sum(final_null_counts.row(0))\n",
    "except Exception as e:\n",
    "    print(f\"  Warning: Could not calculate null counts - {e}\")\n",
    "    remaining_nulls_total = 0\n",
    "\n",
    "print(f\"  - Remaining missing values: {remaining_nulls_total}\")\n",
    "print(f\"  - Data integrity: {'✓ PASSED' if remaining_nulls_total == 0 else '✗ FAILED'}\")\n",
    "print(f\"  - Memory usage: {df_no_duplicates.estimated_size('mb'):.1f} MB\")\n",
    "\n",
    "validation_time = time.time() - start_time\n",
    "print(f\"  ✓ Validation completed in {validation_time:.2f} seconds\")\n",
    "\n",
    "# Store cleaned dataset reference\n",
    "df_cleaned = df_no_duplicates\n",
    "print(f\"✓ Dataset ready for downstream processing\")\n",
    "\n",
    "# Update processing summary with actual values\n",
    "processing_summary = {\n",
    "    'original_record_count': total_records,\n",
    "    'final_record_count': final_record_count,\n",
    "    'duplicates_removed': duplicate_count,\n",
    "    'missing_values_imputed': len(missing_summary),\n",
    "    'outliers_handled': outlier_count if 'outlier_count' in locals() else 0,\n",
    "    'remaining_nulls': remaining_nulls_total,\n",
    "    'data_integrity_passed': remaining_nulls_total == 0,\n",
    "    'memory_usage_mb': df_cleaned.estimated_size('mb'),\n",
    "    'processing_framework': f'Polars {pl.__version__}'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Cleaned Dataset\n",
    "\n",
    "Save the cleaned dataset in Parquet format using Polars' efficient I/O for optimal storage and future analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "• Saving Cleaned Dataset:\n",
      "  ✓ Saved preprocessed and cleaned dataset as Parquet file\n",
      "  - Location: ../data/processed/hm_customer_data_cleaned.parquet\n",
      "  - File size: 228.1 MB\n",
      "  - Save time: 2.26 seconds\n"
     ]
    }
   ],
   "source": [
    "# Save cleaned dataset efficiently\n",
    "print(f\"\\n• Saving Cleaned Dataset:\")\n",
    "start_time = time.time()\n",
    "\n",
    "output_dir = os.path.join(data_dir, 'processed')\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "output_path = os.path.join(output_dir, 'hm_customer_data_cleaned.parquet')\n",
    "\n",
    "# Polars writes Parquet files very efficiently\n",
    "df_cleaned.write_parquet(output_path)\n",
    "\n",
    "save_time = time.time() - start_time\n",
    "file_size_mb = os.path.getsize(output_path) / (1024 * 1024)\n",
    "\n",
    "print(f\"  ✓ Saved preprocessed and cleaned dataset as Parquet file\")\n",
    "print(f\"  - Location: {output_path}\")\n",
    "print(f\"  - File size: {file_size_mb:.1f} MB\")\n",
    "print(f\"  - Save time: {save_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Summary Statistics\n",
    "\n",
    "Generate comprehensive summary statistics for the cleaned dataset using Polars' efficient statistical functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "• Dataset Summary Statistics:\n",
      "\n",
      "Summary statistics for numerical columns:\n",
      "\n",
      "article_id:\n",
      "  Count: 3,141,366\n",
      "  Mean:  696371443.13\n",
      "  Std:   133312459.46\n",
      "  Min:   108775015.00\n",
      "  Max:   956217002.00\n",
      "\n",
      "price:\n",
      "  Count: 3,141,366\n",
      "  Mean:  0.03\n",
      "  Std:   0.01\n",
      "  Min:   0.00\n",
      "  Max:   0.06\n",
      "\n",
      "sales_channel_id:\n",
      "  Count: 3,141,366\n",
      "  Mean:  1.70\n",
      "  Std:   0.46\n",
      "  Min:   1.00\n",
      "  Max:   2.00\n",
      "\n",
      "FN:\n",
      "  Count: 3,141,366\n",
      "  Mean:  1.00\n",
      "  Std:   0.00\n",
      "  Min:   1.00\n",
      "  Max:   1.00\n",
      "\n",
      "Active:\n",
      "  Count: 3,141,366\n",
      "  Mean:  1.00\n",
      "  Std:   0.00\n",
      "  Min:   1.00\n",
      "  Max:   1.00\n",
      "\n",
      "Data types summary:\n",
      "  t_dat: String\n",
      "  customer_id: String\n",
      "  article_id: Int64\n",
      "  price: Float64\n",
      "  sales_channel_id: Int64\n",
      "  FN: Float64\n",
      "  Active: Float64\n",
      "  club_member_status: String\n",
      "  fashion_news_frequency: String\n",
      "  age: Float64\n",
      "  postal_code: String\n",
      "  product_code: Int64\n",
      "  prod_name: String\n",
      "  product_type_no: Int64\n",
      "  product_type_name: String\n",
      "  product_group_name: String\n",
      "  graphical_appearance_no: Int64\n",
      "  graphical_appearance_name: String\n",
      "  colour_group_code: Int64\n",
      "  colour_group_name: String\n",
      "  perceived_colour_value_id: Int64\n",
      "  perceived_colour_value_name: String\n",
      "  perceived_colour_master_id: Int64\n",
      "  perceived_colour_master_name: String\n",
      "  department_no: Int64\n",
      "  department_name: String\n",
      "  index_code: String\n",
      "  index_name: String\n",
      "  index_group_no: Int64\n",
      "  index_group_name: String\n",
      "  section_no: Int64\n",
      "  section_name: String\n",
      "  garment_group_no: Int64\n",
      "  garment_group_name: String\n",
      "  detail_desc: String\n"
     ]
    }
   ],
   "source": [
    "# Generate comprehensive summary statistics\n",
    "print(f\"\\n• Dataset Summary Statistics:\")\n",
    "\n",
    "# Get schema information\n",
    "schema_info = df_cleaned.schema\n",
    "numerical_columns = [col for col, dtype in schema_info.items() \n",
    "                    if dtype in [pl.Int64, pl.Int32, pl.Float64, pl.Float32]]\n",
    "\n",
    "if numerical_columns:\n",
    "    print(f\"\\nSummary statistics for numerical columns:\")\n",
    "    \n",
    "    # Generate comprehensive statistics efficiently\n",
    "    stats_expr = []\n",
    "    for col in numerical_columns[:5]:  # Limit to first 5 for readability\n",
    "        stats_expr.extend([\n",
    "            pl.col(col).count().alias(f\"{col}_count\"),\n",
    "            pl.col(col).mean().alias(f\"{col}_mean\"),\n",
    "            pl.col(col).std().alias(f\"{col}_std\"),\n",
    "            pl.col(col).min().alias(f\"{col}_min\"),\n",
    "            pl.col(col).max().alias(f\"{col}_max\")\n",
    "        ])\n",
    "    \n",
    "    summary_stats = df_cleaned.select(stats_expr)\n",
    "    \n",
    "    # Display statistics in a readable format\n",
    "    for col in numerical_columns[:5]:\n",
    "        stats = summary_stats.select([f\"{col}_count\", f\"{col}_mean\", f\"{col}_std\", f\"{col}_min\", f\"{col}_max\"]).to_pandas().iloc[0]\n",
    "        print(f\"\\n{col}:\")\n",
    "        print(f\"  Count: {stats[f'{col}_count']:,.0f}\")\n",
    "        print(f\"  Mean:  {stats[f'{col}_mean']:.2f}\")\n",
    "        print(f\"  Std:   {stats[f'{col}_std']:.2f}\")\n",
    "        print(f\"  Min:   {stats[f'{col}_min']:.2f}\")\n",
    "        print(f\"  Max:   {stats[f'{col}_max']:.2f}\")\n",
    "else:\n",
    "    print(\"No numerical columns found for summary statistics\")\n",
    "\n",
    "# Data type summary\n",
    "print(f\"\\nData types summary:\")\n",
    "for col, dtype in schema_info.items():\n",
    "    print(f\"  {col}: {dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Processing Summary and Performance Metrics\n",
    "\n",
    "Compile and display comprehensive preprocessing results with performance metrics showcasing Polars' efficiency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "PREPROCESSING PIPELINE COMPLETED SUCCESSFULLY\n",
      "============================================================\n",
      "\n",
      "✓ Processing Summary:\n",
      "  - Original records: 3,178,832\n",
      "  - Final records: 3,141,366\n",
      "  - Duplicates removed: 37,466\n",
      "  - Missing value columns handled: 6\n",
      "  - Outliers capped: 154,382\n",
      "  - Data quality: High\n",
      "  - Memory usage: 2023.9 MB\n",
      "  - Framework: Polars 1.32.0\n",
      "\n",
      "✓ Dataset ready for downstream analysis and modelling\n",
      "✓ Cleaned dataset available as 'df_cleaned'\n",
      "✓ Processing summary available as 'processing_summary'\n"
     ]
    }
   ],
   "source": [
    "# Compile comprehensive processing summary\n",
    "processing_summary = {\n",
    "    'original_record_count': total_records,\n",
    "    'final_record_count': final_record_count,\n",
    "    'duplicates_removed': duplicate_count,\n",
    "    'missing_values_imputed': len(missing_summary),\n",
    "    'outliers_handled': outlier_count if 'price' in df_cleaned.columns else 0,\n",
    "    'remaining_nulls': remaining_nulls_total,\n",
    "    'data_integrity_passed': remaining_nulls_total == 0,\n",
    "    'memory_usage_mb': df_cleaned.estimated_size('mb'),\n",
    "    'processing_framework': f'Polars {pl.__version__}'\n",
    "}\n",
    "\n",
    "print(f\"\\n\" + \"=\" * 60)\n",
    "print(\"PREPROCESSING PIPELINE COMPLETED SUCCESSFULLY\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "print(f\"\\n✓ Processing Summary:\")\n",
    "print(f\"  - Original records: {processing_summary['original_record_count']:,}\")\n",
    "print(f\"  - Final records: {processing_summary['final_record_count']:,}\")\n",
    "print(f\"  - Duplicates removed: {processing_summary['duplicates_removed']:,}\")\n",
    "print(f\"  - Missing value columns handled: {processing_summary['missing_values_imputed']}\")\n",
    "print(f\"  - Outliers capped: {processing_summary['outliers_handled']:,}\")\n",
    "print(f\"  - Data quality: {'High' if processing_summary['data_integrity_passed'] else 'Needs attention'}\")\n",
    "print(f\"  - Memory usage: {processing_summary['memory_usage_mb']:.1f} MB\")\n",
    "print(f\"  - Framework: {processing_summary['processing_framework']}\")\n",
    "\n",
    "print(f\"\\n✓ Dataset ready for downstream analysis and modelling\")\n",
    "print(f\"✓ Cleaned dataset available as 'df_cleaned'\")\n",
    "print(f\"✓ Processing summary available as 'processing_summary'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Data Preview and Validation\n",
    "\n",
    "Display a sample of the cleaned dataset to verify preprocessing results and demonstrate data quality improvements.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of cleaned dataset:\n",
      "\n",
      "First 10 records:\n",
      "shape: (10, 7)\n",
      "┌────────────────┬────────────┬──────────┬────────────────┬──────┬────────────────┬────────────────┐\n",
      "│ customer_id    ┆ article_id ┆ price    ┆ sales_channel_ ┆ age  ┆ club_member_st ┆ product_type_n │\n",
      "│ ---            ┆ ---        ┆ ---      ┆ id             ┆ ---  ┆ atus           ┆ ame            │\n",
      "│ str            ┆ i64        ┆ f64      ┆ ---            ┆ f64  ┆ ---            ┆ ---            │\n",
      "│                ┆            ┆          ┆ i64            ┆      ┆ str            ┆ str            │\n",
      "╞════════════════╪════════════╪══════════╪════════════════╪══════╪════════════════╪════════════════╡\n",
      "│ 26ffd54842e734 ┆ 685417006  ┆ 0.013542 ┆ 2              ┆ 25.0 ┆ ACTIVE         ┆ Dress          │\n",
      "│ 80b6186e90bc0f ┆            ┆          ┆                ┆      ┆                ┆                │\n",
      "│ 9c…            ┆            ┆          ┆                ┆      ┆                ┆                │\n",
      "│ 3a11e2e731bb08 ┆ 903846001  ┆ 0.02439  ┆ 2              ┆ 33.0 ┆ ACTIVE         ┆ Other          │\n",
      "│ 18c9e6df9b09ef ┆            ┆          ┆                ┆      ┆                ┆ accessories    │\n",
      "│ 0f…            ┆            ┆          ┆                ┆      ┆                ┆                │\n",
      "│ 10998fa81f88b3 ┆ 562251007  ┆ 0.015237 ┆ 2              ┆ 26.0 ┆ ACTIVE         ┆ Trousers       │\n",
      "│ 5b0c7ebbe92495 ┆            ┆          ┆                ┆      ┆                ┆                │\n",
      "│ b2…            ┆            ┆          ┆                ┆      ┆                ┆                │\n",
      "│ 394f127cdf87c8 ┆ 827687002  ┆ 0.011847 ┆ 1              ┆ 23.0 ┆ ACTIVE         ┆ Trousers       │\n",
      "│ 9ae172ad38224e ┆            ┆          ┆                ┆      ┆                ┆                │\n",
      "│ 75…            ┆            ┆          ┆                ┆      ┆                ┆                │\n",
      "│ 8a06d7a646e1f8 ┆ 760281004  ┆ 0.042356 ┆ 2              ┆ 50.0 ┆ ACTIVE         ┆ Blouse         │\n",
      "│ df7a800b9c2328 ┆            ┆          ┆                ┆      ┆                ┆                │\n",
      "│ ab…            ┆            ┆          ┆                ┆      ┆                ┆                │\n",
      "│ 246de6617b344d ┆ 547780020  ┆ 0.025407 ┆ 1              ┆ 20.0 ┆ ACTIVE         ┆ Trousers       │\n",
      "│ d5fdd9c1a319a1 ┆            ┆          ┆                ┆      ┆                ┆                │\n",
      "│ d5…            ┆            ┆          ┆                ┆      ┆                ┆                │\n",
      "│ 50d04a4dd49284 ┆ 630116023  ┆ 0.019814 ┆ 2              ┆ 24.0 ┆ ACTIVE         ┆ Leggings/Tight │\n",
      "│ f04cd7137eb796 ┆            ┆          ┆                ┆      ┆                ┆ s              │\n",
      "│ 35…            ┆            ┆          ┆                ┆      ┆                ┆                │\n",
      "│ dcc1ffdc166385 ┆ 684340003  ┆ 0.030492 ┆ 1              ┆ 28.0 ┆ ACTIVE         ┆ Bikini top     │\n",
      "│ 8a1d2c973bb805 ┆            ┆          ┆                ┆      ┆                ┆                │\n",
      "│ ef…            ┆            ┆          ┆                ┆      ┆                ┆                │\n",
      "│ 8af2db94853f3e ┆ 771329016  ┆ 0.022017 ┆ 2              ┆ 22.0 ┆ ACTIVE         ┆ Top            │\n",
      "│ c6207e44590788 ┆            ┆          ┆                ┆      ┆                ┆                │\n",
      "│ b8…            ┆            ┆          ┆                ┆      ┆                ┆                │\n",
      "│ 9699c2aafba7dc ┆ 507533002  ┆ 0.016932 ┆ 2              ┆ 43.0 ┆ ACTIVE         ┆ Sweater        │\n",
      "│ 40e2fe87b3851f ┆            ┆          ┆                ┆      ┆                ┆                │\n",
      "│ 40…            ┆            ┆          ┆                ┆      ┆                ┆                │\n",
      "└────────────────┴────────────┴──────────┴────────────────┴──────┴────────────────┴────────────────┘\n",
      "\n",
      "• Data Quality Verification:\n",
      "  - Total null values: 0\n",
      "  - Data completeness: 100.00%\n",
      "  - Dataset shape: 3,141,366 rows × 35 columns\n"
     ]
    }
   ],
   "source": [
    "# Show sample of cleaned data\n",
    "print(f\"Sample of cleaned dataset:\")\n",
    "sample_cols = [\"customer_id\", \"article_id\", \"price\", \"sales_channel_id\", \"age\", \"club_member_status\", \"product_type_name\"]\n",
    "available_cols = [col for col in sample_cols if col in df_cleaned.columns]\n",
    "\n",
    "print(f\"\\nFirst 10 records:\")\n",
    "print(df_cleaned.select(available_cols).head(10))\n",
    "\n",
    "# Verify data quality improvements\n",
    "print(f\"\\n• Data Quality Verification:\")\n",
    "final_null_check = df_cleaned.null_count()\n",
    "total_nulls = sum(final_null_check.row(0))\n",
    "print(f\"  - Total null values: {total_nulls}\")\n",
    "print(f\"  - Data completeness: {((df_cleaned.height * len(df_cleaned.columns) - total_nulls) / (df_cleaned.height * len(df_cleaned.columns)) * 100):.2f}%\")\n",
    "print(f\"  - Dataset shape: {df_cleaned.height:,} rows × {len(df_cleaned.columns)} columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Advanced Analytics Preparation\n",
    "\n",
    "Prepare the cleaned dataset for advanced analytics by creating derived features and performing initial segmentation preparation using Polars' powerful expression API.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "• Preparing for Advanced Analytics:\n",
      "  ✓ Customer transaction summary created (822,211 customers)\n",
      "  ✓ Ready for RFM analysis and customer segmentation\n",
      "  ✓ Product performance summary created (128 product types)\n",
      "  ✓ Ready for product recommendation analysis\n",
      "\n",
      "✓ Dataset optimised for machine learning and advanced analytics\n"
     ]
    }
   ],
   "source": [
    "# Optional: Create derived features for analytics\n",
    "print(f\"\\n• Preparing for Advanced Analytics:\")\n",
    "\n",
    "# Customer transaction summary (useful for segmentation)\n",
    "if all(col in df_cleaned.columns for col in ['customer_id', 'price']):\n",
    "    customer_summary = (\n",
    "        df_cleaned\n",
    "        .group_by('customer_id')\n",
    "        .agg([\n",
    "            pl.col('price').count().alias('transaction_count'),\n",
    "            pl.col('price').sum().alias('total_spent'),\n",
    "            pl.col('price').mean().alias('avg_transaction_value'),\n",
    "            pl.col('article_id').n_unique().alias('unique_items_purchased')\n",
    "        ])\n",
    "        .sort('total_spent', descending=True)\n",
    "    )\n",
    "    \n",
    "    print(f\"  ✓ Customer transaction summary created ({customer_summary.height:,} customers)\")\n",
    "    print(f\"  ✓ Ready for RFM analysis and customer segmentation\")\n",
    "\n",
    "# Product performance summary\n",
    "if 'product_type_name' in df_cleaned.columns:\n",
    "    product_summary = (\n",
    "        df_cleaned\n",
    "        .group_by('product_type_name')\n",
    "        .agg([\n",
    "            pl.col('customer_id').count().alias('sales_volume'),\n",
    "            pl.col('price').mean().alias('avg_price'),\n",
    "            pl.col('customer_id').n_unique().alias('unique_customers')\n",
    "        ])\n",
    "        .sort('sales_volume', descending=True)\n",
    "    )\n",
    "    \n",
    "    print(f\"  ✓ Product performance summary created ({product_summary.height:,} product types)\")\n",
    "    print(f\"  ✓ Ready for product recommendation analysis\")\n",
    "\n",
    "print(f\"\\n✓ Dataset optimised for machine learning and advanced analytics\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
