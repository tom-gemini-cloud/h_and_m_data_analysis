{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# H&M Feature Engineering\n",
    "\n",
    "This notebook performs comprehensiv feature engineering on the H&M dataset to extract relevant features for machine learning models. The engineered features focus on customer behaviour, product preferences, demographics, and temporal patterns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries and Setup\n",
    "\n",
    "Import necessary libraries for feature engineering and data processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully\n",
      "Polars version: 0.20.31\n",
      "Pandas version: 2.2.0\n",
      "NumPy version: 1.26.4\n",
      "Current memory usage: 4822.1 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4822.10546875"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "import os\n",
    "import warnings\n",
    "import gc\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Configure Polars for memory efficiency\n",
    "try:\n",
    "    pl.Config.set_streaming_chunk_size(5000)  # Reduced chunk size\n",
    "    pl.Config.set_fmt_str_lengths(50)\n",
    "    if hasattr(pl.Config, 'set_table_width'):\n",
    "        pl.Config.set_table_width(120)\n",
    "except AttributeError:\n",
    "    pass\n",
    "\n",
    "# Memory monitoring function\n",
    "def check_memory_usage():\n",
    "    import psutil\n",
    "    process = psutil.Process(os.getpid())\n",
    "    memory_mb = process.memory_info().rss / 1024 / 1024\n",
    "    print(f\"Current memory usage: {memory_mb:.1f} MB\")\n",
    "    return memory_mb\n",
    "\n",
    "print(f\"Libraries imported successfully\")\n",
    "print(f\"Polars version: {pl.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "check_memory_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset\n",
    "\n",
    "Load the integrated H&M dataset that was created in the data exploration phase.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading integrated dataset with memory optimization...\n",
      "Analyzing dataset structure...\n",
      "âœ“ Found 35 columns\n",
      "âœ“ Sample loaded: 1,000 records\n",
      "âœ“ Full dataset: 3,178,832 records with 35 columns\n",
      "âœ“ Memory usage: 1634.5 MB\n",
      "\n",
      "Dataset Overview:\n",
      "â€¢ Records: 3,178,832\n",
      "â€¢ Unique customers: 821,920\n",
      "â€¢ Unique articles: 87,121\n",
      "Sample dates: ['2019-05-02', '2019-04-26', '2020-03-07']\n",
      "â€¢ Date range: 2018-09-20 00:00:00 to 2020-09-22 00:00:00\n",
      "â€¢ Reference date for recency: 2020-09-22\n",
      "Current memory usage: 4046.4 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4046.40625"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load integrated dataset with memory optimization\n",
    "data_dir = '../data'\n",
    "integrated_path = os.path.join(data_dir, 'processed', 'hm_integrated_dataset.parquet')\n",
    "\n",
    "if os.path.exists(integrated_path):\n",
    "    print(\"Loading integrated dataset with memory optimization...\")\n",
    "    \n",
    "    # Load with lazy evaluation for memory efficiency\n",
    "    df_integrated = pl.scan_parquet(integrated_path)\n",
    "    \n",
    "    # Get basic info without loading everything into memory\n",
    "    print(\"Analyzing dataset structure...\")\n",
    "    schema_info = df_integrated.schema\n",
    "    print(f\"âœ“ Found {len(schema_info)} columns\")\n",
    "    \n",
    "    # Sample to check data without loading full dataset\n",
    "    sample_df = df_integrated.head(1000).collect()\n",
    "    print(f\"âœ“ Sample loaded: {sample_df.height:,} records\")\n",
    "    \n",
    "    # Get estimated size\n",
    "    full_df = df_integrated.collect()\n",
    "    print(f\"âœ“ Full dataset: {full_df.height:,} records with {len(full_df.columns)} columns\")\n",
    "    print(f\"âœ“ Memory usage: {full_df.estimated_size('mb'):.1f} MB\")\n",
    "    \n",
    "    # Free up the sample\n",
    "    del sample_df\n",
    "    gc.collect()\n",
    "    \n",
    "else:\n",
    "    print(\"Integrated dataset not found. Please run the data exploration notebook first.\")\n",
    "    raise FileNotFoundError(f\"File not found: {integrated_path}\")\n",
    "\n",
    "# Display basic information\n",
    "print(f\"\\nDataset Overview:\")\n",
    "print(f\"â€¢ Records: {full_df.height:,}\")\n",
    "print(f\"â€¢ Unique customers: {full_df['customer_id'].n_unique():,}\")\n",
    "print(f\"â€¢ Unique articles: {full_df['article_id'].n_unique():,}\")\n",
    "\n",
    "# Check date range for temporal features - optimized\n",
    "if 't_dat' in full_df.columns:\n",
    "    print(f\"Sample dates: {full_df.select('t_dat').head(3).to_pandas()['t_dat'].tolist()}\")\n",
    "    \n",
    "    # Get date range efficiently\n",
    "    date_stats = full_df.select([\n",
    "        pl.col('t_dat').str.to_date().min().alias('min_date'),\n",
    "        pl.col('t_dat').str.to_date().max().alias('max_date')\n",
    "    ]).to_pandas().iloc[0]\n",
    "    \n",
    "    print(f\"â€¢ Date range: {date_stats['min_date']} to {date_stats['max_date']}\")\n",
    "    \n",
    "    # Set reference date for recency calculations\n",
    "    REFERENCE_DATE = pd.to_datetime(date_stats['max_date'])\n",
    "    print(f\"â€¢ Reference date for recency: {REFERENCE_DATE.date()}\")\n",
    "\n",
    "# Store the main dataframe for processing\n",
    "df_integrated = full_df\n",
    "del full_df\n",
    "gc.collect()\n",
    "check_memory_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customer Behavioural Features (RFM Analysis)\n",
    "\n",
    "Create features based on customer purchasing behaviour including Recency, Frequency, and Monetary value analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating customer behavioural features with memory optimization...\n",
      "Current memory usage: 4046.4 MB\n",
      "Processing RFM features in batches...\n",
      "Processing 821,920 customers in batches of 50,000\n",
      "Processing batch 1/17: customers 0 to 50,000\n",
      "Processing batch 2/17: customers 50,000 to 100,000\n",
      "Processing batch 3/17: customers 100,000 to 150,000\n",
      "Processing batch 4/17: customers 150,000 to 200,000\n",
      "Processing batch 5/17: customers 200,000 to 250,000\n",
      "Current memory usage: 4000.5 MB\n",
      "  Memory usage: 4000.5 MB (change: +-45.9 MB)\n",
      "Processing batch 6/17: customers 250,000 to 300,000\n",
      "Processing batch 7/17: customers 300,000 to 350,000\n",
      "Processing batch 8/17: customers 350,000 to 400,000\n",
      "Processing batch 9/17: customers 400,000 to 450,000\n",
      "Processing batch 10/17: customers 450,000 to 500,000\n",
      "Current memory usage: 4041.9 MB\n",
      "  Memory usage: 4041.9 MB (change: +-4.5 MB)\n",
      "Processing batch 11/17: customers 500,000 to 550,000\n",
      "Processing batch 12/17: customers 550,000 to 600,000\n",
      "Processing batch 13/17: customers 600,000 to 650,000\n",
      "Processing batch 14/17: customers 650,000 to 700,000\n",
      "Processing batch 15/17: customers 700,000 to 750,000\n",
      "Current memory usage: 4095.4 MB\n",
      "  Memory usage: 4095.4 MB (change: +49.0 MB)\n",
      "Processing batch 16/17: customers 750,000 to 800,000\n",
      "Processing batch 17/17: customers 800,000 to 821,920\n",
      "Combining RFM batches...\n",
      "âœ“ Created RFM features for 821,920 customers\n",
      "\n",
      "RFM Summary Statistics (sample):\n",
      "  â€¢ avg_recency: 270.44\n",
      "  â€¢ avg_frequency: 3.92\n",
      "  â€¢ avg_monetary: 0.11\n",
      "\n",
      "Sample RFM Features:\n",
      "                                         customer_id  recency_days  frequency  \\\n",
      "0  0785996d681149d11a659aaee406948b789c022dcef7ce...             6         55   \n",
      "1  09988ee85da298c6e84d6476b5229a171667e470b9cc6f...           404          3   \n",
      "2  d49c01584d628e321c8065c055847d0690d8b7435109ad...           437          4   \n",
      "\n",
      "   monetary_value  avg_transaction_value  unique_products_purchased  \\\n",
      "0        2.105966               0.038290                         54   \n",
      "1        0.093169               0.031056                          3   \n",
      "2        0.081458               0.020364                          4   \n",
      "\n",
      "   spending_variability  customer_lifespan_days first_purchase_date  \\\n",
      "0              0.025036                     716          2018-10-01   \n",
      "1              0.017641                     168          2019-02-28   \n",
      "2              0.008525                     128          2019-03-07   \n",
      "\n",
      "  last_purchase_date  purchase_frequency_per_day  \n",
      "0         2020-09-16                    0.076816  \n",
      "1         2019-08-15                    0.017857  \n",
      "2         2019-07-13                    0.031250  \n",
      "Current memory usage: 4080.8 MB\n",
      "Memory after RFM processing: 4080.8 MB\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating customer behavioural features with memory optimization...\")\n",
    "\n",
    "# Clear memory first\n",
    "gc.collect()\n",
    "initial_memory = check_memory_usage()\n",
    "\n",
    "# Prepare temporal data efficiently - only select needed columns\n",
    "df_temporal = df_integrated.select([\n",
    "    'customer_id', 'article_id', 'price', 't_dat'\n",
    "]).with_columns([\n",
    "    pl.col('t_dat').str.to_date().alias('transaction_date')\n",
    "])\n",
    "\n",
    "print(\"Processing RFM features in batches...\")\n",
    "\n",
    "# Get unique customers and process in batches to manage memory\n",
    "unique_customers = df_temporal.select('customer_id').unique().to_pandas()['customer_id'].tolist()\n",
    "total_customers = len(unique_customers)\n",
    "batch_size = 50000  # Process 50k customers at a time\n",
    "\n",
    "print(f\"Processing {total_customers:,} customers in batches of {batch_size:,}\")\n",
    "\n",
    "# Set reference date\n",
    "reference_date = pd.to_datetime('2020-09-22')\n",
    "\n",
    "# Initialize list to store batch results\n",
    "rfm_batches = []\n",
    "\n",
    "for i in range(0, total_customers, batch_size):\n",
    "    batch_customers = unique_customers[i:i+batch_size]\n",
    "    print(f\"Processing batch {i//batch_size + 1}/{(total_customers-1)//batch_size + 1}: customers {i:,} to {min(i+batch_size, total_customers):,}\")\n",
    "    \n",
    "    # Filter data for current batch\n",
    "    batch_data = df_temporal.filter(pl.col('customer_id').is_in(batch_customers))\n",
    "    \n",
    "    # Calculate RFM for this batch\n",
    "    batch_rfm = (\n",
    "        batch_data\n",
    "        .group_by('customer_id')\n",
    "        .agg([\n",
    "            # Recency: Days since last purchase\n",
    "            ((pl.lit(reference_date) - pl.col('transaction_date').max()).dt.total_days()).alias('recency_days'),\n",
    "            \n",
    "            # Frequency: Number of transactions\n",
    "            pl.col('transaction_date').count().alias('frequency'),\n",
    "            \n",
    "            # Monetary: Total amount spent\n",
    "            pl.col('price').sum().alias('monetary_value'),\n",
    "            \n",
    "            # Additional behavioural metrics\n",
    "            pl.col('price').mean().alias('avg_transaction_value'),\n",
    "            pl.col('article_id').n_unique().alias('unique_products_purchased'),\n",
    "            pl.col('price').std().alias('spending_variability'),\n",
    "            \n",
    "            # Temporal behaviour\n",
    "            ((pl.col('transaction_date').max() - pl.col('transaction_date').min()).dt.total_days()).alias('customer_lifespan_days'),\n",
    "            pl.col('transaction_date').min().alias('first_purchase_date'),\n",
    "            pl.col('transaction_date').max().alias('last_purchase_date')\n",
    "        ])\n",
    "    )\n",
    "    \n",
    "    # Apply safe calculations\n",
    "    batch_rfm = batch_rfm.with_columns([\n",
    "        # Purchases per day (safely handling division by zero)\n",
    "        pl.when(pl.col('customer_lifespan_days') > 0)\n",
    "        .then(pl.col('frequency').cast(pl.Float64) / pl.col('customer_lifespan_days').cast(pl.Float64))\n",
    "        .otherwise(pl.col('frequency').cast(pl.Float64))\n",
    "        .alias('purchase_frequency_per_day'),\n",
    "        \n",
    "        # Fill null spending variability with 0\n",
    "        pl.col('spending_variability').fill_null(0.0).alias('spending_variability'),\n",
    "        \n",
    "        # Ensure recency is positive\n",
    "        pl.col('recency_days').abs().alias('recency_days')\n",
    "    ])\n",
    "    \n",
    "    rfm_batches.append(batch_rfm)\n",
    "    \n",
    "    # Clear batch data\n",
    "    del batch_data\n",
    "    gc.collect()\n",
    "    \n",
    "    # Monitor memory every few batches\n",
    "    if (i // batch_size + 1) % 5 == 0:\n",
    "        current_memory = check_memory_usage()\n",
    "        print(f\"  Memory usage: {current_memory:.1f} MB (change: +{current_memory - initial_memory:.1f} MB)\")\n",
    "\n",
    "# Combine all batches\n",
    "print(\"Combining RFM batches...\")\n",
    "customer_rfm = pl.concat(rfm_batches)\n",
    "del rfm_batches\n",
    "gc.collect()\n",
    "\n",
    "print(f\"âœ“ Created RFM features for {customer_rfm.height:,} customers\")\n",
    "\n",
    "# Calculate summary statistics on a sample to avoid memory issues\n",
    "sample_rfm = customer_rfm.head(10000)\n",
    "rfm_stats = sample_rfm.select([\n",
    "    pl.col('recency_days').mean().alias('avg_recency'),\n",
    "    pl.col('frequency').mean().alias('avg_frequency'),\n",
    "    pl.col('monetary_value').mean().alias('avg_monetary')\n",
    "]).to_pandas().iloc[0]\n",
    "\n",
    "print(f\"\\nRFM Summary Statistics (sample):\")\n",
    "for metric, value in rfm_stats.items():\n",
    "    print(f\"  â€¢ {metric}: {value:.2f}\")\n",
    "\n",
    "# Display sample of RFM features\n",
    "print(f\"\\nSample RFM Features:\")\n",
    "print(customer_rfm.head(3).to_pandas())\n",
    "\n",
    "# Clear temporal data to free memory\n",
    "del df_temporal\n",
    "gc.collect()\n",
    "final_memory = check_memory_usage()\n",
    "print(f\"Memory after RFM processing: {final_memory:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Product Preference Features\n",
    "\n",
    "Engineer features related to customer product preferences, including category affinity and brand loyalty.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating product preference features with memory optimization...\n",
      "Current memory usage: 4078.8 MB\n",
      "Processing product preferences...\n",
      "â€¢ Processing category preferences...\n",
      "Current memory usage: 4170.3 MB\n",
      "  Memory usage: 4170.3 MB\n",
      "â€¢ Processing department preferences...\n",
      "Current memory usage: 4219.6 MB\n",
      "  Memory usage: 4219.6 MB\n",
      "â€¢ Processing colour preferences...\n",
      "Current memory usage: 4236.7 MB\n",
      "  Memory usage: 4236.7 MB\n",
      "â€¢ Processing price behaviour...\n",
      "Current memory usage: 4301.5 MB\n",
      "  Memory usage: 4301.5 MB\n",
      "â€¢ Processing channel preferences...\n",
      "âœ“ Created product preference features\n",
      "  â€¢ Category preferences: 821,920 customers\n",
      "  â€¢ Department preferences: 821,920 customers\n",
      "  â€¢ Colour preferences: 821,920 customers\n",
      "  â€¢ Price behaviour: 821,920 customers\n",
      "  â€¢ Channel preferences: 821,920 customers\n",
      "Current memory usage: 4549.0 MB\n",
      "Final memory usage: 4549.0 MB (change: +470.3 MB)\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating product preference features with memory optimization...\")\n",
    "\n",
    "# Clear memory and monitor\n",
    "gc.collect()\n",
    "initial_memory = check_memory_usage()\n",
    "\n",
    "# Select only needed columns for product preferences\n",
    "product_data = df_integrated.select([\n",
    "    'customer_id', 'product_type_name', 'department_name', \n",
    "    'colour_group_name', 'price', 'sales_channel_id'\n",
    "])\n",
    "\n",
    "print(\"Processing product preferences...\")\n",
    "\n",
    "# Customer product category preferences - optimized\n",
    "print(\"â€¢ Processing category preferences...\")\n",
    "category_preferences = (\n",
    "    product_data\n",
    "    .group_by(['customer_id', 'product_type_name'])\n",
    "    .agg([\n",
    "        pl.col('price').count().alias('category_purchase_count'),\n",
    "        pl.col('price').sum().alias('category_total_spent')\n",
    "    ])\n",
    "    .group_by('customer_id')\n",
    "    .agg([\n",
    "        # Most purchased category\n",
    "        pl.col('product_type_name').first().alias('most_purchased_category'),\n",
    "        \n",
    "        # Number of different categories purchased\n",
    "        pl.col('product_type_name').n_unique().alias('category_diversity'),\n",
    "        \n",
    "        # Concentration ratio (top category percentage)\n",
    "        (pl.col('category_purchase_count').max() / pl.col('category_purchase_count').sum()).alias('category_concentration')\n",
    "    ])\n",
    ")\n",
    "\n",
    "gc.collect()\n",
    "print(f\"  Memory usage: {check_memory_usage():.1f} MB\")\n",
    "\n",
    "# Department preferences - optimized\n",
    "print(\"â€¢ Processing department preferences...\")\n",
    "dept_preferences = (\n",
    "    product_data\n",
    "    .group_by(['customer_id', 'department_name'])\n",
    "    .agg(pl.col('price').count().alias('dept_purchase_count'))\n",
    "    .group_by('customer_id')\n",
    "    .agg([\n",
    "        pl.col('department_name').first().alias('preferred_department'),\n",
    "        pl.col('department_name').n_unique().alias('department_diversity')\n",
    "    ])\n",
    ")\n",
    "\n",
    "gc.collect()\n",
    "print(f\"  Memory usage: {check_memory_usage():.1f} MB\")\n",
    "\n",
    "# Colour preferences - optimized\n",
    "print(\"â€¢ Processing colour preferences...\")\n",
    "colour_preferences = (\n",
    "    product_data\n",
    "    .group_by(['customer_id', 'colour_group_name'])\n",
    "    .agg(pl.col('price').count().alias('colour_purchase_count'))\n",
    "    .group_by('customer_id')\n",
    "    .agg([\n",
    "        pl.col('colour_group_name').first().alias('preferred_colour'),\n",
    "        pl.col('colour_group_name').n_unique().alias('colour_diversity')\n",
    "    ])\n",
    ")\n",
    "\n",
    "gc.collect()\n",
    "print(f\"  Memory usage: {check_memory_usage():.1f} MB\")\n",
    "\n",
    "# Price sensitivity features - using streaming approach\n",
    "print(\"â€¢ Processing price behaviour...\")\n",
    "price_behaviour = (\n",
    "    product_data\n",
    "    .group_by('customer_id')\n",
    "    .agg([\n",
    "        pl.col('price').min().alias('min_price_paid'),\n",
    "        pl.col('price').max().alias('max_price_paid'),\n",
    "        pl.col('price').quantile(0.25).alias('price_q1'),\n",
    "        pl.col('price').quantile(0.75).alias('price_q3'),\n",
    "        pl.col('price').median().alias('median_price_paid')\n",
    "    ])\n",
    "    .with_columns([\n",
    "        (pl.col('max_price_paid') - pl.col('min_price_paid')).alias('price_range'),\n",
    "        (pl.col('price_q3') - pl.col('price_q1')).alias('price_iqr')\n",
    "    ])\n",
    ")\n",
    "\n",
    "gc.collect()\n",
    "print(f\"  Memory usage: {check_memory_usage():.1f} MB\")\n",
    "\n",
    "# Sales channel preferences - optimized\n",
    "print(\"â€¢ Processing channel preferences...\")\n",
    "channel_preferences = (\n",
    "    product_data\n",
    "    .group_by(['customer_id', 'sales_channel_id'])\n",
    "    .agg(pl.col('price').count().alias('channel_purchases'))\n",
    "    .with_columns([\n",
    "        pl.when(pl.col('sales_channel_id') == 1)\n",
    "        .then(pl.lit('online'))\n",
    "        .otherwise(pl.lit('store'))\n",
    "        .alias('channel_type')\n",
    "    ])\n",
    "    .group_by('customer_id')\n",
    "    .agg([\n",
    "        pl.col('channel_type').first().alias('preferred_channel'),\n",
    "        pl.col('sales_channel_id').n_unique().alias('channel_diversity'),\n",
    "        \n",
    "        # Calculate online percentage\n",
    "        (pl.when(pl.col('channel_type') == 'online')\n",
    "         .then(pl.col('channel_purchases'))\n",
    "         .otherwise(0).sum() / pl.col('channel_purchases').sum()).alias('online_purchase_ratio')\n",
    "    ])\n",
    ")\n",
    "\n",
    "# Clear product data to free memory\n",
    "del product_data\n",
    "gc.collect()\n",
    "\n",
    "print(f\"âœ“ Created product preference features\")\n",
    "print(f\"  â€¢ Category preferences: {category_preferences.height:,} customers\")\n",
    "print(f\"  â€¢ Department preferences: {dept_preferences.height:,} customers\")\n",
    "print(f\"  â€¢ Colour preferences: {colour_preferences.height:,} customers\")\n",
    "print(f\"  â€¢ Price behaviour: {price_behaviour.height:,} customers\")\n",
    "print(f\"  â€¢ Channel preferences: {channel_preferences.height:,} customers\")\n",
    "\n",
    "final_memory = check_memory_usage()\n",
    "print(f\"Final memory usage: {final_memory:.1f} MB (change: +{final_memory - initial_memory:.1f} MB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demographic and Temporal Features\n",
    "\n",
    "Create features based on customer demographics and temporal purchasing patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating demographic and temporal features with memory optimization...\n",
      "Current memory usage: 4549.0 MB\n",
      "Current memory usage: 4470.7 MB\n",
      "  Memory after demographics: 4470.7 MB\n",
      "Processing temporal features...\n",
      "âœ“ Created demographic features for 821,920 customers\n",
      "âœ“ Created temporal features for 821,920 customers\n",
      "\n",
      "Age Group Distribution:\n",
      "  â€¢ 25-34: 260,173 customers\n",
      "  â€¢ 18-24: 201,076 customers\n",
      "  â€¢ 45-54: 154,680 customers\n",
      "  â€¢ 35-44: 101,115 customers\n",
      "  â€¢ 55-64: 78,276 customers\n",
      "  â€¢ 65+: 26,600 customers\n",
      "Current memory usage: 4640.5 MB\n",
      "Final memory usage: 4640.5 MB (change: +91.5 MB)\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating demographic and temporal features with memory optimization...\")\n",
    "\n",
    "# Clear memory and monitor\n",
    "gc.collect()\n",
    "initial_memory = check_memory_usage()\n",
    "\n",
    "# Demographic features from customer data - select needed columns only\n",
    "demographic_data = df_integrated.select([\n",
    "    'customer_id', 'age', 'club_member_status', 'fashion_news_frequency', 'FN', 'Active'\n",
    "]).unique(subset=['customer_id'])\n",
    "\n",
    "demographic_features = (\n",
    "    demographic_data\n",
    "    .with_columns([\n",
    "        # Age groups\n",
    "        pl.when(pl.col('age') < 25).then(pl.lit('18-24'))\n",
    "        .when(pl.col('age') < 35).then(pl.lit('25-34'))\n",
    "        .when(pl.col('age') < 45).then(pl.lit('35-44'))\n",
    "        .when(pl.col('age') < 55).then(pl.lit('45-54'))\n",
    "        .when(pl.col('age') < 65).then(pl.lit('55-64'))\n",
    "        .otherwise(pl.lit('65+'))\n",
    "        .alias('age_group'),\n",
    "        \n",
    "        # Fashion engagement level\n",
    "        pl.when(pl.col('fashion_news_frequency') == 'Regularly').then(pl.lit('high'))\n",
    "        .when(pl.col('fashion_news_frequency') == 'Monthly').then(pl.lit('medium'))\n",
    "        .otherwise(pl.lit('low'))\n",
    "        .alias('fashion_engagement')\n",
    "    ])\n",
    ")\n",
    "\n",
    "# Clean up demographic data\n",
    "del demographic_data\n",
    "gc.collect()\n",
    "print(f\"  Memory after demographics: {check_memory_usage():.1f} MB\")\n",
    "\n",
    "# Temporal purchasing patterns - recreate temporal data efficiently\n",
    "print(\"Processing temporal features...\")\n",
    "temporal_data = df_integrated.select([\n",
    "    'customer_id', 't_dat'\n",
    "]).with_columns([\n",
    "    pl.col('t_dat').str.to_date().alias('transaction_date')\n",
    "]).with_columns([\n",
    "    pl.col('transaction_date').dt.year().alias('year'),\n",
    "    pl.col('transaction_date').dt.month().alias('month'),\n",
    "    pl.col('transaction_date').dt.weekday().alias('weekday'),\n",
    "    pl.col('transaction_date').dt.quarter().alias('quarter')\n",
    "])\n",
    "\n",
    "temporal_features = (\n",
    "    temporal_data\n",
    "    .group_by('customer_id')\n",
    "    .agg([\n",
    "        # Seasonal preferences - use mode or most frequent\n",
    "        pl.col('quarter').mode().first().alias('preferred_quarter'),\n",
    "        pl.col('month').mode().first().alias('preferred_month'), \n",
    "        pl.col('weekday').mode().first().alias('preferred_weekday'),\n",
    "        \n",
    "        # Shopping pattern diversity\n",
    "        pl.col('quarter').n_unique().alias('quarter_diversity'),\n",
    "        pl.col('month').n_unique().alias('month_diversity'),\n",
    "        pl.col('weekday').n_unique().alias('weekday_diversity'),\n",
    "        \n",
    "        # Activity span\n",
    "        (pl.col('year').max() - pl.col('year').min() + 1).alias('active_years')\n",
    "    ])\n",
    "    .with_columns([\n",
    "        # Weekend vs weekday preference\n",
    "        pl.when(pl.col('preferred_weekday').is_in([6, 7]))\n",
    "        .then(pl.lit('weekend'))\n",
    "        .otherwise(pl.lit('weekday'))\n",
    "        .alias('weekend_preference'),\n",
    "        \n",
    "        # Season mapping\n",
    "        pl.when(pl.col('preferred_quarter') == 1).then(pl.lit('winter'))\n",
    "        .when(pl.col('preferred_quarter') == 2).then(pl.lit('spring'))\n",
    "        .when(pl.col('preferred_quarter') == 3).then(pl.lit('summer'))\n",
    "        .otherwise(pl.lit('autumn'))\n",
    "        .alias('preferred_season')\n",
    "    ])\n",
    ")\n",
    "\n",
    "# Clean up temporal data\n",
    "del temporal_data\n",
    "gc.collect()\n",
    "\n",
    "print(f\"âœ“ Created demographic features for {demographic_features.height:,} customers\")\n",
    "print(f\"âœ“ Created temporal features for {temporal_features.height:,} customers\")\n",
    "\n",
    "# Display age group distribution\n",
    "age_dist = demographic_features.group_by('age_group').agg(pl.count().alias('count')).sort('count', descending=True)\n",
    "print(f\"\\nAge Group Distribution:\")\n",
    "for row in age_dist.head(6).to_dicts():\n",
    "    print(f\"  â€¢ {row['age_group']}: {row['count']:,} customers\")\n",
    "\n",
    "final_memory = check_memory_usage()\n",
    "print(f\"Final memory usage: {final_memory:.1f} MB (change: +{final_memory - initial_memory:.1f} MB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Features (Product Descriptions)\n",
    "\n",
    "Apply TF-IDF to extract features from product text data if available.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"Creating text features from product descriptions with memory optimization...\")\n\n# Clear memory and monitor\ngc.collect()\ninitial_memory = check_memory_usage()\n\n# Check if we have text columns for TF-IDF\ntext_columns = ['product_type_name', 'product_group_name', 'department_name', 'colour_group_name']\navailable_text_cols = [col for col in text_columns if col in df_integrated.columns]\n\nif available_text_cols:\n    print(f\"Available text columns: {available_text_cols}\")\n    \n    # Select only needed text columns to reduce memory usage\n    text_data = df_integrated.select(['customer_id'] + available_text_cols)\n    \n    # Create combined text features for each customer - compatible with Polars 0.20.31\n    print(\"Processing customer text profiles...\")\n    customer_text_features = (\n        text_data\n        .group_by('customer_id')\n        .agg([\n            # Concatenate unique values using str.concat (compatible with 0.20.31)\n            pl.col('product_type_name').unique().str.concat(' ').alias('purchased_product_types'),\n            pl.col('product_group_name').unique().str.concat(' ').alias('purchased_product_groups'),\n            pl.col('department_name').unique().str.concat(' ').alias('purchased_departments'),\n            pl.col('colour_group_name').unique().str.concat(' ').alias('purchased_colours')\n        ])\n        .with_columns([\n            # Combine all text into a single customer profile\n            pl.concat_str([\n                pl.col('purchased_product_types'),\n                pl.col('purchased_product_groups'),\n                pl.col('purchased_departments'),\n                pl.col('purchased_colours')\n            ], separator=' ').alias('customer_text_profile')\n        ])\n    )\n    \n    # Clean up text data\n    del text_data\n    gc.collect()\n    print(f\"  Memory after text aggregation: {check_memory_usage():.1f} MB\")\n    \n    # Convert to pandas for sklearn TF-IDF (process in smaller batches if needed)\n    print(\"Converting to pandas for TF-IDF...\")\n    customer_text_pd = customer_text_features.to_pandas()\n    \n    # Apply TF-IDF to customer text profiles\n    print(\"Applying TF-IDF vectorisation...\")\n    \n    # Configure TF-IDF with reduced features for memory efficiency\n    tfidf = TfidfVectorizer(\n        max_features=50,  # Reduced from 100 to 50 for memory efficiency\n        stop_words='english',\n        lowercase=True,\n        ngram_range=(1, 1),  # Only unigrams to reduce memory\n        min_df=10,  # Increased minimum document frequency\n        max_df=0.9  # Reduced maximum document frequency\n    )\n    \n    try:\n        # Fit and transform the customer text profiles\n        print(\"Fitting TF-IDF...\")\n        tfidf_matrix = tfidf.fit_transform(customer_text_pd['customer_text_profile'].fillna(''))\n        \n        # Get feature names\n        feature_names = tfidf.get_feature_names_out()\n        \n        print(f\"  TF-IDF matrix shape: {tfidf_matrix.shape}\")\n        print(f\"  Memory usage: {check_memory_usage():.1f} MB\")\n        \n        # Apply PCA immediately to reduce memory footprint\n        print(\"Applying PCA to reduce dimensionality...\")\n        pca = PCA(n_components=min(15, len(feature_names)), random_state=42)\n        tfidf_pca = pca.fit_transform(tfidf_matrix.toarray())\n        \n        # Create PCA feature DataFrame\n        pca_df = pd.DataFrame(\n            tfidf_pca,\n            columns=[f'tfidf_pca_{i+1}' for i in range(tfidf_pca.shape[1])],\n            index=customer_text_pd['customer_id']\n        ).reset_index()\n        \n        tfidf_pca_features = pl.from_pandas(pca_df)\n        \n        print(f\"âœ“ Created {tfidf_pca.shape[1]} PCA components explaining {pca.explained_variance_ratio_.sum():.2%} of variance\")\n        print(f\"  Top 5 TF-IDF features: {list(feature_names[:5])}\")\n        \n        # Clean up intermediate variables\n        del tfidf_matrix, pca_df, customer_text_pd\n        gc.collect()\n        \n    except Exception as e:\n        print(f\"Warning: TF-IDF processing failed due to memory constraints: {e}\")\n        print(\"Skipping TF-IDF features to continue with other features...\")\n        tfidf_pca_features = None\n    \n    # Clean up text features\n    del customer_text_features\n    gc.collect()\n    \nelse:\n    print(\"No suitable text columns found for TF-IDF analysis\")\n    tfidf_pca_features = None\n\nfinal_memory = check_memory_usage()\nprint(f\"Final memory usage: {final_memory:.1f} MB (change: +{final_memory - initial_memory:.1f} MB)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interaction Features\n",
    "\n",
    "Create derived features by combining existing features to capture complex relationships.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating interaction features...\")\n",
    "\n",
    "# Start with RFM as base for interactions\n",
    "interaction_features = customer_rfm.select([\n",
    "    'customer_id', 'recency_days', 'frequency', 'monetary_value', \n",
    "    'avg_transaction_value', 'unique_products_purchased'\n",
    "])\n",
    "\n",
    "# Add key features from other datasets\n",
    "if 'age' in demographic_features.columns:\n",
    "    interaction_features = interaction_features.join(\n",
    "        demographic_features.select(['customer_id', 'age']),\n",
    "        on='customer_id',\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "# Create interaction features\n",
    "interaction_features = interaction_features.with_columns([\n",
    "    # RFM score (simple scoring using percentile ranks)\n",
    "    (pl.col('recency_days').rank(\"min\") * 0.3 + \n",
    "     pl.col('frequency').rank(\"min\") * 0.4 + \n",
    "     pl.col('monetary_value').rank(\"min\") * 0.3).alias('rfm_score'),\n",
    "    \n",
    "    # Customer value segments\n",
    "    (pl.col('frequency') * pl.col('avg_transaction_value')).alias('customer_value'),\n",
    "    \n",
    "    # Product diversity ratio\n",
    "    (pl.col('unique_products_purchased').cast(pl.Float64) / pl.col('frequency').cast(pl.Float64)).alias('product_diversity_ratio'),\n",
    "    \n",
    "    # Spending efficiency (monetary per transaction)\n",
    "    (pl.col('monetary_value') / pl.col('frequency')).alias('spending_efficiency')\n",
    "])\n",
    "\n",
    "# Add age-related interactions if age is available\n",
    "if 'age' in interaction_features.columns:\n",
    "    interaction_features = interaction_features.with_columns([\n",
    "        # Age-spending interaction\n",
    "        (pl.col('age') * pl.col('avg_transaction_value')).alias('age_spending_interaction'),\n",
    "        \n",
    "        # Age-frequency interaction\n",
    "        (pl.col('age') * pl.col('frequency')).alias('age_frequency_interaction')\n",
    "    ])\n",
    "\n",
    "# Create customer segments using quantile-based approach instead of qcut\n",
    "# First calculate quantiles\n",
    "recency_quantiles = interaction_features.select(pl.col('recency_days').quantile([0.25, 0.5, 0.75]))\n",
    "frequency_quantiles = interaction_features.select(pl.col('frequency').quantile([0.25, 0.5, 0.75]))\n",
    "monetary_quantiles = interaction_features.select(pl.col('monetary_value').quantile([0.25, 0.5, 0.75]))\n",
    "\n",
    "# Extract quantile values\n",
    "recency_q25, recency_q50, recency_q75 = recency_quantiles.to_pandas().iloc[0].values\n",
    "frequency_q25, frequency_q50, frequency_q75 = frequency_quantiles.to_pandas().iloc[0].values  \n",
    "monetary_q25, monetary_q50, monetary_q75 = monetary_quantiles.to_pandas().iloc[0].values\n",
    "\n",
    "# Create segments using when/then logic\n",
    "interaction_features = interaction_features.with_columns([\n",
    "    # Recency segments (lower recency = more recent = better)\n",
    "    pl.when(pl.col('recency_days') <= recency_q25).then(pl.lit('recent'))\n",
    "    .when(pl.col('recency_days') <= recency_q50).then(pl.lit('moderate'))\n",
    "    .when(pl.col('recency_days') <= recency_q75).then(pl.lit('old'))\n",
    "    .otherwise(pl.lit('very_old'))\n",
    "    .alias('recency_segment'),\n",
    "    \n",
    "    # Frequency segments (higher frequency = better)\n",
    "    pl.when(pl.col('frequency') >= frequency_q75).then(pl.lit('very_high'))\n",
    "    .when(pl.col('frequency') >= frequency_q50).then(pl.lit('high'))\n",
    "    .when(pl.col('frequency') >= frequency_q25).then(pl.lit('moderate'))\n",
    "    .otherwise(pl.lit('low'))\n",
    "    .alias('frequency_segment'),\n",
    "    \n",
    "    # Monetary segments (higher monetary = better)\n",
    "    pl.when(pl.col('monetary_value') >= monetary_q75).then(pl.lit('very_high'))\n",
    "    .when(pl.col('monetary_value') >= monetary_q50).then(pl.lit('high'))\n",
    "    .when(pl.col('monetary_value') >= monetary_q25).then(pl.lit('moderate'))\n",
    "    .otherwise(pl.lit('low'))\n",
    "    .alias('monetary_segment')\n",
    "])\n",
    "\n",
    "print(f\"âœ“ Created interaction features for {interaction_features.height:,} customers\")\n",
    "print(f\"\\nInteraction Feature Summary:\")\n",
    "print(f\"  â€¢ RFM score range: {interaction_features['rfm_score'].min():.2f} - {interaction_features['rfm_score'].max():.2f}\")\n",
    "print(f\"  â€¢ Customer value range: Â£{interaction_features['customer_value'].min():.2f} - Â£{interaction_features['customer_value'].max():.2f}\")\n",
    "print(f\"  â€¢ Product diversity ratio: {interaction_features['product_diversity_ratio'].mean():.3f} (avg)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine All Features\n",
    "\n",
    "Merge all engineered features into a comprehensive customer feature dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Combining all engineered features...\")\n",
    "\n",
    "# Start with customer RFM features as the base\n",
    "final_features = customer_rfm\n",
    "\n",
    "# Join product preference features\n",
    "final_features = final_features.join(category_preferences, on='customer_id', how='left')\n",
    "final_features = final_features.join(dept_preferences, on='customer_id', how='left')\n",
    "final_features = final_features.join(colour_preferences, on='customer_id', how='left')\n",
    "final_features = final_features.join(price_behaviour, on='customer_id', how='left')\n",
    "final_features = final_features.join(channel_preferences, on='customer_id', how='left')\n",
    "\n",
    "# Join demographic and temporal features\n",
    "final_features = final_features.join(demographic_features, on='customer_id', how='left')\n",
    "final_features = final_features.join(temporal_features, on='customer_id', how='left')\n",
    "\n",
    "# Join TF-IDF features if available\n",
    "if tfidf_pca_features is not None:\n",
    "    final_features = final_features.join(tfidf_pca_features, on='customer_id', how='left')\n",
    "    print(f\"âœ“ Added TF-IDF PCA features\")\n",
    "\n",
    "# Join interaction features (excluding duplicates)\n",
    "interaction_cols_to_add = [\n",
    "    'customer_id', 'rfm_score', 'customer_value', 'product_diversity_ratio', \n",
    "    'spending_efficiency', 'recency_segment', 'frequency_segment', 'monetary_segment'\n",
    "]\n",
    "\n",
    "# Add age interactions if available\n",
    "if 'age_spending_interaction' in interaction_features.columns:\n",
    "    interaction_cols_to_add.extend(['age_spending_interaction', 'age_frequency_interaction'])\n",
    "\n",
    "final_features = final_features.join(\n",
    "    interaction_features.select(interaction_cols_to_add), \n",
    "    on='customer_id', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(f\"\\nâœ“ Final feature dataset created!\")\n",
    "print(f\"  â€¢ Total customers: {final_features.height:,}\")\n",
    "print(f\"  â€¢ Total features: {len(final_features.columns)}\")\n",
    "print(f\"  â€¢ Memory usage: {final_features.estimated_size('mb'):.1f} MB\")\n",
    "\n",
    "# Display feature categories\n",
    "feature_categories = {\n",
    "    'RFM Features': ['recency_days', 'frequency', 'monetary_value', 'avg_transaction_value'],\n",
    "    'Product Preferences': ['most_purchased_category', 'category_diversity', 'preferred_department'],\n",
    "    'Demographics': ['age', 'age_group', 'club_member_status', 'fashion_engagement'],\n",
    "    'Temporal': ['preferred_season', 'weekend_preference', 'active_years'],\n",
    "    'Interaction': ['rfm_score', 'customer_value', 'product_diversity_ratio']\n",
    "}\n",
    "\n",
    "print(f\"\\nFeature Categories:\")\n",
    "for category, features in feature_categories.items():\n",
    "    available_features = [f for f in features if f in final_features.columns]\n",
    "    print(f\"  â€¢ {category}: {len(available_features)} features\")\n",
    "\n",
    "# Display sample of final features\n",
    "print(f\"\\nSample of Final Features:\")\n",
    "sample_features = final_features.head(3)\n",
    "for col in final_features.columns[:10]:  # Show first 10 columns\n",
    "    print(f\"  â€¢ {col}: {sample_features[col].to_list()}\")\n",
    "if len(final_features.columns) > 10:\n",
    "    print(f\"  ... and {len(final_features.columns) - 10} more features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Quality Analysis\n",
    "\n",
    "Analyse the quality and distribution of engineered features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Analysing feature quality...\")\n",
    "\n",
    "# Check for missing values\n",
    "null_counts = final_features.null_count()\n",
    "total_records = final_features.height\n",
    "\n",
    "missing_analysis = []\n",
    "for col_name in final_features.columns:\n",
    "    if col_name != 'customer_id':\n",
    "        missing_count = null_counts[col_name][0]\n",
    "        missing_percentage = (missing_count / total_records) * 100\n",
    "        if missing_percentage > 0:\n",
    "            missing_analysis.append({\n",
    "                'feature': col_name,\n",
    "                'missing_count': missing_count,\n",
    "                'missing_percentage': missing_percentage\n",
    "            })\n",
    "\n",
    "if missing_analysis:\n",
    "    missing_df = pd.DataFrame(missing_analysis).sort_values('missing_percentage', ascending=False)\n",
    "    print(f\"\\nFeatures with Missing Values:\")\n",
    "    for _, row in missing_df.head(10).iterrows():\n",
    "        print(f\"  â€¢ {row['feature']}: {row['missing_percentage']:.1f}% ({row['missing_count']:,} records)\")\n",
    "else:\n",
    "    print(f\"\\nâœ“ No missing values detected in engineered features!\")\n",
    "\n",
    "# Analyse numerical feature distributions\n",
    "numerical_features = [\n",
    "    'recency_days', 'frequency', 'monetary_value', 'avg_transaction_value',\n",
    "    'unique_products_purchased', 'category_diversity', 'rfm_score'\n",
    "]\n",
    "\n",
    "available_numerical = [f for f in numerical_features if f in final_features.columns]\n",
    "\n",
    "if available_numerical:\n",
    "    print(f\"\\nNumerical Feature Statistics:\")\n",
    "    stats = final_features.select(available_numerical).describe().to_pandas()\n",
    "    \n",
    "    for feature in available_numerical[:5]:  # Show first 5 for brevity\n",
    "        mean_val = final_features[feature].mean()\n",
    "        std_val = final_features[feature].std()\n",
    "        print(f\"  â€¢ {feature}: Î¼={mean_val:.2f}, Ïƒ={std_val:.2f}\")\n",
    "\n",
    "# Analyse categorical feature distributions\n",
    "categorical_features = [\n",
    "    'age_group', 'club_member_status', 'preferred_channel', \n",
    "    'preferred_season', 'recency_segment', 'frequency_segment'\n",
    "]\n",
    "\n",
    "available_categorical = [f for f in categorical_features if f in final_features.columns]\n",
    "\n",
    "if available_categorical:\n",
    "    print(f\"\\nCategorical Feature Distributions:\")\n",
    "    for feature in available_categorical[:3]:  # Show first 3 for brevity\n",
    "        dist = final_features.group_by(feature).agg(pl.count().alias('count')).sort('count', descending=True)\n",
    "        top_categories = dist.head(3).to_dicts()\n",
    "        print(f\"  â€¢ {feature}: {', '.join([f\"{cat[feature]}({cat['count']})\" for cat in top_categories])}...\")\n",
    "\n",
    "print(f\"\\nâœ“ Feature quality analysis completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Engineered Features\n",
    "\n",
    "Save the final engineered feature dataset for use in machine learning models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Saving engineered features...\")\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "output_dir = os.path.join(data_dir, 'processed')\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save main feature dataset\n",
    "features_path = os.path.join(output_dir, 'hm_engineered_features.parquet')\n",
    "final_features.write_parquet(features_path)\n",
    "print(f\"âœ“ Saved engineered features to: {features_path}\")\n",
    "\n",
    "# Save individual feature components for analysis\n",
    "components = {\n",
    "    'rfm_features': customer_rfm,\n",
    "    'product_preferences': category_preferences,\n",
    "    'demographic_features': demographic_features,\n",
    "    'temporal_features': temporal_features\n",
    "}\n",
    "\n",
    "for name, dataset in components.items():\n",
    "    component_path = os.path.join(output_dir, f'hm_{name}.parquet')\n",
    "    dataset.write_parquet(component_path)\n",
    "    print(f\"âœ“ Saved {name} to: {component_path}\")\n",
    "\n",
    "# Save TF-IDF features separately if they exist\n",
    "if tfidf_pca_features is not None:\n",
    "    tfidf_path = os.path.join(output_dir, 'hm_tfidf_features.parquet')\n",
    "    tfidf_pca_features.write_parquet(tfidf_path)\n",
    "    print(f\"âœ“ Saved TF-IDF features to: {tfidf_path}\")\n",
    "\n",
    "# Create feature documentation\n",
    "feature_docs = {\n",
    "    'dataset_info': {\n",
    "        'total_customers': final_features.height,\n",
    "        'total_features': len(final_features.columns),\n",
    "        'creation_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'memory_usage_mb': final_features.estimated_size('mb')\n",
    "    },\n",
    "    'feature_categories': {\n",
    "        'behavioural_rfm': ['recency_days', 'frequency', 'monetary_value', 'avg_transaction_value'],\n",
    "        'product_preferences': ['most_purchased_category', 'category_diversity', 'preferred_department'],\n",
    "        'demographics': ['age', 'age_group', 'club_member_status', 'fashion_engagement'],\n",
    "        'temporal_patterns': ['preferred_season', 'weekend_preference', 'active_years'],\n",
    "        'interactions': ['rfm_score', 'customer_value', 'product_diversity_ratio']\n",
    "    }\n",
    "}\n",
    "\n",
    "import json\n",
    "docs_path = os.path.join(output_dir, 'feature_documentation.json')\n",
    "with open(docs_path, 'w') as f:\n",
    "    json.dump(feature_docs, f, indent=2)\n",
    "print(f\"âœ“ Saved feature documentation to: {docs_path}\")\n",
    "\n",
    "print(f\"\\nðŸŽ‰ Feature engineering completed successfully!\")\n",
    "print(f\"\\nSummary:\")\n",
    "print(f\"  â€¢ Customers processed: {final_features.height:,}\")\n",
    "print(f\"  â€¢ Features created: {len(final_features.columns)}\")\n",
    "print(f\"  â€¢ Output files: {len(components) + 2} datasets saved\")\n",
    "print(f\"  â€¢ Ready for machine learning model training!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}