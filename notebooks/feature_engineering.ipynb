{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# H&M Feature Engineering\n",
    "\n",
    "This notebook performs comprehensiv feature engineering on the H&M dataset to extract relevant features for machine learning models. The engineered features focus on customer behaviour, product preferences, demographics, and temporal patterns.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries and Setup\n",
    "\n",
    "Import necessary libraries for feature engineering and data processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully\n",
      "Polars version: 1.32.0\n",
      "Pandas version: 2.2.0\n",
      "NumPy version: 1.26.4\n",
      "Current memory usage: 111.8 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "111.75"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "import os\n",
    "import warnings\n",
    "import gc\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Configure Polars for memory efficiency\n",
    "try:\n",
    "    pl.Config.set_streaming_chunk_size(5000)  # Reduced chunk size\n",
    "    pl.Config.set_fmt_str_lengths(50)\n",
    "    if hasattr(pl.Config, 'set_table_width'):\n",
    "        pl.Config.set_table_width(120)\n",
    "except AttributeError:\n",
    "    pass\n",
    "\n",
    "# Memory monitoring function\n",
    "def check_memory_usage():\n",
    "    import psutil\n",
    "    process = psutil.Process(os.getpid())\n",
    "    memory_mb = process.memory_info().rss / 1024 / 1024\n",
    "    print(f\"Current memory usage: {memory_mb:.1f} MB\")\n",
    "    return memory_mb\n",
    "\n",
    "print(f\"Libraries imported successfully\")\n",
    "print(f\"Polars version: {pl.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "check_memory_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Dataset\n",
    "\n",
    "Load the integrated H&M dataset that was created in the data exploration phase.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading integrated dataset with memory optimization...\n",
      "Analyzing dataset structure...\n",
      "✓ Found 35 columns\n",
      "✓ Sample loaded: 1,000 records\n",
      "✓ Full dataset: 3,178,832 records with 35 columns\n",
      "✓ Memory usage: 1636.3 MB\n",
      "\n",
      "Dataset Overview:\n",
      "• Records: 3,178,832\n",
      "• Unique customers: 822,211\n",
      "• Unique articles: 86,988\n",
      "Sample dates: ['2020-05-18', '2019-05-12', '2020-09-08']\n",
      "• Date range: 2018-09-20 00:00:00 to 2020-09-22 00:00:00\n",
      "• Reference date for recency: 2020-09-22\n",
      "Current memory usage: 2168.4 MB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2168.40625"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load integrated dataset with memory optimization\n",
    "data_dir = '../data'\n",
    "integrated_path = os.path.join(data_dir, 'processed', 'hm_integrated_dataset.parquet')\n",
    "\n",
    "if os.path.exists(integrated_path):\n",
    "    print(\"Loading integrated dataset with memory optimization...\")\n",
    "    \n",
    "    # Load with lazy evaluation for memory efficiency\n",
    "    df_integrated = pl.scan_parquet(integrated_path)\n",
    "    \n",
    "    # Get basic info without loading everything into memory\n",
    "    print(\"Analyzing dataset structure...\")\n",
    "    schema_info = df_integrated.schema\n",
    "    print(f\"✓ Found {len(schema_info)} columns\")\n",
    "    \n",
    "    # Sample to check data without loading full dataset\n",
    "    sample_df = df_integrated.head(1000).collect()\n",
    "    print(f\"✓ Sample loaded: {sample_df.height:,} records\")\n",
    "    \n",
    "    # Get estimated size\n",
    "    full_df = df_integrated.collect()\n",
    "    print(f\"✓ Full dataset: {full_df.height:,} records with {len(full_df.columns)} columns\")\n",
    "    print(f\"✓ Memory usage: {full_df.estimated_size('mb'):.1f} MB\")\n",
    "    \n",
    "    # Free up the sample\n",
    "    del sample_df\n",
    "    gc.collect()\n",
    "    \n",
    "else:\n",
    "    print(\"Integrated dataset not found. Please run the data exploration notebook first.\")\n",
    "    raise FileNotFoundError(f\"File not found: {integrated_path}\")\n",
    "\n",
    "# Display basic information\n",
    "print(f\"\\nDataset Overview:\")\n",
    "print(f\"• Records: {full_df.height:,}\")\n",
    "print(f\"• Unique customers: {full_df['customer_id'].n_unique():,}\")\n",
    "print(f\"• Unique articles: {full_df['article_id'].n_unique():,}\")\n",
    "\n",
    "# Check date range for temporal features - optimized\n",
    "if 't_dat' in full_df.columns:\n",
    "    print(f\"Sample dates: {full_df.select('t_dat').head(3).to_pandas()['t_dat'].tolist()}\")\n",
    "    \n",
    "    # Get date range efficiently\n",
    "    date_stats = full_df.select([\n",
    "        pl.col('t_dat').str.to_date().min().alias('min_date'),\n",
    "        pl.col('t_dat').str.to_date().max().alias('max_date')\n",
    "    ]).to_pandas().iloc[0]\n",
    "    \n",
    "    print(f\"• Date range: {date_stats['min_date']} to {date_stats['max_date']}\")\n",
    "    \n",
    "    # Set reference date for recency calculations\n",
    "    REFERENCE_DATE = pd.to_datetime(date_stats['max_date'])\n",
    "    print(f\"• Reference date for recency: {REFERENCE_DATE.date()}\")\n",
    "\n",
    "# Store the main dataframe for processing\n",
    "df_integrated = full_df\n",
    "del full_df\n",
    "gc.collect()\n",
    "check_memory_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Customer Behavioural Features (RFM Analysis)\n",
    "\n",
    "Create features based on customer purchasing behaviour including Recency, Frequency, and Monetary value analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating customer behavioural features with memory optimization...\n",
      "Current memory usage: 2161.8 MB\n",
      "Processing RFM features in batches...\n",
      "Processing 822,211 customers in batches of 50,000\n",
      "Processing batch 1/17: customers 0 to 50,000\n",
      "Processing batch 2/17: customers 50,000 to 100,000\n",
      "Processing batch 3/17: customers 100,000 to 150,000\n",
      "Processing batch 4/17: customers 150,000 to 200,000\n",
      "Processing batch 5/17: customers 200,000 to 250,000\n",
      "Current memory usage: 2496.8 MB\n",
      "  Memory usage: 2496.8 MB (change: +334.9 MB)\n",
      "Processing batch 6/17: customers 250,000 to 300,000\n",
      "Processing batch 7/17: customers 300,000 to 350,000\n",
      "Processing batch 8/17: customers 350,000 to 400,000\n",
      "Processing batch 9/17: customers 400,000 to 450,000\n",
      "Processing batch 10/17: customers 450,000 to 500,000\n",
      "Current memory usage: 2527.7 MB\n",
      "  Memory usage: 2527.7 MB (change: +365.8 MB)\n",
      "Processing batch 11/17: customers 500,000 to 550,000\n",
      "Processing batch 12/17: customers 550,000 to 600,000\n",
      "Processing batch 13/17: customers 600,000 to 650,000\n",
      "Processing batch 14/17: customers 650,000 to 700,000\n",
      "Processing batch 15/17: customers 700,000 to 750,000\n",
      "Current memory usage: 2550.3 MB\n",
      "  Memory usage: 2550.3 MB (change: +388.4 MB)\n",
      "Processing batch 16/17: customers 750,000 to 800,000\n",
      "Processing batch 17/17: customers 800,000 to 822,211\n",
      "Combining RFM batches...\n",
      "✓ Created RFM features for 822,211 customers\n",
      "\n",
      "RFM Summary Statistics (sample):\n",
      "  • avg_recency: 273.73\n",
      "  • avg_frequency: 3.86\n",
      "  • avg_monetary: 0.11\n",
      "\n",
      "Sample RFM Features:\n",
      "                                         customer_id  recency_days  frequency  \\\n",
      "0  c5ead8322e1bb92b4d779bd3da8029933dbf0dab8ecbee...           174          2   \n",
      "1  37aab04a0edc482deb4995b113d53a9ce4ce64b4c84656...           138          1   \n",
      "2  9d7ac305fad67e32843a80947e0ce76be9833e61eb109e...           616          2   \n",
      "\n",
      "   monetary_value  avg_transaction_value  unique_products_purchased  \\\n",
      "0        0.030475               0.015237                          2   \n",
      "1        0.023712               0.023712                          1   \n",
      "2        0.048780               0.024390                          2   \n",
      "\n",
      "   spending_variability  customer_lifespan_days first_purchase_date  \\\n",
      "0              0.002397                     544          2018-10-05   \n",
      "1              0.000000                       0          2020-05-07   \n",
      "2              0.013423                     110          2018-09-27   \n",
      "\n",
      "  last_purchase_date  purchase_frequency_per_day  \n",
      "0         2020-04-01                    0.003676  \n",
      "1         2020-05-07                    1.000000  \n",
      "2         2019-01-15                    0.018182  \n",
      "Current memory usage: 2559.6 MB\n",
      "Memory after RFM processing: 2559.6 MB\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating customer behavioural features with memory optimization...\")\n",
    "\n",
    "# Clear memory first\n",
    "gc.collect()\n",
    "initial_memory = check_memory_usage()\n",
    "\n",
    "# Prepare temporal data efficiently - only select needed columns\n",
    "df_temporal = df_integrated.select([\n",
    "    'customer_id', 'article_id', 'price', 't_dat'\n",
    "]).with_columns([\n",
    "    pl.col('t_dat').str.to_date().alias('transaction_date')\n",
    "])\n",
    "\n",
    "print(\"Processing RFM features in batches...\")\n",
    "\n",
    "# Get unique customers and process in batches to manage memory\n",
    "unique_customers = df_temporal.select('customer_id').unique().to_pandas()['customer_id'].tolist()\n",
    "total_customers = len(unique_customers)\n",
    "batch_size = 50000  # Process 50k customers at a time\n",
    "\n",
    "print(f\"Processing {total_customers:,} customers in batches of {batch_size:,}\")\n",
    "\n",
    "# Set reference date\n",
    "reference_date = pd.to_datetime('2020-09-22')\n",
    "\n",
    "# Initialize list to store batch results\n",
    "rfm_batches = []\n",
    "\n",
    "for i in range(0, total_customers, batch_size):\n",
    "    batch_customers = unique_customers[i:i+batch_size]\n",
    "    print(f\"Processing batch {i//batch_size + 1}/{(total_customers-1)//batch_size + 1}: customers {i:,} to {min(i+batch_size, total_customers):,}\")\n",
    "    \n",
    "    # Filter data for current batch\n",
    "    batch_data = df_temporal.filter(pl.col('customer_id').is_in(batch_customers))\n",
    "    \n",
    "    # Calculate RFM for this batch\n",
    "    batch_rfm = (\n",
    "        batch_data\n",
    "        .group_by('customer_id')\n",
    "        .agg([\n",
    "            # Recency: Days since last purchase\n",
    "            ((pl.lit(reference_date) - pl.col('transaction_date').max()).dt.total_days()).alias('recency_days'),\n",
    "            \n",
    "            # Frequency: Number of transactions\n",
    "            pl.col('transaction_date').count().alias('frequency'),\n",
    "            \n",
    "            # Monetary: Total amount spent\n",
    "            pl.col('price').sum().alias('monetary_value'),\n",
    "            \n",
    "            # Additional behavioural metrics\n",
    "            pl.col('price').mean().alias('avg_transaction_value'),\n",
    "            pl.col('article_id').n_unique().alias('unique_products_purchased'),\n",
    "            pl.col('price').std().alias('spending_variability'),\n",
    "            \n",
    "            # Temporal behaviour\n",
    "            ((pl.col('transaction_date').max() - pl.col('transaction_date').min()).dt.total_days()).alias('customer_lifespan_days'),\n",
    "            pl.col('transaction_date').min().alias('first_purchase_date'),\n",
    "            pl.col('transaction_date').max().alias('last_purchase_date')\n",
    "        ])\n",
    "    )\n",
    "    \n",
    "    # Apply safe calculations\n",
    "    batch_rfm = batch_rfm.with_columns([\n",
    "        # Purchases per day (safely handling division by zero)\n",
    "        pl.when(pl.col('customer_lifespan_days') > 0)\n",
    "        .then(pl.col('frequency').cast(pl.Float64) / pl.col('customer_lifespan_days').cast(pl.Float64))\n",
    "        .otherwise(pl.col('frequency').cast(pl.Float64))\n",
    "        .alias('purchase_frequency_per_day'),\n",
    "        \n",
    "        # Fill null spending variability with 0\n",
    "        pl.col('spending_variability').fill_null(0.0).alias('spending_variability'),\n",
    "        \n",
    "        # Ensure recency is positive\n",
    "        pl.col('recency_days').abs().alias('recency_days')\n",
    "    ])\n",
    "    \n",
    "    rfm_batches.append(batch_rfm)\n",
    "    \n",
    "    # Clear batch data\n",
    "    del batch_data\n",
    "    gc.collect()\n",
    "    \n",
    "    # Monitor memory every few batches\n",
    "    if (i // batch_size + 1) % 5 == 0:\n",
    "        current_memory = check_memory_usage()\n",
    "        print(f\"  Memory usage: {current_memory:.1f} MB (change: +{current_memory - initial_memory:.1f} MB)\")\n",
    "\n",
    "# Combine all batches\n",
    "print(\"Combining RFM batches...\")\n",
    "customer_rfm = pl.concat(rfm_batches)\n",
    "del rfm_batches\n",
    "gc.collect()\n",
    "\n",
    "print(f\"✓ Created RFM features for {customer_rfm.height:,} customers\")\n",
    "\n",
    "# Calculate summary statistics on a sample to avoid memory issues\n",
    "sample_rfm = customer_rfm.head(10000)\n",
    "rfm_stats = sample_rfm.select([\n",
    "    pl.col('recency_days').mean().alias('avg_recency'),\n",
    "    pl.col('frequency').mean().alias('avg_frequency'),\n",
    "    pl.col('monetary_value').mean().alias('avg_monetary')\n",
    "]).to_pandas().iloc[0]\n",
    "\n",
    "print(f\"\\nRFM Summary Statistics (sample):\")\n",
    "for metric, value in rfm_stats.items():\n",
    "    print(f\"  • {metric}: {value:.2f}\")\n",
    "\n",
    "# Display sample of RFM features\n",
    "print(f\"\\nSample RFM Features:\")\n",
    "print(customer_rfm.head(3).to_pandas())\n",
    "\n",
    "# Clear temporal data to free memory\n",
    "del df_temporal\n",
    "gc.collect()\n",
    "final_memory = check_memory_usage()\n",
    "print(f\"Memory after RFM processing: {final_memory:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Product Preference Features\n",
    "\n",
    "Engineer features related to customer product preferences, including category affinity and brand loyalty.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating product preference features with memory optimization...\n",
      "Current memory usage: 2560.5 MB\n",
      "Processing product preferences...\n",
      "• Processing category preferences...\n",
      "Current memory usage: 2073.4 MB\n",
      "  Memory usage: 2073.4 MB\n",
      "• Processing department preferences...\n",
      "Current memory usage: 2281.4 MB\n",
      "  Memory usage: 2281.4 MB\n",
      "• Processing colour preferences...\n",
      "Current memory usage: 2468.4 MB\n",
      "  Memory usage: 2468.4 MB\n",
      "• Processing price behaviour...\n",
      "Current memory usage: 2551.0 MB\n",
      "  Memory usage: 2551.0 MB\n",
      "• Processing channel preferences...\n",
      "✓ Created product preference features\n",
      "  • Category preferences: 822,211 customers\n",
      "  • Department preferences: 822,211 customers\n",
      "  • Colour preferences: 822,211 customers\n",
      "  • Price behaviour: 822,211 customers\n",
      "  • Channel preferences: 822,211 customers\n",
      "Current memory usage: 2685.9 MB\n",
      "Final memory usage: 2685.9 MB (change: +125.4 MB)\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating product preference features with memory optimization...\")\n",
    "\n",
    "# Clear memory and monitor\n",
    "gc.collect()\n",
    "initial_memory = check_memory_usage()\n",
    "\n",
    "# Select only needed columns for product preferences\n",
    "product_data = df_integrated.select([\n",
    "    'customer_id', 'product_type_name', 'department_name', \n",
    "    'colour_group_name', 'price', 'sales_channel_id'\n",
    "])\n",
    "\n",
    "print(\"Processing product preferences...\")\n",
    "\n",
    "# Customer product category preferences - optimized\n",
    "print(\"• Processing category preferences...\")\n",
    "category_preferences = (\n",
    "    product_data\n",
    "    .group_by(['customer_id', 'product_type_name'])\n",
    "    .agg([\n",
    "        pl.col('price').count().alias('category_purchase_count'),\n",
    "        pl.col('price').sum().alias('category_total_spent')\n",
    "    ])\n",
    "    .group_by('customer_id')\n",
    "    .agg([\n",
    "        # Most purchased category\n",
    "        pl.col('product_type_name').first().alias('most_purchased_category'),\n",
    "        \n",
    "        # Number of different categories purchased\n",
    "        pl.col('product_type_name').n_unique().alias('category_diversity'),\n",
    "        \n",
    "        # Concentration ratio (top category percentage)\n",
    "        (pl.col('category_purchase_count').max() / pl.col('category_purchase_count').sum()).alias('category_concentration')\n",
    "    ])\n",
    ")\n",
    "\n",
    "gc.collect()\n",
    "print(f\"  Memory usage: {check_memory_usage():.1f} MB\")\n",
    "\n",
    "# Department preferences - optimized\n",
    "print(\"• Processing department preferences...\")\n",
    "dept_preferences = (\n",
    "    product_data\n",
    "    .group_by(['customer_id', 'department_name'])\n",
    "    .agg(pl.col('price').count().alias('dept_purchase_count'))\n",
    "    .group_by('customer_id')\n",
    "    .agg([\n",
    "        pl.col('department_name').first().alias('preferred_department'),\n",
    "        pl.col('department_name').n_unique().alias('department_diversity')\n",
    "    ])\n",
    ")\n",
    "\n",
    "gc.collect()\n",
    "print(f\"  Memory usage: {check_memory_usage():.1f} MB\")\n",
    "\n",
    "# Colour preferences - optimized\n",
    "print(\"• Processing colour preferences...\")\n",
    "colour_preferences = (\n",
    "    product_data\n",
    "    .group_by(['customer_id', 'colour_group_name'])\n",
    "    .agg(pl.col('price').count().alias('colour_purchase_count'))\n",
    "    .group_by('customer_id')\n",
    "    .agg([\n",
    "        pl.col('colour_group_name').first().alias('preferred_colour'),\n",
    "        pl.col('colour_group_name').n_unique().alias('colour_diversity')\n",
    "    ])\n",
    ")\n",
    "\n",
    "gc.collect()\n",
    "print(f\"  Memory usage: {check_memory_usage():.1f} MB\")\n",
    "\n",
    "# Price sensitivity features - using streaming approach\n",
    "print(\"• Processing price behaviour...\")\n",
    "price_behaviour = (\n",
    "    product_data\n",
    "    .group_by('customer_id')\n",
    "    .agg([\n",
    "        pl.col('price').min().alias('min_price_paid'),\n",
    "        pl.col('price').max().alias('max_price_paid'),\n",
    "        pl.col('price').quantile(0.25).alias('price_q1'),\n",
    "        pl.col('price').quantile(0.75).alias('price_q3'),\n",
    "        pl.col('price').median().alias('median_price_paid')\n",
    "    ])\n",
    "    .with_columns([\n",
    "        (pl.col('max_price_paid') - pl.col('min_price_paid')).alias('price_range'),\n",
    "        (pl.col('price_q3') - pl.col('price_q1')).alias('price_iqr')\n",
    "    ])\n",
    ")\n",
    "\n",
    "gc.collect()\n",
    "print(f\"  Memory usage: {check_memory_usage():.1f} MB\")\n",
    "\n",
    "# Sales channel preferences - optimized\n",
    "print(\"• Processing channel preferences...\")\n",
    "channel_preferences = (\n",
    "    product_data\n",
    "    .group_by(['customer_id', 'sales_channel_id'])\n",
    "    .agg(pl.col('price').count().alias('channel_purchases'))\n",
    "    .with_columns([\n",
    "        pl.when(pl.col('sales_channel_id') == 1)\n",
    "        .then(pl.lit('online'))\n",
    "        .otherwise(pl.lit('store'))\n",
    "        .alias('channel_type')\n",
    "    ])\n",
    "    .group_by('customer_id')\n",
    "    .agg([\n",
    "        pl.col('channel_type').first().alias('preferred_channel'),\n",
    "        pl.col('sales_channel_id').n_unique().alias('channel_diversity'),\n",
    "        \n",
    "        # Calculate online percentage\n",
    "        (pl.when(pl.col('channel_type') == 'online')\n",
    "         .then(pl.col('channel_purchases'))\n",
    "         .otherwise(0).sum() / pl.col('channel_purchases').sum()).alias('online_purchase_ratio')\n",
    "    ])\n",
    ")\n",
    "\n",
    "# Clear product data to free memory\n",
    "del product_data\n",
    "gc.collect()\n",
    "\n",
    "print(f\"✓ Created product preference features\")\n",
    "print(f\"  • Category preferences: {category_preferences.height:,} customers\")\n",
    "print(f\"  • Department preferences: {dept_preferences.height:,} customers\")\n",
    "print(f\"  • Colour preferences: {colour_preferences.height:,} customers\")\n",
    "print(f\"  • Price behaviour: {price_behaviour.height:,} customers\")\n",
    "print(f\"  • Channel preferences: {channel_preferences.height:,} customers\")\n",
    "\n",
    "final_memory = check_memory_usage()\n",
    "print(f\"Final memory usage: {final_memory:.1f} MB (change: +{final_memory - initial_memory:.1f} MB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demographic and Temporal Features\n",
    "\n",
    "Create features based on customer demographics and temporal purchasing patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating demographic and temporal features with memory optimization...\n",
      "Current memory usage: 2686.4 MB\n",
      "Current memory usage: 2939.3 MB\n",
      "  Memory after demographics: 2939.3 MB\n",
      "Processing temporal features...\n",
      "✓ Created demographic features for 822,211 customers\n",
      "✓ Created temporal features for 822,211 customers\n",
      "\n",
      "Age Group Distribution:\n",
      "  • 25-34: 260,168 customers\n",
      "  • 18-24: 201,741 customers\n",
      "  • 45-54: 154,496 customers\n",
      "  • 35-44: 101,189 customers\n",
      "  • 55-64: 78,056 customers\n",
      "  • 65+: 26,561 customers\n",
      "Current memory usage: 2867.5 MB\n",
      "Final memory usage: 2867.5 MB (change: +181.1 MB)\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating demographic and temporal features with memory optimization...\")\n",
    "\n",
    "# Clear memory and monitor\n",
    "gc.collect()\n",
    "initial_memory = check_memory_usage()\n",
    "\n",
    "# Demographic features from customer data - select needed columns only\n",
    "demographic_data = df_integrated.select([\n",
    "    'customer_id', 'age', 'club_member_status', 'fashion_news_frequency', 'FN', 'Active'\n",
    "]).unique(subset=['customer_id'])\n",
    "\n",
    "demographic_features = (\n",
    "    demographic_data\n",
    "    .with_columns([\n",
    "        # Age groups\n",
    "        pl.when(pl.col('age') < 25).then(pl.lit('18-24'))\n",
    "        .when(pl.col('age') < 35).then(pl.lit('25-34'))\n",
    "        .when(pl.col('age') < 45).then(pl.lit('35-44'))\n",
    "        .when(pl.col('age') < 55).then(pl.lit('45-54'))\n",
    "        .when(pl.col('age') < 65).then(pl.lit('55-64'))\n",
    "        .otherwise(pl.lit('65+'))\n",
    "        .alias('age_group'),\n",
    "        \n",
    "        # Fashion engagement level\n",
    "        pl.when(pl.col('fashion_news_frequency') == 'Regularly').then(pl.lit('high'))\n",
    "        .when(pl.col('fashion_news_frequency') == 'Monthly').then(pl.lit('medium'))\n",
    "        .otherwise(pl.lit('low'))\n",
    "        .alias('fashion_engagement')\n",
    "    ])\n",
    ")\n",
    "\n",
    "# Clean up demographic data\n",
    "del demographic_data\n",
    "gc.collect()\n",
    "print(f\"  Memory after demographics: {check_memory_usage():.1f} MB\")\n",
    "\n",
    "# Temporal purchasing patterns - recreate temporal data efficiently\n",
    "print(\"Processing temporal features...\")\n",
    "temporal_data = df_integrated.select([\n",
    "    'customer_id', 't_dat'\n",
    "]).with_columns([\n",
    "    pl.col('t_dat').str.to_date().alias('transaction_date')\n",
    "]).with_columns([\n",
    "    pl.col('transaction_date').dt.year().alias('year'),\n",
    "    pl.col('transaction_date').dt.month().alias('month'),\n",
    "    pl.col('transaction_date').dt.weekday().alias('weekday'),\n",
    "    pl.col('transaction_date').dt.quarter().alias('quarter')\n",
    "])\n",
    "\n",
    "temporal_features = (\n",
    "    temporal_data\n",
    "    .group_by('customer_id')\n",
    "    .agg([\n",
    "        # Seasonal preferences - use mode or most frequent\n",
    "        pl.col('quarter').mode().first().alias('preferred_quarter'),\n",
    "        pl.col('month').mode().first().alias('preferred_month'), \n",
    "        pl.col('weekday').mode().first().alias('preferred_weekday'),\n",
    "        \n",
    "        # Shopping pattern diversity\n",
    "        pl.col('quarter').n_unique().alias('quarter_diversity'),\n",
    "        pl.col('month').n_unique().alias('month_diversity'),\n",
    "        pl.col('weekday').n_unique().alias('weekday_diversity'),\n",
    "        \n",
    "        # Activity span\n",
    "        (pl.col('year').max() - pl.col('year').min() + 1).alias('active_years')\n",
    "    ])\n",
    "    .with_columns([\n",
    "        # Weekend vs weekday preference\n",
    "        pl.when(pl.col('preferred_weekday').is_in([6, 7]))\n",
    "        .then(pl.lit('weekend'))\n",
    "        .otherwise(pl.lit('weekday'))\n",
    "        .alias('weekend_preference'),\n",
    "        \n",
    "        # Season mapping\n",
    "        pl.when(pl.col('preferred_quarter') == 1).then(pl.lit('winter'))\n",
    "        .when(pl.col('preferred_quarter') == 2).then(pl.lit('spring'))\n",
    "        .when(pl.col('preferred_quarter') == 3).then(pl.lit('summer'))\n",
    "        .otherwise(pl.lit('autumn'))\n",
    "        .alias('preferred_season')\n",
    "    ])\n",
    ")\n",
    "\n",
    "# Clean up temporal data\n",
    "del temporal_data\n",
    "gc.collect()\n",
    "\n",
    "print(f\"✓ Created demographic features for {demographic_features.height:,} customers\")\n",
    "print(f\"✓ Created temporal features for {temporal_features.height:,} customers\")\n",
    "\n",
    "# Display age group distribution\n",
    "age_dist = demographic_features.group_by('age_group').agg(pl.count().alias('count')).sort('count', descending=True)\n",
    "print(f\"\\nAge Group Distribution:\")\n",
    "for row in age_dist.head(6).to_dicts():\n",
    "    print(f\"  • {row['age_group']}: {row['count']:,} customers\")\n",
    "\n",
    "final_memory = check_memory_usage()\n",
    "print(f\"Final memory usage: {final_memory:.1f} MB (change: +{final_memory - initial_memory:.1f} MB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Features (Product Descriptions)\n",
    "\n",
    "Apply TF-IDF to extract features from product text data if available.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating text features from product descriptions with memory optimization...\n",
      "Current memory usage: 2868.2 MB\n",
      "Available text columns: ['product_type_name', 'product_group_name', 'department_name', 'colour_group_name']\n",
      "Processing customer text profiles...\n",
      "Current memory usage: 3927.8 MB\n",
      "  Memory after text aggregation: 3927.8 MB\n",
      "Converting to pandas for TF-IDF...\n",
      "Applying TF-IDF vectorisation...\n",
      "Fitting TF-IDF...\n",
      "  TF-IDF matrix shape: (822211, 50)\n",
      "Current memory usage: 3449.5 MB\n",
      "  Memory usage: 3449.5 MB\n",
      "Applying PCA to reduce dimensionality...\n",
      "✓ Created 15 PCA components explaining 63.77% of variance\n",
      "  Top 5 TF-IDF features: ['accessories', 'basic', 'beige', 'bikini', 'black']\n",
      "Current memory usage: 1166.6 MB\n",
      "Final memory usage: 1166.6 MB (change: +-1701.6 MB)\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating text features from product descriptions with memory optimization...\")\n",
    "\n",
    "# Clear memory and monitor\n",
    "gc.collect()\n",
    "initial_memory = check_memory_usage()\n",
    "\n",
    "# Check if we have text columns for TF-IDF\n",
    "text_columns = ['product_type_name', 'product_group_name', 'department_name', 'colour_group_name']\n",
    "available_text_cols = [col for col in text_columns if col in df_integrated.columns]\n",
    "\n",
    "if available_text_cols:\n",
    "    print(f\"Available text columns: {available_text_cols}\")\n",
    "    \n",
    "    # Select only needed text columns to reduce memory usage\n",
    "    text_data = df_integrated.select(['customer_id'] + available_text_cols)\n",
    "    \n",
    "    # Create combined text features for each customer - compatible with Polars 0.20.31\n",
    "    print(\"Processing customer text profiles...\")\n",
    "    customer_text_features = (\n",
    "        text_data\n",
    "        .group_by('customer_id')\n",
    "        .agg([\n",
    "            # Concatenate unique values using str.concat (compatible with 0.20.31)\n",
    "            pl.col('product_type_name').unique().str.concat(' ').alias('purchased_product_types'),\n",
    "            pl.col('product_group_name').unique().str.concat(' ').alias('purchased_product_groups'),\n",
    "            pl.col('department_name').unique().str.concat(' ').alias('purchased_departments'),\n",
    "            pl.col('colour_group_name').unique().str.concat(' ').alias('purchased_colours')\n",
    "        ])\n",
    "        .with_columns([\n",
    "            # Combine all text into a single customer profile\n",
    "            pl.concat_str([\n",
    "                pl.col('purchased_product_types'),\n",
    "                pl.col('purchased_product_groups'),\n",
    "                pl.col('purchased_departments'),\n",
    "                pl.col('purchased_colours')\n",
    "            ], separator=' ').alias('customer_text_profile')\n",
    "        ])\n",
    "    )\n",
    "    \n",
    "    # Clean up text data\n",
    "    del text_data\n",
    "    gc.collect()\n",
    "    print(f\"  Memory after text aggregation: {check_memory_usage():.1f} MB\")\n",
    "    \n",
    "    # Convert to pandas for sklearn TF-IDF (process in smaller batches if needed)\n",
    "    print(\"Converting to pandas for TF-IDF...\")\n",
    "    customer_text_pd = customer_text_features.to_pandas()\n",
    "    \n",
    "    # Apply TF-IDF to customer text profiles\n",
    "    print(\"Applying TF-IDF vectorisation...\")\n",
    "    \n",
    "    # Configure TF-IDF with reduced features for memory efficiency\n",
    "    tfidf = TfidfVectorizer(\n",
    "        max_features=50,  # Reduced from 100 to 50 for memory efficiency\n",
    "        stop_words='english',\n",
    "        lowercase=True,\n",
    "        ngram_range=(1, 1),  # Only unigrams to reduce memory\n",
    "        min_df=10,  # Increased minimum document frequency\n",
    "        max_df=0.9  # Reduced maximum document frequency\n",
    "    )\n",
    "    \n",
    "    try:\n",
    "        # Fit and transform the customer text profiles\n",
    "        print(\"Fitting TF-IDF...\")\n",
    "        tfidf_matrix = tfidf.fit_transform(customer_text_pd['customer_text_profile'].fillna(''))\n",
    "        \n",
    "        # Get feature names\n",
    "        feature_names = tfidf.get_feature_names_out()\n",
    "        \n",
    "        print(f\"  TF-IDF matrix shape: {tfidf_matrix.shape}\")\n",
    "        print(f\"  Memory usage: {check_memory_usage():.1f} MB\")\n",
    "        \n",
    "        # Apply PCA immediately to reduce memory footprint\n",
    "        print(\"Applying PCA to reduce dimensionality...\")\n",
    "        pca = PCA(n_components=min(15, len(feature_names)), random_state=42)\n",
    "        tfidf_pca = pca.fit_transform(tfidf_matrix.toarray())\n",
    "        \n",
    "        # Create PCA feature DataFrame\n",
    "        pca_df = pd.DataFrame(\n",
    "            tfidf_pca,\n",
    "            columns=[f'tfidf_pca_{i+1}' for i in range(tfidf_pca.shape[1])],\n",
    "            index=customer_text_pd['customer_id']\n",
    "        ).reset_index()\n",
    "        \n",
    "        tfidf_pca_features = pl.from_pandas(pca_df)\n",
    "        \n",
    "        print(f\"✓ Created {tfidf_pca.shape[1]} PCA components explaining {pca.explained_variance_ratio_.sum():.2%} of variance\")\n",
    "        print(f\"  Top 5 TF-IDF features: {list(feature_names[:5])}\")\n",
    "        \n",
    "        # Clean up intermediate variables\n",
    "        del tfidf_matrix, pca_df, customer_text_pd\n",
    "        gc.collect()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Warning: TF-IDF processing failed due to memory constraints: {e}\")\n",
    "        print(\"Skipping TF-IDF features to continue with other features...\")\n",
    "        tfidf_pca_features = None\n",
    "    \n",
    "    # Clean up text features\n",
    "    del customer_text_features\n",
    "    gc.collect()\n",
    "    \n",
    "else:\n",
    "    print(\"No suitable text columns found for TF-IDF analysis\")\n",
    "    tfidf_pca_features = None\n",
    "\n",
    "final_memory = check_memory_usage()\n",
    "print(f\"Final memory usage: {final_memory:.1f} MB (change: +{final_memory - initial_memory:.1f} MB)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interaction Features\n",
    "\n",
    "Create derived features by combining existing features to capture complex relationships.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating interaction features...\n",
      "✓ Created interaction features for 822,211 customers\n",
      "\n",
      "Interaction Feature Summary:\n",
      "  • RFM score range: 234.40 - 796712.40\n",
      "  • Customer value range: £0.00 - £5.75\n",
      "  • Product diversity ratio: 0.989 (avg)\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating interaction features...\")\n",
    "\n",
    "# Start with RFM as base for interactions\n",
    "interaction_features = customer_rfm.select([\n",
    "    'customer_id', 'recency_days', 'frequency', 'monetary_value', \n",
    "    'avg_transaction_value', 'unique_products_purchased'\n",
    "])\n",
    "\n",
    "# Add key features from other datasets\n",
    "if 'age' in demographic_features.columns:\n",
    "    interaction_features = interaction_features.join(\n",
    "        demographic_features.select(['customer_id', 'age']),\n",
    "        on='customer_id',\n",
    "        how='left'\n",
    "    )\n",
    "\n",
    "# Create interaction features\n",
    "interaction_features = interaction_features.with_columns([\n",
    "    # RFM score (simple scoring using percentile ranks)\n",
    "    (pl.col('recency_days').rank(\"min\") * 0.3 + \n",
    "     pl.col('frequency').rank(\"min\") * 0.4 + \n",
    "     pl.col('monetary_value').rank(\"min\") * 0.3).alias('rfm_score'),\n",
    "    \n",
    "    # Customer value segments\n",
    "    (pl.col('frequency') * pl.col('avg_transaction_value')).alias('customer_value'),\n",
    "    \n",
    "    # Product diversity ratio\n",
    "    (pl.col('unique_products_purchased').cast(pl.Float64) / pl.col('frequency').cast(pl.Float64)).alias('product_diversity_ratio'),\n",
    "    \n",
    "    # Spending efficiency (monetary per transaction)\n",
    "    (pl.col('monetary_value') / pl.col('frequency')).alias('spending_efficiency')\n",
    "])\n",
    "\n",
    "# Add age-related interactions if age is available\n",
    "if 'age' in interaction_features.columns:\n",
    "    interaction_features = interaction_features.with_columns([\n",
    "        # Age-spending interaction\n",
    "        (pl.col('age') * pl.col('avg_transaction_value')).alias('age_spending_interaction'),\n",
    "        \n",
    "        # Age-frequency interaction\n",
    "        (pl.col('age') * pl.col('frequency')).alias('age_frequency_interaction')\n",
    "    ])\n",
    "\n",
    "# Calculate quantiles separately using to_numpy() to extract scalar values\n",
    "recency_q25 = interaction_features.select(pl.col('recency_days').quantile(0.25)).to_numpy()[0, 0]\n",
    "recency_q50 = interaction_features.select(pl.col('recency_days').quantile(0.5)).to_numpy()[0, 0]\n",
    "recency_q75 = interaction_features.select(pl.col('recency_days').quantile(0.75)).to_numpy()[0, 0]\n",
    "\n",
    "frequency_q25 = interaction_features.select(pl.col('frequency').quantile(0.25)).to_numpy()[0, 0]\n",
    "frequency_q50 = interaction_features.select(pl.col('frequency').quantile(0.5)).to_numpy()[0, 0]\n",
    "frequency_q75 = interaction_features.select(pl.col('frequency').quantile(0.75)).to_numpy()[0, 0]\n",
    "\n",
    "monetary_q25 = interaction_features.select(pl.col('monetary_value').quantile(0.25)).to_numpy()[0, 0]\n",
    "monetary_q50 = interaction_features.select(pl.col('monetary_value').quantile(0.5)).to_numpy()[0, 0]\n",
    "monetary_q75 = interaction_features.select(pl.col('monetary_value').quantile(0.75)).to_numpy()[0, 0]\n",
    "\n",
    "# Create segments using when/then logic\n",
    "interaction_features = interaction_features.with_columns([\n",
    "    # Recency segments (lower recency = more recent = better)\n",
    "    pl.when(pl.col('recency_days') <= recency_q25).then(pl.lit('recent'))\n",
    "    .when(pl.col('recency_days') <= recency_q50).then(pl.lit('moderate'))\n",
    "    .when(pl.col('recency_days') <= recency_q75).then(pl.lit('old'))\n",
    "    .otherwise(pl.lit('very_old'))\n",
    "    .alias('recency_segment'),\n",
    "    \n",
    "    # Frequency segments (higher frequency = better)\n",
    "    pl.when(pl.col('frequency') >= frequency_q75).then(pl.lit('very_high'))\n",
    "    .when(pl.col('frequency') >= frequency_q50).then(pl.lit('high'))\n",
    "    .when(pl.col('frequency') >= frequency_q25).then(pl.lit('moderate'))\n",
    "    .otherwise(pl.lit('low'))\n",
    "    .alias('frequency_segment'),\n",
    "    \n",
    "    # Monetary segments (higher monetary = better)\n",
    "    pl.when(pl.col('monetary_value') >= monetary_q75).then(pl.lit('very_high'))\n",
    "    .when(pl.col('monetary_value') >= monetary_q50).then(pl.lit('high'))\n",
    "    .when(pl.col('monetary_value') >= monetary_q25).then(pl.lit('moderate'))\n",
    "    .otherwise(pl.lit('low'))\n",
    "    .alias('monetary_segment')\n",
    "])\n",
    "\n",
    "print(f\"✓ Created interaction features for {interaction_features.height:,} customers\")\n",
    "print(f\"\\nInteraction Feature Summary:\")\n",
    "print(f\"  • RFM score range: {interaction_features['rfm_score'].min():.2f} - {interaction_features['rfm_score'].max():.2f}\")\n",
    "print(f\"  • Customer value range: £{interaction_features['customer_value'].min():.2f} - £{interaction_features['customer_value'].max():.2f}\")\n",
    "print(f\"  • Product diversity ratio: {interaction_features['product_diversity_ratio'].mean():.3f} (avg)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine All Features\n",
    "\n",
    "Merge all engineered features into a comprehensive customer feature dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combining all engineered features...\n",
      "✓ Added TF-IDF PCA features\n",
      "\n",
      "✓ Final feature dataset created!\n",
      "  • Total customers: 822,211\n",
      "  • Total features: 68\n",
      "  • Memory usage: 400.5 MB\n",
      "\n",
      "Feature Categories:\n",
      "  • RFM Features: 4 features\n",
      "  • Product Preferences: 3 features\n",
      "  • Demographics: 4 features\n",
      "  • Temporal: 3 features\n",
      "  • Interaction: 3 features\n",
      "\n",
      "Sample of Final Features:\n",
      "  • customer_id: ['c5ead8322e1bb92b4d779bd3da8029933dbf0dab8ecbee93a6f23db48013c801', '37aab04a0edc482deb4995b113d53a9ce4ce64b4c8465688a21b1eb9ab237972', '9d7ac305fad67e32843a80947e0ce76be9833e61eb109e06d18120d2ed3c3a93']\n",
      "  • recency_days: [174, 138, 616]\n",
      "  • frequency: [2, 1, 2]\n",
      "  • monetary_value: [0.030474576271186438, 0.023711864406779665, 0.048779661016949145]\n",
      "  • avg_transaction_value: [0.015237288135593219, 0.023711864406779665, 0.024389830508474573]\n",
      "  • unique_products_purchased: [2, 1, 2]\n",
      "  • spending_variability: [0.002396972139615415, 0.0, 0.013423043981846324]\n",
      "  • customer_lifespan_days: [544, 0, 110]\n",
      "  • first_purchase_date: [datetime.date(2018, 10, 5), datetime.date(2020, 5, 7), datetime.date(2018, 9, 27)]\n",
      "  • last_purchase_date: [datetime.date(2020, 4, 1), datetime.date(2020, 5, 7), datetime.date(2019, 1, 15)]\n",
      "  ... and 58 more features\n"
     ]
    }
   ],
   "source": [
    "print(\"Combining all engineered features...\")\n",
    "\n",
    "# Start with customer RFM features as the base\n",
    "final_features = customer_rfm\n",
    "\n",
    "# Join product preference features\n",
    "final_features = final_features.join(category_preferences, on='customer_id', how='left')\n",
    "final_features = final_features.join(dept_preferences, on='customer_id', how='left')\n",
    "final_features = final_features.join(colour_preferences, on='customer_id', how='left')\n",
    "final_features = final_features.join(price_behaviour, on='customer_id', how='left')\n",
    "final_features = final_features.join(channel_preferences, on='customer_id', how='left')\n",
    "\n",
    "# Join demographic and temporal features\n",
    "final_features = final_features.join(demographic_features, on='customer_id', how='left')\n",
    "final_features = final_features.join(temporal_features, on='customer_id', how='left')\n",
    "\n",
    "# Join TF-IDF features if available\n",
    "if tfidf_pca_features is not None:\n",
    "    final_features = final_features.join(tfidf_pca_features, on='customer_id', how='left')\n",
    "    print(f\"✓ Added TF-IDF PCA features\")\n",
    "\n",
    "# Join interaction features (excluding duplicates)\n",
    "interaction_cols_to_add = [\n",
    "    'customer_id', 'rfm_score', 'customer_value', 'product_diversity_ratio', \n",
    "    'spending_efficiency', 'recency_segment', 'frequency_segment', 'monetary_segment'\n",
    "]\n",
    "\n",
    "# Add age interactions if available\n",
    "if 'age_spending_interaction' in interaction_features.columns:\n",
    "    interaction_cols_to_add.extend(['age_spending_interaction', 'age_frequency_interaction'])\n",
    "\n",
    "final_features = final_features.join(\n",
    "    interaction_features.select(interaction_cols_to_add), \n",
    "    on='customer_id', \n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Final feature dataset created!\")\n",
    "print(f\"  • Total customers: {final_features.height:,}\")\n",
    "print(f\"  • Total features: {len(final_features.columns)}\")\n",
    "print(f\"  • Memory usage: {final_features.estimated_size('mb'):.1f} MB\")\n",
    "\n",
    "# Display feature categories\n",
    "feature_categories = {\n",
    "    'RFM Features': ['recency_days', 'frequency', 'monetary_value', 'avg_transaction_value'],\n",
    "    'Product Preferences': ['most_purchased_category', 'category_diversity', 'preferred_department'],\n",
    "    'Demographics': ['age', 'age_group', 'club_member_status', 'fashion_engagement'],\n",
    "    'Temporal': ['preferred_season', 'weekend_preference', 'active_years'],\n",
    "    'Interaction': ['rfm_score', 'customer_value', 'product_diversity_ratio']\n",
    "}\n",
    "\n",
    "print(f\"\\nFeature Categories:\")\n",
    "for category, features in feature_categories.items():\n",
    "    available_features = [f for f in features if f in final_features.columns]\n",
    "    print(f\"  • {category}: {len(available_features)} features\")\n",
    "\n",
    "# Display sample of final features\n",
    "print(f\"\\nSample of Final Features:\")\n",
    "sample_features = final_features.head(3)\n",
    "for col in final_features.columns[:10]:  # Show first 10 columns\n",
    "    print(f\"  • {col}: {sample_features[col].to_list()}\")\n",
    "if len(final_features.columns) > 10:\n",
    "    print(f\"  ... and {len(final_features.columns) - 10} more features\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Quality Analysis\n",
    "\n",
    "Analyse the quality and distribution of engineered features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysing feature quality...\n",
      "\n",
      "Features with Missing Values:\n",
      "  • Active: 62.9% (516,989 records)\n",
      "  • FN: 62.1% (510,590 records)\n",
      "  • fashion_news_frequency: 0.8% (6,934 records)\n",
      "  • age: 0.8% (6,661 records)\n",
      "  • age_spending_interaction: 0.8% (6,661 records)\n",
      "  • age_frequency_interaction: 0.8% (6,661 records)\n",
      "  • club_member_status: 0.3% (2,389 records)\n",
      "\n",
      "Numerical Feature Statistics:\n",
      "  • recency_days: μ=269.47, σ=213.96\n",
      "  • frequency: μ=3.87, σ=4.81\n",
      "  • monetary_value: μ=0.11, σ=0.15\n",
      "  • avg_transaction_value: μ=0.03, σ=0.01\n",
      "  • unique_products_purchased: μ=3.79, σ=4.66\n",
      "\n",
      "Categorical Feature Distributions:\n",
      "  • age_group: 25-34(260168), 18-24(201741), 45-54(154496)...\n",
      "  • club_member_status: ACTIVE(782830), PRE-CREATE(36728), None(2389)...\n",
      "  • preferred_channel: store(520548), online(301663)...\n",
      "\n",
      "✓ Feature quality analysis completed!\n"
     ]
    }
   ],
   "source": [
    "print(\"Analysing feature quality...\")\n",
    "\n",
    "# Check for missing values\n",
    "null_counts = final_features.null_count()\n",
    "total_records = final_features.height\n",
    "\n",
    "missing_analysis = []\n",
    "for col_name in final_features.columns:\n",
    "    if col_name != 'customer_id':\n",
    "        missing_count = null_counts[col_name][0]\n",
    "        missing_percentage = (missing_count / total_records) * 100\n",
    "        if missing_percentage > 0:\n",
    "            missing_analysis.append({\n",
    "                'feature': col_name,\n",
    "                'missing_count': missing_count,\n",
    "                'missing_percentage': missing_percentage\n",
    "            })\n",
    "\n",
    "if missing_analysis:\n",
    "    missing_df = pd.DataFrame(missing_analysis).sort_values('missing_percentage', ascending=False)\n",
    "    print(f\"\\nFeatures with Missing Values:\")\n",
    "    for _, row in missing_df.head(10).iterrows():\n",
    "        print(f\"  • {row['feature']}: {row['missing_percentage']:.1f}% ({row['missing_count']:,} records)\")\n",
    "else:\n",
    "    print(f\"\\n✓ No missing values detected in engineered features!\")\n",
    "\n",
    "# Analyse numerical feature distributions\n",
    "numerical_features = [\n",
    "    'recency_days', 'frequency', 'monetary_value', 'avg_transaction_value',\n",
    "    'unique_products_purchased', 'category_diversity', 'rfm_score'\n",
    "]\n",
    "\n",
    "available_numerical = [f for f in numerical_features if f in final_features.columns]\n",
    "\n",
    "if available_numerical:\n",
    "    print(f\"\\nNumerical Feature Statistics:\")\n",
    "    stats = final_features.select(available_numerical).describe().to_pandas()\n",
    "    \n",
    "    for feature in available_numerical[:5]:  # Show first 5 for brevity\n",
    "        mean_val = final_features[feature].mean()\n",
    "        std_val = final_features[feature].std()\n",
    "        print(f\"  • {feature}: μ={mean_val:.2f}, σ={std_val:.2f}\")\n",
    "\n",
    "# Analyse categorical feature distributions\n",
    "categorical_features = [\n",
    "    'age_group', 'club_member_status', 'preferred_channel', \n",
    "    'preferred_season', 'recency_segment', 'frequency_segment'\n",
    "]\n",
    "\n",
    "available_categorical = [f for f in categorical_features if f in final_features.columns]\n",
    "\n",
    "if available_categorical:\n",
    "    print(f\"\\nCategorical Feature Distributions:\")\n",
    "    for feature in available_categorical[:3]:  # Show first 3 for brevity\n",
    "        dist = final_features.group_by(feature).agg(pl.count().alias('count')).sort('count', descending=True)\n",
    "        top_categories = dist.head(3).to_dicts()\n",
    "        # Fix the f-string syntax error by breaking it into separate parts\n",
    "        category_strings = []\n",
    "        for cat in top_categories:\n",
    "            category_strings.append(f\"{cat[feature]}({cat['count']})\")\n",
    "        print(f\"  • {feature}: {', '.join(category_strings)}...\")\n",
    "\n",
    "print(f\"\\n✓ Feature quality analysis completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Engineered Features\n",
    "\n",
    "Save the final engineered feature dataset for use in machine learning models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving engineered features...\n",
      "✓ Saved engineered features to: ../data/processed/hm_engineered_features.parquet\n",
      "✓ Saved rfm_features to: ../data/processed/hm_rfm_features.parquet\n",
      "✓ Saved product_preferences to: ../data/processed/hm_product_preferences.parquet\n",
      "✓ Saved demographic_features to: ../data/processed/hm_demographic_features.parquet\n",
      "✓ Saved temporal_features to: ../data/processed/hm_temporal_features.parquet\n",
      "✓ Saved TF-IDF features to: ../data/processed/hm_tfidf_features.parquet\n",
      "✓ Saved feature documentation to: ../data/processed/feature_documentation.json\n",
      "\n",
      "🎉 Feature engineering completed successfully!\n",
      "\n",
      "Summary:\n",
      "  • Customers processed: 822,211\n",
      "  • Features created: 68\n",
      "  • Output files: 6 datasets saved\n",
      "  • Ready for machine learning model training!\n"
     ]
    }
   ],
   "source": [
    "print(\"Saving engineered features...\")\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "output_dir = os.path.join(data_dir, 'processed')\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save main feature dataset\n",
    "features_path = os.path.join(output_dir, 'hm_engineered_features.parquet')\n",
    "final_features.write_parquet(features_path)\n",
    "print(f\"✓ Saved engineered features to: {features_path}\")\n",
    "\n",
    "# Save individual feature components for analysis\n",
    "components = {\n",
    "    'rfm_features': customer_rfm,\n",
    "    'product_preferences': category_preferences,\n",
    "    'demographic_features': demographic_features,\n",
    "    'temporal_features': temporal_features\n",
    "}\n",
    "\n",
    "for name, dataset in components.items():\n",
    "    component_path = os.path.join(output_dir, f'hm_{name}.parquet')\n",
    "    dataset.write_parquet(component_path)\n",
    "    print(f\"✓ Saved {name} to: {component_path}\")\n",
    "\n",
    "# Save TF-IDF features separately if they exist\n",
    "if tfidf_pca_features is not None:\n",
    "    tfidf_path = os.path.join(output_dir, 'hm_tfidf_features.parquet')\n",
    "    tfidf_pca_features.write_parquet(tfidf_path)\n",
    "    print(f\"✓ Saved TF-IDF features to: {tfidf_path}\")\n",
    "\n",
    "# Create feature documentation\n",
    "feature_docs = {\n",
    "    'dataset_info': {\n",
    "        'total_customers': final_features.height,\n",
    "        'total_features': len(final_features.columns),\n",
    "        'creation_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "        'memory_usage_mb': final_features.estimated_size('mb')\n",
    "    },\n",
    "    'feature_categories': {\n",
    "        'behavioural_rfm': ['recency_days', 'frequency', 'monetary_value', 'avg_transaction_value'],\n",
    "        'product_preferences': ['most_purchased_category', 'category_diversity', 'preferred_department'],\n",
    "        'demographics': ['age', 'age_group', 'club_member_status', 'fashion_engagement'],\n",
    "        'temporal_patterns': ['preferred_season', 'weekend_preference', 'active_years'],\n",
    "        'interactions': ['rfm_score', 'customer_value', 'product_diversity_ratio']\n",
    "    }\n",
    "}\n",
    "\n",
    "import json\n",
    "docs_path = os.path.join(output_dir, 'feature_documentation.json')\n",
    "with open(docs_path, 'w') as f:\n",
    "    json.dump(feature_docs, f, indent=2)\n",
    "print(f\"✓ Saved feature documentation to: {docs_path}\")\n",
    "\n",
    "print(f\"\\n🎉 Feature engineering completed successfully!\")\n",
    "print(f\"\\nSummary:\")\n",
    "print(f\"  • Customers processed: {final_features.height:,}\")\n",
    "print(f\"  • Features created: {len(final_features.columns)}\")\n",
    "print(f\"  • Output files: {len(components) + 2} datasets saved\")\n",
    "print(f\"  • Ready for machine learning model training!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
