{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# H&M Feature Engineering - Memory Optimized Version\n",
    "\n",
    "This notebook performs feature engineering on the H&M dataset using aggressive memory optimization techniques including streaming processing, smaller batch sizes, and progressive saving."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import os\n",
    "import warnings\n",
    "import gc\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure Polars for extreme memory efficiency\n",
    "try:\n",
    "    pl.Config.set_streaming_chunk_size(1000)  # Very small chunks\n",
    "    pl.Config.set_fmt_str_lengths(30)\n",
    "except AttributeError:\n",
    "    pass\n",
    "\n",
    "# Memory monitoring function\n",
    "def check_memory_usage():\n",
    "    try:\n",
    "        import psutil\n",
    "        process = psutil.Process(os.getpid())\n",
    "        memory_mb = process.memory_info().rss / 1024 / 1024\n",
    "        print(f\"Memory usage: {memory_mb:.1f} MB\")\n",
    "        return memory_mb\n",
    "    except ImportError:\n",
    "        print(\"psutil not available for memory monitoring\")\n",
    "        return 0\n",
    "\n",
    "# Configuration - adjust these based on your system\n",
    "SAMPLE_SIZE = 500000  # Process only 500k records for testing\n",
    "BATCH_SIZE = 10000    # Very small batch size\n",
    "USE_SAMPLE = True     # Set to False to process full dataset\n",
    "\n",
    "print(f\"Libraries imported successfully\")\n",
    "print(f\"Polars version: {pl.__version__}\")\n",
    "print(f\"Configuration: Sample={USE_SAMPLE}, Sample size={SAMPLE_SIZE:,}, Batch size={BATCH_SIZE:,}\")\n",
    "check_memory_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Load Dataset with Extreme Memory Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset with streaming\n",
    "data_dir = '../data'\n",
    "integrated_path = os.path.join(data_dir, 'processed', 'hm_integrated_dataset.parquet')\n",
    "\n",
    "if not os.path.exists(integrated_path):\n",
    "    raise FileNotFoundError(f\"File not found: {integrated_path}\")\n",
    "\n",
    "print(\"Loading dataset with streaming...\")\n",
    "\n",
    "# Use lazy loading\n",
    "df_lazy = pl.scan_parquet(integrated_path)\n",
    "\n",
    "if USE_SAMPLE:\n",
    "    print(f\"Using sample of {SAMPLE_SIZE:,} records for testing...\")\n",
    "    df_integrated = df_lazy.head(SAMPLE_SIZE).collect()\n",
    "else:\n",
    "    print(\"Loading full dataset...\")\n",
    "    df_integrated = df_lazy.collect()\n",
    "\n",
    "print(f\"✓ Loaded {df_integrated.height:,} records with {len(df_integrated.columns)} columns\")\n",
    "print(f\"✓ Memory usage: {df_integrated.estimated_size('mb'):.1f} MB\")\n",
    "\n",
    "# Get basic info\n",
    "unique_customers = df_integrated['customer_id'].n_unique()\n",
    "print(f\"• Unique customers: {unique_customers:,}\")\n",
    "print(f\"• Date range: {df_integrated['t_dat'].min()} to {df_integrated['t_dat'].max()}\")\n",
    "\n",
    "# Set reference date\n",
    "REFERENCE_DATE = pd.to_datetime(df_integrated['t_dat'].max())\n",
    "print(f\"• Reference date: {REFERENCE_DATE.date()}\")\n",
    "\n",
    "check_memory_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. RFM Features with Ultra-Small Batches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating RFM features with ultra-small batches...\")\n",
    "gc.collect()\n",
    "initial_memory = check_memory_usage()\n",
    "\n",
    "# Get list of unique customers\n",
    "customer_list = df_integrated.select('customer_id').unique().to_pandas()['customer_id'].tolist()\n",
    "total_customers = len(customer_list)\n",
    "\n",
    "print(f\"Processing {total_customers:,} customers in batches of {BATCH_SIZE:,}\")\n",
    "\n",
    "# Prepare minimal data for RFM\n",
    "rfm_data = df_integrated.select([\n",
    "    'customer_id', 'price', 't_dat', 'article_id'\n",
    "]).with_columns([\n",
    "    pl.col('t_dat').str.to_date().alias('transaction_date')\n",
    "])\n",
    "\n",
    "# Process in very small batches\n",
    "rfm_results = []\n",
    "batch_count = 0\n",
    "\n",
    "for i in range(0, total_customers, BATCH_SIZE):\n",
    "    batch_customers = customer_list[i:i+BATCH_SIZE]\n",
    "    batch_count += 1\n",
    "    \n",
    "    if batch_count % 10 == 0:\n",
    "        print(f\"Processing batch {batch_count}: customers {i:,} to {min(i+BATCH_SIZE, total_customers):,}\")\n",
    "        check_memory_usage()\n",
    "    \n",
    "    # Filter to batch customers\n",
    "    batch_data = rfm_data.filter(pl.col('customer_id').is_in(batch_customers))\n",
    "    \n",
    "    if batch_data.height == 0:\n",
    "        continue\n",
    "    \n",
    "    # Calculate RFM for this batch\n",
    "    batch_rfm = (\n",
    "        batch_data\n",
    "        .group_by('customer_id')\n",
    "        .agg([\n",
    "            # Basic RFM metrics only\n",
    "            ((pl.lit(REFERENCE_DATE) - pl.col('transaction_date').max()).dt.total_days()).abs().alias('recency_days'),\n",
    "            pl.col('transaction_date').count().alias('frequency'),\n",
    "            pl.col('price').sum().alias('monetary_value'),\n",
    "            pl.col('price').mean().alias('avg_transaction_value'),\n",
    "            pl.col('article_id').n_unique().alias('unique_products')\n",
    "        ])\n",
    "    )\n",
    "    \n",
    "    rfm_results.append(batch_rfm)\n",
    "    \n",
    "    # Clear batch data immediately\n",
    "    del batch_data\n",
    "    \n",
    "    # Aggressive garbage collection every 20 batches\n",
    "    if batch_count % 20 == 0:\n",
    "        gc.collect()\n",
    "\n",
    "# Combine results\n",
    "print(\"Combining RFM batches...\")\n",
    "customer_rfm = pl.concat(rfm_results)\n",
    "del rfm_results, rfm_data\n",
    "gc.collect()\n",
    "\n",
    "print(f\"✓ Created RFM features for {customer_rfm.height:,} customers\")\n",
    "print(f\"Sample RFM:\")\n",
    "print(customer_rfm.head(3).to_pandas())\n",
    "check_memory_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Basic Product Preferences (Simplified)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating simplified product preferences...\")\n",
    "gc.collect()\n",
    "\n",
    "# Only keep essential product preference features\n",
    "product_data = df_integrated.select([\n",
    "    'customer_id', 'product_type_name', 'department_name', 'price', 'sales_channel_id'\n",
    "])\n",
    "\n",
    "# Most purchased category per customer\n",
    "print(\"• Processing category preferences...\")\n",
    "category_prefs = (\n",
    "    product_data\n",
    "    .group_by(['customer_id', 'product_type_name'])\n",
    "    .agg(pl.col('price').count().alias('count'))\n",
    "    .sort(['customer_id', 'count'], descending=[False, True])\n",
    "    .group_by('customer_id')\n",
    "    .agg([\n",
    "        pl.col('product_type_name').first().alias('top_category'),\n",
    "        pl.col('product_type_name').n_unique().alias('category_diversity')\n",
    "    ])\n",
    ")\n",
    "\n",
    "gc.collect()\n",
    "print(f\"  Memory: {check_memory_usage():.1f} MB\")\n",
    "\n",
    "# Price behaviour (simplified)\n",
    "print(\"• Processing price behaviour...\")\n",
    "price_behaviour = (\n",
    "    product_data\n",
    "    .group_by('customer_id')\n",
    "    .agg([\n",
    "        pl.col('price').min().alias('min_price'),\n",
    "        pl.col('price').max().alias('max_price'),\n",
    "        pl.col('price').median().alias('median_price')\n",
    "    ])\n",
    ")\n",
    "\n",
    "# Channel preference (simplified)\n",
    "print(\"• Processing channel preferences...\")\n",
    "channel_prefs = (\n",
    "    product_data\n",
    "    .group_by(['customer_id', 'sales_channel_id'])\n",
    "    .agg(pl.col('price').count().alias('count'))\n",
    "    .sort(['customer_id', 'count'], descending=[False, True])\n",
    "    .group_by('customer_id')\n",
    "    .agg(pl.col('sales_channel_id').first().alias('preferred_channel'))\n",
    ")\n",
    "\n",
    "del product_data\n",
    "gc.collect()\n",
    "\n",
    "print(f\"✓ Created simplified product preferences\")\n",
    "print(f\"  • Categories: {category_prefs.height:,} customers\")\n",
    "print(f\"  • Price behaviour: {price_behaviour.height:,} customers\")\n",
    "print(f\"  • Channels: {channel_prefs.height:,} customers\")\n",
    "check_memory_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Demographics (Essential Only)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Creating essential demographic features...\")\n",
    "gc.collect()\n",
    "\n",
    "# Get unique customer demographics only\n",
    "demographics = (\n",
    "    df_integrated\n",
    "    .select(['customer_id', 'age', 'club_member_status', 'fashion_news_frequency'])\n",
    "    .unique(subset=['customer_id'])\n",
    "    .with_columns([\n",
    "        # Simplified age groups\n",
    "        pl.when(pl.col('age') < 30).then(pl.lit('young'))\n",
    "        .when(pl.col('age') < 50).then(pl.lit('middle'))\n",
    "        .otherwise(pl.lit('senior'))\n",
    "        .alias('age_group')\n",
    "    ])\n",
    ")\n",
    "\n",
    "print(f\"✓ Created demographics for {demographics.height:,} customers\")\n",
    "check_memory_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Combine Features and Save Progressively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Combining features progressively...\")\n",
    "gc.collect()\n",
    "\n",
    "# Start with RFM as base\n",
    "final_features = customer_rfm\n",
    "\n",
    "# Join other features one by one with immediate cleanup\n",
    "print(\"• Joining category preferences...\")\n",
    "final_features = final_features.join(category_prefs, on='customer_id', how='left')\n",
    "del category_prefs\n",
    "gc.collect()\n",
    "\n",
    "print(\"• Joining price behaviour...\")\n",
    "final_features = final_features.join(price_behaviour, on='customer_id', how='left')\n",
    "del price_behaviour\n",
    "gc.collect()\n",
    "\n",
    "print(\"• Joining channel preferences...\")\n",
    "final_features = final_features.join(channel_prefs, on='customer_id', how='left')\n",
    "del channel_prefs\n",
    "gc.collect()\n",
    "\n",
    "print(\"• Joining demographics...\")\n",
    "final_features = final_features.join(demographics, on='customer_id', how='left')\n",
    "del demographics\n",
    "gc.collect()\n",
    "\n",
    "# Add simple interaction features\n",
    "final_features = final_features.with_columns([\n",
    "    (pl.col('frequency') * pl.col('avg_transaction_value')).alias('customer_value'),\n",
    "    (pl.col('unique_products').cast(pl.Float64) / pl.col('frequency').cast(pl.Float64)).alias('product_diversity_ratio')\n",
    "])\n",
    "\n",
    "print(f\"\\n✓ Final feature dataset created!\")\n",
    "print(f\"  • Total customers: {final_features.height:,}\")\n",
    "print(f\"  • Total features: {len(final_features.columns)}\")\n",
    "print(f\"  • Memory usage: {final_features.estimated_size('mb'):.1f} MB\")\n",
    "\n",
    "print(f\"\\nFeature columns: {final_features.columns}\")\n",
    "print(f\"\\nSample features:\")\n",
    "print(final_features.head(3).to_pandas())\n",
    "\n",
    "check_memory_usage()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Saving engineered features...\")\n",
    "\n",
    "# Create output directory\n",
    "output_dir = os.path.join(data_dir, 'processed')\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save with appropriate suffix\n",
    "suffix = '_sample' if USE_SAMPLE else '_full'\n",
    "features_path = os.path.join(output_dir, f'hm_engineered_features{suffix}.parquet')\n",
    "\n",
    "final_features.write_parquet(features_path)\n",
    "print(f\"✓ Saved features to: {features_path}\")\n",
    "\n",
    "# Save metadata\n",
    "metadata = {\n",
    "    'creation_date': datetime.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "    'total_customers': final_features.height,\n",
    "    'total_features': len(final_features.columns),\n",
    "    'is_sample': USE_SAMPLE,\n",
    "    'sample_size': SAMPLE_SIZE if USE_SAMPLE else 'full'\n",
    "}\n",
    "\n",
    "import json\n",
    "metadata_path = os.path.join(output_dir, f'feature_metadata{suffix}.json')\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(f\"✓ Saved metadata to: {metadata_path}\")\n",
    "print(f\"\\n🎉 Feature engineering completed successfully!\")\n",
    "print(f\"Summary: {final_features.height:,} customers, {len(final_features.columns)} features\")\n",
    "\n",
    "check_memory_usage()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}