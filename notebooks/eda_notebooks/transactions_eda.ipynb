{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# H&M Transactions Data - Exploratory Data Analysis\n",
    "\n",
    "This notebook performs comprehensive EDA on the cleaned transactions dataset to understand:\n",
    "- Transaction patterns and trends over time\n",
    "- Price distributions and purchasing behaviour\n",
    "- Customer transaction frequency patterns\n",
    "- Seasonal and temporal trends\n",
    "- Revenue insights and business metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Import required libraries\nimport polars as pl\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pathlib import Path\nimport warnings\nfrom datetime import datetime, timedelta\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\n# Add project root to path\nimport sys\nproject_root = Path.cwd().parent.parent  # Go up two levels from notebooks/eda_notebooks/\nsys.path.append(str(project_root))\n\ntry:\n    from hnm_data_analysis.exploratory_data_analysis.eda_module import EDAModule\n    print(\"‚úÖ EDA module imported successfully\")\nexcept ImportError as e:\n    print(f\"‚ö†Ô∏è Could not import EDA module: {e}\")\n    print(\"Continuing without EDA module...\")\n\n# Configure plotting\nplt.style.use('seaborn-v0_8')\nsns.set_palette('viridis')\nplt.rcParams['figure.figsize'] = (12, 6)\nplt.rcParams['figure.dpi'] = 100\nwarnings.filterwarnings('ignore')\n\nprint(\"üì¶ Libraries imported successfully\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load transactions data\ndata_path = \"../../data/cleaned/transactions_cleaned.parquet\"\nprint(f\"Loading data from: {data_path}\")\n\ntry:\n    df = pl.read_parquet(data_path)\n    print(f\"‚úÖ Data loaded successfully: {df.shape[0]:,} rows √ó {df.shape[1]} columns\")\n    print(f\"Memory usage: {df.estimated_size('mb'):.1f} MB\")\nexcept Exception as e:\n    print(f\"‚ùå Error loading data: {e}\")\n    raise"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initial Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic information about the dataset\n",
    "print(\"üìä Dataset Overview\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Columns: {df.columns}\")\n",
    "print(f\"Data types: {dict(zip(df.columns, df.dtypes))}\")\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\nüîç First 5 rows:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values and data quality\n",
    "print(\"üîç Data Quality Assessment\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "missing_summary = []\n",
    "for col in df.columns:\n",
    "    null_count = df.select(pl.col(col).null_count()).item()\n",
    "    null_pct = (null_count / df.height) * 100\n",
    "    missing_summary.append({\n",
    "        'Column': col,\n",
    "        'Missing Count': null_count,\n",
    "        'Missing %': f\"{null_pct:.2f}%\"\n",
    "    })\n",
    "\n",
    "missing_df = pd.DataFrame(missing_summary)\n",
    "print(missing_df.to_string(index=False))\n",
    "\n",
    "# Check for duplicates\n",
    "duplicate_count = df.height - df.n_unique()\n",
    "print(f\"\\nüîÑ Duplicate rows: {duplicate_count:,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic descriptive statistics\n",
    "print(\"üìà Descriptive Statistics\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Get numeric columns\n",
    "numeric_cols = df.select(pl.col(pl.NUMERIC_DTYPES)).columns\n",
    "print(f\"Numeric columns: {numeric_cols}\")\n",
    "\n",
    "if 'price' in df.columns:\n",
    "    price_stats = df.select([\n",
    "        pl.col('price').count().alias('count'),\n",
    "        pl.col('price').mean().alias('mean'),\n",
    "        pl.col('price').median().alias('median'),\n",
    "        pl.col('price').std().alias('std'),\n",
    "        pl.col('price').min().alias('min'),\n",
    "        pl.col('price').max().alias('max'),\n",
    "        pl.col('price').quantile(0.25).alias('Q1'),\n",
    "        pl.col('price').quantile(0.75).alias('Q3')\n",
    "    ])\n",
    "    \n",
    "    print(\"\\nüí∞ Price Statistics:\")\n",
    "    for col in price_stats.columns:\n",
    "        value = price_stats.select(col).item()\n",
    "        print(f\"{col}: ¬£{value:.2f}\" if col != 'count' else f\"{col}: {value:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Transaction Volume Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transaction volume over time\n",
    "print(\"üìÖ Transaction Volume Analysis\")\n",
    "\n",
    "# Daily transaction counts\n",
    "daily_transactions = (\n",
    "    df.group_by('t_dat')\n",
    "    .agg([\n",
    "        pl.len().alias('transaction_count'),\n",
    "        pl.col('price').sum().alias('daily_revenue'),\n",
    "        pl.col('customer_id').n_unique().alias('unique_customers')\n",
    "    ])\n",
    "    .sort('t_dat')\n",
    ")\n",
    "\n",
    "print(f\"Date range: {daily_transactions.select(pl.col('t_dat').min()).item()} to {daily_transactions.select(pl.col('t_dat').max()).item()}\")\n",
    "print(f\"Total days: {daily_transactions.height:,}\")\n",
    "\n",
    "# Convert to pandas for plotting\n",
    "daily_pd = daily_transactions.to_pandas()\n",
    "daily_pd['t_dat'] = pd.to_datetime(daily_pd['t_dat'])\n",
    "\n",
    "# Create transaction volume plots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Transaction Volume Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Daily transaction count\n",
    "axes[0, 0].plot(daily_pd['t_dat'], daily_pd['transaction_count'], alpha=0.7)\n",
    "axes[0, 0].set_title('Daily Transaction Count')\n",
    "axes[0, 0].set_ylabel('Transactions')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Daily revenue\n",
    "axes[0, 1].plot(daily_pd['t_dat'], daily_pd['daily_revenue'], color='green', alpha=0.7)\n",
    "axes[0, 1].set_title('Daily Revenue')\n",
    "axes[0, 1].set_ylabel('Revenue (¬£)')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Daily unique customers\n",
    "axes[1, 0].plot(daily_pd['t_dat'], daily_pd['unique_customers'], color='orange', alpha=0.7)\n",
    "axes[1, 0].set_title('Daily Unique Customers')\n",
    "axes[1, 0].set_ylabel('Unique Customers')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Average transaction value\n",
    "daily_pd['avg_transaction_value'] = daily_pd['daily_revenue'] / daily_pd['transaction_count']\n",
    "axes[1, 1].plot(daily_pd['t_dat'], daily_pd['avg_transaction_value'], color='red', alpha=0.7)\n",
    "axes[1, 1].set_title('Average Transaction Value')\n",
    "axes[1, 1].set_ylabel('Average Value (¬£)')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Price Distribution Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Price distribution analysis\n",
    "print(\"üí∞ Price Distribution Analysis\")\n",
    "\n",
    "# Create comprehensive price distribution plots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Price Distribution Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Convert price to pandas for plotting\n",
    "price_data = df.select('price').to_pandas()['price'].dropna()\n",
    "\n",
    "# Histogram\n",
    "axes[0, 0].hist(price_data, bins=100, alpha=0.7, edgecolor='black')\n",
    "axes[0, 0].set_title('Price Distribution (All Transactions)')\n",
    "axes[0, 0].set_xlabel('Price (¬£)')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Log-scale histogram for better visibility\n",
    "axes[0, 1].hist(price_data, bins=100, alpha=0.7, edgecolor='black')\n",
    "axes[0, 1].set_yscale('log')\n",
    "axes[0, 1].set_title('Price Distribution (Log Scale)')\n",
    "axes[0, 1].set_xlabel('Price (¬£)')\n",
    "axes[0, 1].set_ylabel('Frequency (log)')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Box plot\n",
    "axes[1, 0].boxplot(price_data, vert=True)\n",
    "axes[1, 0].set_title('Price Box Plot')\n",
    "axes[1, 0].set_ylabel('Price (¬£)')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Price distribution for reasonable range (remove extreme outliers for visibility)\n",
    "price_95th = np.percentile(price_data, 95)\n",
    "price_filtered = price_data[price_data <= price_95th]\n",
    "axes[1, 1].hist(price_filtered, bins=50, alpha=0.7, edgecolor='black', color='orange')\n",
    "axes[1, 1].set_title(f'Price Distribution (Up to 95th Percentile: ¬£{price_95th:.2f})')\n",
    "axes[1, 1].set_xlabel('Price (¬£)')\n",
    "axes[1, 1].set_ylabel('Frequency')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Price quantiles\n",
    "quantiles = [0.1, 0.25, 0.5, 0.75, 0.9, 0.95, 0.99]\n",
    "price_quantiles = {}\n",
    "for q in quantiles:\n",
    "    price_quantiles[f'{int(q*100)}th percentile'] = df.select(pl.col('price').quantile(q)).item()\n",
    "\n",
    "print(\"\\nüìä Price Quantiles:\")\n",
    "for k, v in price_quantiles.items():\n",
    "    print(f\"{k}: ¬£{v:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Customer Behaviour Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customer transaction frequency analysis\n",
    "print(\"üë• Customer Transaction Behaviour\")\n",
    "\n",
    "# Customer transaction frequency\n",
    "customer_freq = (\n",
    "    df.group_by('customer_id')\n",
    "    .agg([\n",
    "        pl.len().alias('transaction_count'),\n",
    "        pl.col('price').sum().alias('total_spend'),\n",
    "        pl.col('price').mean().alias('avg_transaction_value'),\n",
    "        pl.col('t_dat').min().alias('first_purchase'),\n",
    "        pl.col('t_dat').max().alias('last_purchase')\n",
    "    ])\n",
    "    .with_columns([\n",
    "        (pl.col('last_purchase') - pl.col('first_purchase')).dt.total_days().alias('customer_lifespan_days')\n",
    "    ])\n",
    ")\n",
    "\n",
    "print(f\"Total unique customers: {customer_freq.height:,}\")\n",
    "\n",
    "# Customer frequency distribution\n",
    "freq_stats = customer_freq.select([\n",
    "    pl.col('transaction_count').mean().alias('avg_transactions'),\n",
    "    pl.col('transaction_count').median().alias('median_transactions'),\n",
    "    pl.col('transaction_count').max().alias('max_transactions'),\n",
    "    pl.col('total_spend').mean().alias('avg_total_spend'),\n",
    "    pl.col('total_spend').median().alias('median_total_spend')\n",
    "]).to_dict(as_series=False)\n",
    "\n",
    "print(\"\\nüìä Customer Transaction Statistics:\")\n",
    "for k, v in freq_stats.items():\n",
    "    if 'spend' in k:\n",
    "        print(f\"{k}: ¬£{v[0]:.2f}\")\n",
    "    else:\n",
    "        print(f\"{k}: {v[0]:.1f}\")\n",
    "\n",
    "# Plot customer behaviour\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Customer Behaviour Analysis', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Convert to pandas for plotting\n",
    "customer_pd = customer_freq.to_pandas()\n",
    "\n",
    "# Transaction frequency distribution\n",
    "axes[0, 0].hist(customer_pd['transaction_count'], bins=50, alpha=0.7, edgecolor='black')\n",
    "axes[0, 0].set_title('Customer Transaction Frequency')\n",
    "axes[0, 0].set_xlabel('Number of Transactions')\n",
    "axes[0, 0].set_ylabel('Number of Customers')\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Total spend distribution\n",
    "spend_95th = customer_pd['total_spend'].quantile(0.95)\n",
    "spend_filtered = customer_pd[customer_pd['total_spend'] <= spend_95th]['total_spend']\n",
    "axes[0, 1].hist(spend_filtered, bins=50, alpha=0.7, edgecolor='black', color='green')\n",
    "axes[0, 1].set_title(f'Customer Total Spend (Up to 95th Percentile)')\n",
    "axes[0, 1].set_xlabel('Total Spend (¬£)')\n",
    "axes[0, 1].set_ylabel('Number of Customers')\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Average transaction value\n",
    "avg_val_95th = customer_pd['avg_transaction_value'].quantile(0.95)\n",
    "avg_val_filtered = customer_pd[customer_pd['avg_transaction_value'] <= avg_val_95th]['avg_transaction_value']\n",
    "axes[1, 0].hist(avg_val_filtered, bins=50, alpha=0.7, edgecolor='black', color='orange')\n",
    "axes[1, 0].set_title('Customer Average Transaction Value')\n",
    "axes[1, 0].set_xlabel('Average Transaction Value (¬£)')\n",
    "axes[1, 0].set_ylabel('Number of Customers')\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Customer lifespan\n",
    "lifespan_filtered = customer_pd[customer_pd['customer_lifespan_days'] >= 0]['customer_lifespan_days']\n",
    "axes[1, 1].hist(lifespan_filtered, bins=50, alpha=0.7, edgecolor='black', color='red')\n",
    "axes[1, 1].set_title('Customer Lifespan (Days)')\n",
    "axes[1, 1].set_xlabel('Lifespan (Days)')\n",
    "axes[1, 1].set_ylabel('Number of Customers')\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Temporal Patterns and Seasonality"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temporal patterns analysis\n",
    "print(\"üìÖ Temporal Patterns and Seasonality\")\n",
    "\n",
    "# Add temporal features\n",
    "df_temporal = df.with_columns([\n",
    "    pl.col('t_dat').dt.day_of_week().alias('day_of_week'),\n",
    "    pl.col('t_dat').dt.month().alias('month'),\n",
    "    pl.col('t_dat').dt.year().alias('year'),\n",
    "    pl.col('t_dat').dt.quarter().alias('quarter')\n",
    "])\n",
    "\n",
    "# Monthly patterns\n",
    "monthly_patterns = (\n",
    "    df_temporal.group_by('month')\n",
    "    .agg([\n",
    "        pl.len().alias('transaction_count'),\n",
    "        pl.col('price').sum().alias('total_revenue'),\n",
    "        pl.col('price').mean().alias('avg_price')\n",
    "    ])\n",
    "    .sort('month')\n",
    ")\n",
    "\n",
    "# Day of week patterns\n",
    "dow_patterns = (\n",
    "    df_temporal.group_by('day_of_week')\n",
    "    .agg([\n",
    "        pl.len().alias('transaction_count'),\n",
    "        pl.col('price').sum().alias('total_revenue'),\n",
    "        pl.col('price').mean().alias('avg_price')\n",
    "    ])\n",
    "    .sort('day_of_week')\n",
    ")\n",
    "\n",
    "# Create temporal pattern plots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "fig.suptitle('Temporal Patterns and Seasonality', fontsize=16, fontweight='bold')\n",
    "\n",
    "# Monthly transaction volume\n",
    "monthly_pd = monthly_patterns.to_pandas()\n",
    "month_names = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'Jun', \n",
    "               'Jul', 'Aug', 'Sep', 'Oct', 'Nov', 'Dec']\n",
    "axes[0, 0].bar(range(1, 13), monthly_pd['transaction_count'], alpha=0.7)\n",
    "axes[0, 0].set_title('Monthly Transaction Volume')\n",
    "axes[0, 0].set_xlabel('Month')\n",
    "axes[0, 0].set_ylabel('Transaction Count')\n",
    "axes[0, 0].set_xticks(range(1, 13))\n",
    "axes[0, 0].set_xticklabels(month_names, rotation=45)\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Monthly revenue\n",
    "axes[0, 1].bar(range(1, 13), monthly_pd['total_revenue'], alpha=0.7, color='green')\n",
    "axes[0, 1].set_title('Monthly Revenue')\n",
    "axes[0, 1].set_xlabel('Month')\n",
    "axes[0, 1].set_ylabel('Total Revenue (¬£)')\n",
    "axes[0, 1].set_xticks(range(1, 13))\n",
    "axes[0, 1].set_xticklabels(month_names, rotation=45)\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Day of week patterns\n",
    "dow_pd = dow_patterns.to_pandas()\n",
    "dow_names = ['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun']\n",
    "axes[1, 0].bar(range(1, 8), dow_pd['transaction_count'], alpha=0.7, color='orange')\n",
    "axes[1, 0].set_title('Day of Week Transaction Volume')\n",
    "axes[1, 0].set_xlabel('Day of Week')\n",
    "axes[1, 0].set_ylabel('Transaction Count')\n",
    "axes[1, 0].set_xticks(range(1, 8))\n",
    "axes[1, 0].set_xticklabels(dow_names)\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Average price by day of week\n",
    "axes[1, 1].bar(range(1, 8), dow_pd['avg_price'], alpha=0.7, color='red')\n",
    "axes[1, 1].set_title('Average Price by Day of Week')\n",
    "axes[1, 1].set_xlabel('Day of Week')\n",
    "axes[1, 1].set_ylabel('Average Price (¬£)')\n",
    "axes[1, 1].set_xticks(range(1, 8))\n",
    "axes[1, 1].set_xticklabels(dow_names)\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Revenue and Business Insights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Revenue analysis and business insights\n",
    "print(\"üíº Revenue and Business Insights\")\n",
    "\n",
    "# Calculate key business metrics\n",
    "total_revenue = df.select(pl.col('price').sum()).item()\n",
    "total_transactions = df.height\n",
    "unique_customers = df.select(pl.col('customer_id').n_unique()).item()\n",
    "unique_articles = df.select(pl.col('article_id').n_unique()).item()\n",
    "avg_transaction_value = total_revenue / total_transactions\n",
    "\n",
    "# Date range\n",
    "date_range = df.select([\n",
    "    pl.col('t_dat').min().alias('start_date'),\n",
    "    pl.col('t_dat').max().alias('end_date')\n",
    "])\n",
    "start_date = date_range.select('start_date').item()\n",
    "end_date = date_range.select('end_date').item()\n",
    "period_days = (end_date - start_date).days + 1\n",
    "\n",
    "print(\"\\nüéØ Key Business Metrics:\")\n",
    "print(f\"Total Revenue: ¬£{total_revenue:,.2f}\")\n",
    "print(f\"Total Transactions: {total_transactions:,}\")\n",
    "print(f\"Unique Customers: {unique_customers:,}\")\n",
    "print(f\"Unique Articles: {unique_articles:,}\")\n",
    "print(f\"Average Transaction Value: ¬£{avg_transaction_value:.2f}\")\n",
    "print(f\"Revenue per Customer: ¬£{total_revenue/unique_customers:.2f}\")\n",
    "print(f\"Transactions per Customer: {total_transactions/unique_customers:.1f}\")\n",
    "print(f\"Analysis Period: {start_date} to {end_date} ({period_days} days)\")\n",
    "print(f\"Daily Average Revenue: ¬£{total_revenue/period_days:,.2f}\")\n",
    "print(f\"Daily Average Transactions: {total_transactions/period_days:,.0f}\")\n",
    "\n",
    "# Top performing articles by revenue\n",
    "top_articles = (\n",
    "    df.group_by('article_id')\n",
    "    .agg([\n",
    "        pl.col('price').sum().alias('total_revenue'),\n",
    "        pl.len().alias('units_sold'),\n",
    "        pl.col('price').mean().alias('avg_price')\n",
    "    ])\n",
    "    .sort('total_revenue', descending=True)\n",
    "    .head(10)\n",
    ")\n",
    "\n",
    "print(\"\\nüèÜ Top 10 Articles by Revenue:\")\n",
    "for i, row in enumerate(top_articles.iter_rows(named=True), 1):\n",
    "    print(f\"{i:2d}. Article {row['article_id']}: ¬£{row['total_revenue']:,.2f} ({row['units_sold']:,} units, avg ¬£{row['avg_price']:.2f})\")\n",
    "\n",
    "# Customer value segmentation\n",
    "customer_value = customer_freq.with_columns([\n",
    "    pl.when(pl.col('total_spend') >= pl.col('total_spend').quantile(0.9))\n",
    "    .then(pl.lit('High Value'))\n",
    "    .when(pl.col('total_spend') >= pl.col('total_spend').quantile(0.7))\n",
    "    .then(pl.lit('Medium Value'))\n",
    "    .otherwise(pl.lit('Low Value'))\n",
    "    .alias('value_segment')\n",
    "])\n",
    "\n",
    "value_segments = (\n",
    "    customer_value.group_by('value_segment')\n",
    "    .agg([\n",
    "        pl.len().alias('customer_count'),\n",
    "        pl.col('total_spend').sum().alias('segment_revenue'),\n",
    "        pl.col('total_spend').mean().alias('avg_spend_per_customer')\n",
    "    ])\n",
    ")\n",
    "\n",
    "print(\"\\nüíé Customer Value Segmentation:\")\n",
    "for row in value_segments.iter_rows(named=True):\n",
    "    pct_customers = (row['customer_count'] / unique_customers) * 100\n",
    "    pct_revenue = (row['segment_revenue'] / total_revenue) * 100\n",
    "    print(f\"{row['value_segment']}: {row['customer_count']:,} customers ({pct_customers:.1f}%) - ¬£{row['segment_revenue']:,.2f} revenue ({pct_revenue:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Interactive Visualisations (Plotly)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interactive visualisations using Plotly\n",
    "print(\"üìä Creating Interactive Visualisations\")\n",
    "\n",
    "# Interactive time series of daily revenue\n",
    "fig_ts = go.Figure()\n",
    "\n",
    "fig_ts.add_trace(go.Scatter(\n",
    "    x=daily_pd['t_dat'],\n",
    "    y=daily_pd['daily_revenue'],\n",
    "    mode='lines',\n",
    "    name='Daily Revenue',\n",
    "    line=dict(color='blue', width=1),\n",
    "    hovertemplate='<b>Date:</b> %{x}<br><b>Revenue:</b> ¬£%{y:,.2f}<extra></extra>'\n",
    "))\n",
    "\n",
    "# Add 7-day rolling average\n",
    "daily_pd['revenue_7d_ma'] = daily_pd['daily_revenue'].rolling(window=7).mean()\n",
    "fig_ts.add_trace(go.Scatter(\n",
    "    x=daily_pd['t_dat'],\n",
    "    y=daily_pd['revenue_7d_ma'],\n",
    "    mode='lines',\n",
    "    name='7-Day Moving Average',\n",
    "    line=dict(color='red', width=2),\n",
    "    hovertemplate='<b>Date:</b> %{x}<br><b>7-Day Avg Revenue:</b> ¬£%{y:,.2f}<extra></extra>'\n",
    "))\n",
    "\n",
    "fig_ts.update_layout(\n",
    "    title='Daily Revenue Trends',\n",
    "    xaxis_title='Date',\n",
    "    yaxis_title='Revenue (¬£)',\n",
    "    hovermode='x unified',\n",
    "    height=500\n",
    ")\n",
    "\n",
    "fig_ts.show()\n",
    "\n",
    "# Interactive price distribution\n",
    "price_sample = df.select('price').to_pandas()['price'].sample(min(10000, df.height)).dropna()\n",
    "\n",
    "fig_hist = go.Figure(data=[go.Histogram(\n",
    "    x=price_sample,\n",
    "    nbinsx=100,\n",
    "    name='Price Distribution',\n",
    "    hovertemplate='<b>Price Range:</b> ¬£%{x}<br><b>Count:</b> %{y}<extra></extra>'\n",
    ")])\n",
    "\n",
    "fig_hist.update_layout(\n",
    "    title='Interactive Price Distribution',\n",
    "    xaxis_title='Price (¬£)',\n",
    "    yaxis_title='Frequency',\n",
    "    height=400\n",
    ")\n",
    "\n",
    "fig_hist.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Key Insights and Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive insights summary\n",
    "print(\"üí° Key Insights from Transaction Data Analysis\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Calculate additional metrics for insights\n",
    "repeat_customers = customer_freq.filter(pl.col('transaction_count') > 1).height\n",
    "repeat_rate = (repeat_customers / unique_customers) * 100\n",
    "\n",
    "high_value_threshold = customer_freq.select(pl.col('total_spend').quantile(0.8)).item()\n",
    "high_value_customers = customer_freq.filter(pl.col('total_spend') >= high_value_threshold).height\n",
    "high_value_rate = (high_value_customers / unique_customers) * 100\n",
    "\n",
    "# Price insights\n",
    "price_median = df.select(pl.col('price').median()).item()\n",
    "price_mode_range = df.filter(\n",
    "    (pl.col('price') >= price_median * 0.9) & \n",
    "    (pl.col('price') <= price_median * 1.1)\n",
    ").height / df.height * 100\n",
    "\n",
    "print(\"üéØ BUSINESS PERFORMANCE:\")\n",
    "print(f\"‚Ä¢ Total Revenue: ¬£{total_revenue:,.2f} over {period_days} days\")\n",
    "print(f\"‚Ä¢ Average Daily Revenue: ¬£{total_revenue/period_days:,.2f}\")\n",
    "print(f\"‚Ä¢ Customer Base: {unique_customers:,} unique customers\")\n",
    "print(f\"‚Ä¢ Product Range: {unique_articles:,} unique articles\")\n",
    "print(f\"‚Ä¢ Average Transaction Value: ¬£{avg_transaction_value:.2f}\")\n",
    "\n",
    "print(\"\\nüë• CUSTOMER BEHAVIOUR:\")\n",
    "print(f\"‚Ä¢ Repeat Customer Rate: {repeat_rate:.1f}% ({repeat_customers:,} customers)\")\n",
    "print(f\"‚Ä¢ High-Value Customers: {high_value_rate:.1f}% (spend >¬£{high_value_threshold:.2f})\")\n",
    "print(f\"‚Ä¢ Average Transactions per Customer: {total_transactions/unique_customers:.1f}\")\n",
    "print(f\"‚Ä¢ Average Spend per Customer: ¬£{total_revenue/unique_customers:.2f}\")\n",
    "\n",
    "print(\"\\nüí∞ PRICING INSIGHTS:\")\n",
    "print(f\"‚Ä¢ Median Price: ¬£{price_median:.2f}\")\n",
    "print(f\"‚Ä¢ Price Range: ¬£{df.select(pl.col('price').min()).item():.2f} - ¬£{df.select(pl.col('price').max()).item():.2f}\")\n",
    "print(f\"‚Ä¢ Most Common Price Range: Around ¬£{price_median:.2f} ({price_mode_range:.1f}% of transactions)\")\n",
    "\n",
    "# Seasonal insights\n",
    "peak_month = monthly_pd.loc[monthly_pd['transaction_count'].idxmax(), 'month']\n",
    "peak_dow = dow_pd.loc[dow_pd['transaction_count'].idxmax(), 'day_of_week']\n",
    "peak_month_name = month_names[int(peak_month) - 1]\n",
    "peak_dow_name = dow_names[int(peak_dow) - 1]\n",
    "\n",
    "print(\"\\nüìÖ TEMPORAL PATTERNS:\")\n",
    "print(f\"‚Ä¢ Peak Sales Month: {peak_month_name} (Month {peak_month})\")\n",
    "print(f\"‚Ä¢ Peak Sales Day: {peak_dow_name}\")\n",
    "print(f\"‚Ä¢ Seasonal Variation: {((monthly_pd['transaction_count'].max() - monthly_pd['transaction_count'].min()) / monthly_pd['transaction_count'].mean() * 100):.1f}% difference between peak and low months\")\n",
    "\n",
    "print(\"\\nüîç RECOMMENDATIONS:\")\n",
    "print(\"‚Ä¢ Focus on converting one-time buyers to repeat customers\")\n",
    "print(f\"‚Ä¢ Leverage peak sales periods ({peak_month_name} and {peak_dow_name}s) for promotions\")\n",
    "print(\"‚Ä¢ Consider pricing strategies around the ¬£{:.2f} sweet spot\".format(price_median))\n",
    "print(\"‚Ä¢ Develop targeted campaigns for high-value customer segment\")\n",
    "print(\"‚Ä¢ Analyse article performance to optimise inventory\")\n",
    "\n",
    "print(\"\\nüìä DATA QUALITY:\")\n",
    "print(f\"‚Ä¢ Dataset Completeness: {((df.height - duplicate_count) / df.height * 100):.1f}% unique records\")\n",
    "print(f\"‚Ä¢ Date Coverage: {period_days} days of transaction history\")\n",
    "print(\"‚Ä¢ No missing values in critical fields (customer_id, article_id, price, date)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}