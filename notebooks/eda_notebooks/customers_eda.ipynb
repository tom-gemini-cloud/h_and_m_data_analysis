{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# H&M Customer Features - Exploratory Data Analysis\n",
    "\n",
    "This notebook performs comprehensive EDA on the engineered customer features dataset to understand:\n",
    "- Customer segmentation patterns and characteristics\n",
    "- RFM (Recency, Frequency, Monetary) analysis insights\n",
    "- Behavioural feature distributions and correlations\n",
    "- Customer lifecycle and value propositions\n",
    "- Advanced clustering and segmentation opportunities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Import required libraries\nimport polars as pl\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pathlib import Path\nimport warnings\nfrom datetime import datetime, timedelta\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\nfrom scipy import stats\nfrom scipy.cluster.hierarchy import linkage, dendrogram\n\n# Add project root to path\nimport sys\nproject_root = Path.cwd().parent.parent  # Go up two levels from notebooks/eda_notebooks/\nsys.path.append(str(project_root))\n\ntry:\n    from hnm_data_analysis.exploratory_data_analysis.eda_module import EDAModule\n    print(\"‚úÖ EDA module imported successfully\")\nexcept ImportError as e:\n    print(f\"‚ö†Ô∏è Could not import EDA module: {e}\")\n    print(\"Continuing without EDA module...\")\n\n# Configure plotting for UK publication standards\nplt.style.use('seaborn-v0_8')\nsns.set_palette('viridis')\nplt.rcParams['figure.figsize'] = (12, 6)\nplt.rcParams['figure.dpi'] = 100\nplt.rcParams['font.family'] = 'DejaVu Sans'\nwarnings.filterwarnings('ignore')\n\nprint(\"üì¶ Libraries imported successfully\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load customer features data\ndata_path = \"../../data/features/final/customers_features_final.parquet\"\nprint(f\"Loading customer features data from: {data_path}\")\n\ntry:\n    df = pl.read_parquet(data_path)\n    print(f\"‚úÖ Data loaded successfully: {df.shape[0]:,} customers √ó {df.shape[1]} features\")\n    print(f\"Memory usage: {df.estimated_size('mb'):.1f} MB\")\nexcept Exception as e:\n    print(f\"‚ùå Error loading data: {e}\")\n    raise"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Initial Data Exploration and Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display comprehensive dataset overview\n",
    "print(\"üìä Customer Features Dataset Overview\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Columns: {list(df.columns)}\")\n",
    "print(f\"Data types: {dict(zip(df.columns, [str(dtype) for dtype in df.dtypes]))}\")\n",
    "\n",
    "# Display first few rows\n",
    "print(\"\\nüîç Sample Customer Features:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive data quality assessment\n",
    "print(\"üîç Data Quality Assessment\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Missing values analysis\n",
    "missing_analysis = []\n",
    "for col in df.columns:\n",
    "    null_count = df.select(pl.col(col).null_count()).item()\n",
    "    null_pct = (null_count / df.height) * 100\n",
    "    missing_analysis.append({\n",
    "        'Feature': col,\n",
    "        'Missing Count': null_count,\n",
    "        'Missing %': f\"{null_pct:.2f}%\"\n",
    "    })\n",
    "\n",
    "missing_df = pd.DataFrame(missing_analysis)\n",
    "print(missing_df.to_string(index=False))\n",
    "\n",
    "# Check for duplicates and data integrity\n",
    "duplicate_count = df.height - df.n_unique()\n",
    "print(f\"\\nüîÑ Duplicate customer records: {duplicate_count:,}\")\n",
    "\n",
    "# Feature completeness\n",
    "complete_records = df.filter(pl.all_horizontal(pl.all().is_not_null())).height\n",
    "completeness_rate = (complete_records / df.height) * 100\n",
    "print(f\"üìã Complete feature records: {complete_records:,} ({completeness_rate:.1f}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Descriptive statistics for all numeric features\n",
    "print(\"üìà Descriptive Statistics for Customer Features\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# Get numeric columns (excluding customer_id and metadata)\n",
    "numeric_cols = [col for col in df.select(pl.col(pl.NUMERIC_DTYPES)).columns \n",
    "                if col not in ['customer_id'] and not col.endswith('_at')]\n",
    "\n",
    "print(f\"Analysing {len(numeric_cols)} numeric features: {numeric_cols}\")\n",
    "\n",
    "# Calculate comprehensive statistics\n",
    "for col in numeric_cols:\n",
    "    if col in df.columns:\n",
    "        stats_data = df.select([\n",
    "            pl.col(col).count().alias('count'),\n",
    "            pl.col(col).mean().alias('mean'),\n",
    "            pl.col(col).median().alias('median'),\n",
    "            pl.col(col).std().alias('std'),\n",
    "            pl.col(col).min().alias('min'),\n",
    "            pl.col(col).max().alias('max'),\n",
    "            pl.col(col).quantile(0.25).alias('Q1'),\n",
    "            pl.col(col).quantile(0.75).alias('Q3'),\n",
    "            pl.col(col).quantile(0.95).alias('P95'),\n",
    "            pl.col(col).quantile(0.99).alias('P99')\n",
    "        ]).to_dict(as_series=False)\n",
    "        \n",
    "        print(f\"\\nüéØ {col.upper()}:\")\n",
    "        for stat, value in stats_data.items():\n",
    "            if stat == 'count':\n",
    "                print(f\"  {stat}: {value[0]:,}\")\n",
    "            else:\n",
    "                print(f\"  {stat}: {value[0]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. RFM Analysis Deep Dive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RFM (Recency, Frequency, Monetary) analysis\n",
    "print(\"üìä RFM Analysis - Customer Value Segmentation\")\n",
    "\n",
    "# Check if RFM features exist\n",
    "rfm_features = ['recency', 'frequency', 'monetary']\n",
    "available_rfm = [f for f in rfm_features if f in df.columns]\n",
    "\n",
    "if len(available_rfm) == 3:\n",
    "    # RFM distribution analysis\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    fig.suptitle('RFM Analysis - Customer Behaviour Patterns', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    # Convert to pandas for plotting\n",
    "    rfm_data = df.select(rfm_features).to_pandas()\n",
    "    \n",
    "    # Distribution plots\n",
    "    for i, feature in enumerate(rfm_features):\n",
    "        # Histogram\n",
    "        axes[0, i].hist(rfm_data[feature], bins=50, alpha=0.7, edgecolor='black')\n",
    "        axes[0, i].set_title(f'{feature.title()} Distribution')\n",
    "        axes[0, i].set_xlabel(feature.title())\n",
    "        axes[0, i].set_ylabel('Number of Customers')\n",
    "        axes[0, i].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Box plot\n",
    "        axes[1, i].boxplot(rfm_data[feature], vert=True)\n",
    "        axes[1, i].set_title(f'{feature.title()} Box Plot')\n",
    "        axes[1, i].set_ylabel(feature.title())\n",
    "        axes[1, i].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # RFM correlation analysis\n",
    "    rfm_corr = rfm_data.corr()\n",
    "    print(\"\\nüîó RFM Correlation Matrix:\")\n",
    "    print(rfm_corr.round(3))\n",
    "    \n",
    "    # RFM quintile segmentation\n",
    "    rfm_quintiles = df.select([\n",
    "        pl.col('customer_id'),\n",
    "        pl.col('recency').qcut(5, labels=['1','2','3','4','5']).alias('R_Score'),\n",
    "        pl.col('frequency').qcut(5, labels=['1','2','3','4','5'], allow_duplicates=True).alias('F_Score'),\n",
    "        pl.col('monetary').qcut(5, labels=['1','2','3','4','5'], allow_duplicates=True).alias('M_Score')\n",
    "    ]).with_columns([\n",
    "        (pl.col('R_Score').cast(pl.Int8) + \n",
    "         pl.col('F_Score').cast(pl.Int8) + \n",
    "         pl.col('M_Score').cast(pl.Int8)).alias('RFM_Score')\n",
    "    ])\n",
    "    \n",
    "    # RFM score distribution\n",
    "    rfm_score_dist = rfm_quintiles.group_by('RFM_Score').count().sort('RFM_Score')\n",
    "    print(\"\\nüéØ RFM Score Distribution:\")\n",
    "    for row in rfm_score_dist.iter_rows(named=True):\n",
    "        percentage = (row['count'] / df.height) * 100\n",
    "        print(f\"Score {row['RFM_Score']}: {row['count']:,} customers ({percentage:.1f}%)\")\n",
    "        \n",
    "else:\n",
    "    print(f\"‚ùå Missing RFM features. Available: {available_rfm}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Behavioural Features Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Behavioural features analysis\n",
    "print(\"üß† Behavioural Features Analysis\")\n",
    "\n",
    "# Define behavioural features\n",
    "behavioural_features = [\n",
    "    'purchase_diversity_score', 'price_sensitivity_index', \n",
    "    'colour_preference_entropy', 'style_consistency_score'\n",
    "]\n",
    "\n",
    "available_behavioural = [f for f in behavioural_features if f in df.columns]\n",
    "print(f\"Available behavioural features: {available_behavioural}\")\n",
    "\n",
    "if available_behavioural:\n",
    "    # Create comprehensive behavioural analysis plots\n",
    "    n_features = len(available_behavioural)\n",
    "    fig, axes = plt.subplots(2, n_features, figsize=(5*n_features, 10))\n",
    "    if n_features == 1:\n",
    "        axes = axes.reshape(-1, 1)\n",
    "    fig.suptitle('Customer Behavioural Features Analysis', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    behavioural_data = df.select(available_behavioural).to_pandas()\n",
    "    \n",
    "    for i, feature in enumerate(available_behavioural):\n",
    "        # Distribution plot\n",
    "        axes[0, i].hist(behavioural_data[feature].dropna(), bins=50, alpha=0.7, edgecolor='black')\n",
    "        axes[0, i].set_title(f'{feature.replace(\"_\", \" \").title()} Distribution')\n",
    "        axes[0, i].set_xlabel(feature.replace('_', ' ').title())\n",
    "        axes[0, i].set_ylabel('Number of Customers')\n",
    "        axes[0, i].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Box plot with quartile information\n",
    "        bp = axes[1, i].boxplot(behavioural_data[feature].dropna(), vert=True, patch_artist=True)\n",
    "        bp['boxes'][0].set_facecolor('lightblue')\n",
    "        axes[1, i].set_title(f'{feature.replace(\"_\", \" \").title()} Box Plot')\n",
    "        axes[1, i].set_ylabel(feature.replace('_', ' ').title())\n",
    "        axes[1, i].grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Behavioural feature statistics\n",
    "    print(\"\\nüìä Behavioural Feature Insights:\")\n",
    "    for feature in available_behavioural:\n",
    "        feature_stats = df.select([\n",
    "            pl.col(feature).mean().alias('mean'),\n",
    "            pl.col(feature).std().alias('std'),\n",
    "            pl.col(feature).quantile(0.1).alias('10th_percentile'),\n",
    "            pl.col(feature).quantile(0.9).alias('90th_percentile')\n",
    "        ]).to_dict(as_series=False)\n",
    "        \n",
    "        print(f\"\\nüéØ {feature.replace('_', ' ').title()}:\")\n",
    "        print(f\"  Mean: {feature_stats['mean'][0]:.3f} ¬± {feature_stats['std'][0]:.3f}\")\n",
    "        print(f\"  Range (10th-90th percentile): {feature_stats['10th_percentile'][0]:.3f} - {feature_stats['90th_percentile'][0]:.3f}\")\n",
    "        \n",
    "        # Interpret the feature\n",
    "        if 'diversity' in feature:\n",
    "            print(f\"  Interpretation: Higher values indicate more diverse purchasing patterns\")\n",
    "        elif 'sensitivity' in feature:\n",
    "            print(f\"  Interpretation: Higher values indicate more price-sensitive behaviour\")\n",
    "        elif 'entropy' in feature:\n",
    "            print(f\"  Interpretation: Higher values indicate more varied preferences\")\n",
    "        elif 'consistency' in feature:\n",
    "            print(f\"  Interpretation: Higher values indicate more consistent style preferences\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Feature Correlation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive correlation analysis\n",
    "print(\"üîó Feature Correlation Analysis\")\n",
    "\n",
    "# Get all numeric features for correlation\n",
    "correlation_features = [col for col in numeric_cols if col in df.columns]\n",
    "print(f\"Analysing correlations between {len(correlation_features)} features\")\n",
    "\n",
    "if len(correlation_features) > 1:\n",
    "    # Calculate correlation matrix\n",
    "    corr_data = df.select(correlation_features).to_pandas()\n",
    "    correlation_matrix = corr_data.corr()\n",
    "    \n",
    "    # Create correlation heatmap\n",
    "    fig, ax = plt.subplots(figsize=(12, 10))\n",
    "    \n",
    "    # Create mask for upper triangle\n",
    "    mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "    \n",
    "    # Generate heatmap\n",
    "    sns.heatmap(\n",
    "        correlation_matrix,\n",
    "        mask=mask,\n",
    "        annot=True,\n",
    "        cmap='RdBu_r',\n",
    "        center=0,\n",
    "        square=True,\n",
    "        fmt='.2f',\n",
    "        cbar_kws={'shrink': 0.8},\n",
    "        ax=ax\n",
    "    )\n",
    "    \n",
    "    ax.set_title('Customer Feature Correlation Matrix', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Identify strongest correlations\n",
    "    print(\"\\nüîç Strongest Feature Correlations (|r| > 0.3):\")\n",
    "    strong_correlations = []\n",
    "    \n",
    "    for i in range(len(correlation_matrix.columns)):\n",
    "        for j in range(i+1, len(correlation_matrix.columns)):\n",
    "            corr_value = correlation_matrix.iloc[i, j]\n",
    "            if abs(corr_value) > 0.3:\n",
    "                strong_correlations.append({\n",
    "                    'Feature 1': correlation_matrix.columns[i],\n",
    "                    'Feature 2': correlation_matrix.columns[j],\n",
    "                    'Correlation': corr_value\n",
    "                })\n",
    "    \n",
    "    # Sort by absolute correlation strength\n",
    "    strong_correlations.sort(key=lambda x: abs(x['Correlation']), reverse=True)\n",
    "    \n",
    "    for corr in strong_correlations[:10]:  # Top 10\n",
    "        print(f\"  {corr['Feature 1']} ‚Üî {corr['Feature 2']}: {corr['Correlation']:.3f}\")\n",
    "        \n",
    "    if not strong_correlations:\n",
    "        print(\"  No strong correlations (|r| > 0.3) found between features.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Customer Segmentation Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced customer segmentation using multiple approaches\n",
    "print(\"üë• Advanced Customer Segmentation Analysis\")\n",
    "\n",
    "# Prepare data for clustering\n",
    "clustering_features = [f for f in correlation_features if f in df.columns]\n",
    "print(f\"Using {len(clustering_features)} features for segmentation: {clustering_features}\")\n",
    "\n",
    "if len(clustering_features) >= 3:\n",
    "    # Prepare clustering data\n",
    "    cluster_data = df.select(clustering_features).to_pandas().fillna(0)\n",
    "    \n",
    "    # Standardise features\n",
    "    scaler = StandardScaler()\n",
    "    scaled_data = scaler.fit_transform(cluster_data)\n",
    "    \n",
    "    # Determine optimal number of clusters using elbow method\n",
    "    inertias = []\n",
    "    k_range = range(2, 11)\n",
    "    \n",
    "    for k in k_range:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "        kmeans.fit(scaled_data)\n",
    "        inertias.append(kmeans.inertia_)\n",
    "    \n",
    "    # Plot elbow curve\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    ax1.plot(k_range, inertias, 'bo-')\n",
    "    ax1.set_title('Elbow Method for Optimal K')\n",
    "    ax1.set_xlabel('Number of Clusters')\n",
    "    ax1.set_ylabel('Inertia')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Perform clustering with optimal k (let's use k=5 for demonstration)\n",
    "    optimal_k = 5\n",
    "    kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "    cluster_labels = kmeans.fit_predict(scaled_data)\n",
    "    \n",
    "    # PCA for visualisation\n",
    "    pca = PCA(n_components=2)\n",
    "    pca_data = pca.fit_transform(scaled_data)\n",
    "    \n",
    "    # Plot clusters in PCA space\n",
    "    scatter = ax2.scatter(pca_data[:, 0], pca_data[:, 1], c=cluster_labels, cmap='viridis', alpha=0.6)\n",
    "    ax2.set_title(f'Customer Segments (K={optimal_k}, PCA Visualisation)')\n",
    "    ax2.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')\n",
    "    ax2.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')\n",
    "    plt.colorbar(scatter, ax=ax2, label='Cluster')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Cluster analysis\n",
    "    cluster_df = df.select(['customer_id'] + clustering_features).to_pandas()\n",
    "    cluster_df['cluster'] = cluster_labels\n",
    "    \n",
    "    print(f\"\\nüéØ Customer Segment Analysis (K={optimal_k}):\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    for cluster_id in range(optimal_k):\n",
    "        cluster_mask = cluster_labels == cluster_id\n",
    "        cluster_size = np.sum(cluster_mask)\n",
    "        cluster_percentage = (cluster_size / len(cluster_labels)) * 100\n",
    "        \n",
    "        print(f\"\\nüìä SEGMENT {cluster_id + 1}: {cluster_size:,} customers ({cluster_percentage:.1f}%)\")\n",
    "        \n",
    "        # Calculate segment characteristics\n",
    "        segment_data = cluster_df[cluster_df['cluster'] == cluster_id]\n",
    "        \n",
    "        for feature in clustering_features[:5]:  # Show top 5 features\n",
    "            mean_val = segment_data[feature].mean()\n",
    "            overall_mean = cluster_df[feature].mean()\n",
    "            difference = ((mean_val - overall_mean) / overall_mean) * 100\n",
    "            \n",
    "            if abs(difference) > 10:  # Only show significant differences\n",
    "                direction = \"above\" if difference > 0 else \"below\"\n",
    "                print(f\"  ‚Ä¢ {feature}: {mean_val:.2f} ({abs(difference):.0f}% {direction} average)\")\n",
    "    \n",
    "    # Segment naming based on characteristics\n",
    "    print(\"\\nüè∑Ô∏è Suggested Segment Names:\")\n",
    "    segment_names = [\n",
    "        \"High-Value Loyalists\",\n",
    "        \"Price-Conscious Shoppers\", \n",
    "        \"Occasional Purchasers\",\n",
    "        \"Style Enthusiasts\",\n",
    "        \"New Customers\"\n",
    "    ]\n",
    "    \n",
    "    for i in range(optimal_k):\n",
    "        print(f\"  Segment {i + 1}: {segment_names[i] if i < len(segment_names) else f'Segment {i + 1}'}\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå Insufficient features for meaningful clustering analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Customer Lifecycle Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Customer lifecycle and value analysis\n",
    "print(\"üìà Customer Lifecycle and Value Analysis\")\n",
    "\n",
    "if 'recency' in df.columns and 'frequency' in df.columns and 'monetary' in df.columns:\n",
    "    # Create customer value segments based on RFM\n",
    "    value_analysis = df.select([\n",
    "        pl.col('customer_id'),\n",
    "        pl.col('recency'),\n",
    "        pl.col('frequency'), \n",
    "        pl.col('monetary')\n",
    "    ]).with_columns([\n",
    "        # Define customer lifecycle stages\n",
    "        pl.when((pl.col('recency') <= 30) & (pl.col('frequency') >= 3))\n",
    "        .then(pl.lit('Active High-Value'))\n",
    "        .when((pl.col('recency') <= 30) & (pl.col('frequency') < 3))\n",
    "        .then(pl.lit('Active Low-Frequency'))\n",
    "        .when((pl.col('recency') > 30) & (pl.col('recency') <= 90) & (pl.col('frequency') >= 2))\n",
    "        .then(pl.lit('At Risk'))\n",
    "        .when((pl.col('recency') > 90) & (pl.col('frequency') >= 3))\n",
    "        .then(pl.lit('Dormant High-Value'))\n",
    "        .when(pl.col('frequency') == 1)\n",
    "        .then(pl.lit('One-Time Purchaser'))\n",
    "        .otherwise(pl.lit('Inactive'))\n",
    "        .alias('lifecycle_stage'),\n",
    "        \n",
    "        # Customer value tiers\n",
    "        pl.when(pl.col('monetary') >= pl.col('monetary').quantile(0.8))\n",
    "        .then(pl.lit('High Value'))\n",
    "        .when(pl.col('monetary') >= pl.col('monetary').quantile(0.5))\n",
    "        .then(pl.lit('Medium Value'))\n",
    "        .otherwise(pl.lit('Low Value'))\n",
    "        .alias('value_tier')\n",
    "    ])\n",
    "    \n",
    "    # Lifecycle stage distribution\n",
    "    lifecycle_dist = (\n",
    "        value_analysis.group_by('lifecycle_stage')\n",
    "        .agg([\n",
    "            pl.len().alias('customer_count'),\n",
    "            pl.col('monetary').sum().alias('total_value'),\n",
    "            pl.col('monetary').mean().alias('avg_value')\n",
    "        ])\n",
    "        .with_columns([\n",
    "            (pl.col('customer_count') / value_analysis.height * 100).alias('percentage')\n",
    "        ])\n",
    "        .sort('customer_count', descending=True)\n",
    "    )\n",
    "    \n",
    "    print(\"\\nüîÑ Customer Lifecycle Distribution:\")\n",
    "    for row in lifecycle_dist.iter_rows(named=True):\n",
    "        print(f\"  {row['lifecycle_stage']}: {row['customer_count']:,} ({row['percentage']:.1f}%) - Avg Value: ¬£{row['avg_value']:.2f}\")\n",
    "    \n",
    "    # Value tier distribution\n",
    "    value_tier_dist = (\n",
    "        value_analysis.group_by('value_tier')\n",
    "        .agg([\n",
    "            pl.len().alias('customer_count'),\n",
    "            pl.col('monetary').sum().alias('total_revenue')\n",
    "        ])\n",
    "        .with_columns([\n",
    "            (pl.col('customer_count') / value_analysis.height * 100).alias('customer_percentage'),\n",
    "            (pl.col('total_revenue') / pl.col('total_revenue').sum() * 100).alias('revenue_percentage')\n",
    "        ])\n",
    "    )\n",
    "    \n",
    "    print(\"\\nüíé Customer Value Tier Analysis:\")\n",
    "    for row in value_tier_dist.iter_rows(named=True):\n",
    "        print(f\"  {row['value_tier']}: {row['customer_count']:,} customers ({row['customer_percentage']:.1f}%) - Revenue: ¬£{row['total_revenue']:,.2f} ({row['revenue_percentage']:.1f}%)\")\n",
    "    \n",
    "    # Create lifecycle visualisation\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Lifecycle stage pie chart\n",
    "    lifecycle_pd = lifecycle_dist.to_pandas()\n",
    "    ax1.pie(lifecycle_pd['customer_count'], labels=lifecycle_pd['lifecycle_stage'], autopct='%1.1f%%')\n",
    "    ax1.set_title('Customer Lifecycle Distribution')\n",
    "    \n",
    "    # Value tier analysis\n",
    "    value_tier_pd = value_tier_dist.to_pandas()\n",
    "    x = np.arange(len(value_tier_pd))\n",
    "    width = 0.35\n",
    "    \n",
    "    ax2_twin = ax2.twinx()\n",
    "    bars1 = ax2.bar(x - width/2, value_tier_pd['customer_percentage'], width, label='% Customers', alpha=0.7)\n",
    "    bars2 = ax2_twin.bar(x + width/2, value_tier_pd['revenue_percentage'], width, label='% Revenue', alpha=0.7, color='orange')\n",
    "    \n",
    "    ax2.set_title('Customer vs Revenue Distribution by Value Tier')\n",
    "    ax2.set_xlabel('Value Tier')\n",
    "    ax2.set_ylabel('% of Customers', color='blue')\n",
    "    ax2_twin.set_ylabel('% of Revenue', color='orange')\n",
    "    ax2.set_xticks(x)\n",
    "    ax2.set_xticklabels(value_tier_pd['value_tier'])\n",
    "    ax2.legend(loc='upper left')\n",
    "    ax2_twin.legend(loc='upper right')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå RFM features not available for lifecycle analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Interactive Visualisations and Advanced Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interactive visualisations using Plotly\n",
    "print(\"üìä Interactive Customer Analytics Dashboard\")\n",
    "\n",
    "if len(clustering_features) >= 3:\n",
    "    # Interactive 3D scatter plot of customer segments\n",
    "    sample_size = min(5000, df.height)  # Sample for performance\n",
    "    sample_indices = np.random.choice(df.height, sample_size, replace=False)\n",
    "    \n",
    "    plot_data = df.select(['customer_id'] + clustering_features[:3]).to_pandas().iloc[sample_indices]\n",
    "    \n",
    "    if 'cluster_labels' in locals():\n",
    "        plot_data['cluster'] = cluster_labels[sample_indices]\n",
    "    else:\n",
    "        plot_data['cluster'] = 0\n",
    "    \n",
    "    fig_3d = px.scatter_3d(\n",
    "        plot_data,\n",
    "        x=clustering_features[0],\n",
    "        y=clustering_features[1],\n",
    "        z=clustering_features[2],\n",
    "        color='cluster',\n",
    "        title='3D Customer Segmentation View',\n",
    "        hover_data=['customer_id'],\n",
    "        color_continuous_scale='viridis'\n",
    "    )\n",
    "    \n",
    "    fig_3d.update_layout(height=600)\n",
    "    fig_3d.show()\n",
    "\n",
    "# Interactive correlation heatmap\n",
    "if len(correlation_features) > 1:\n",
    "    corr_data = df.select(correlation_features).to_pandas()\n",
    "    correlation_matrix = corr_data.corr()\n",
    "    \n",
    "    fig_corr = px.imshow(\n",
    "        correlation_matrix,\n",
    "        text_auto=True,\n",
    "        aspect=\"auto\",\n",
    "        title=\"Interactive Customer Feature Correlation Matrix\",\n",
    "        color_continuous_scale='RdBu_r',\n",
    "        color_continuous_midpoint=0\n",
    "    )\n",
    "    \n",
    "    fig_corr.update_layout(height=600)\n",
    "    fig_corr.show()\n",
    "\n",
    "# Interactive RFM analysis\n",
    "if all(f in df.columns for f in ['recency', 'frequency', 'monetary']):\n",
    "    rfm_sample = df.select(['customer_id', 'recency', 'frequency', 'monetary']).to_pandas().sample(min(2000, df.height))\n",
    "    \n",
    "    fig_rfm = px.scatter_3d(\n",
    "        rfm_sample,\n",
    "        x='recency',\n",
    "        y='frequency', \n",
    "        z='monetary',\n",
    "        title='Interactive RFM Analysis',\n",
    "        hover_data=['customer_id'],\n",
    "        labels={\n",
    "            'recency': 'Recency (days)',\n",
    "            'frequency': 'Frequency (transactions)',\n",
    "            'monetary': 'Monetary Value (¬£)'\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    fig_rfm.update_layout(height=600)\n",
    "    fig_rfm.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Customer Insights and Business Recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive customer insights and business recommendations\n",
    "print(\"üí° Customer Insights and Business Recommendations\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Calculate key customer metrics\n",
    "total_customers = df.height\n",
    "\n",
    "if 'frequency' in df.columns:\n",
    "    active_customers = df.filter(pl.col('frequency') > 0).height\n",
    "    repeat_customers = df.filter(pl.col('frequency') > 1).height\n",
    "    one_time_customers = df.filter(pl.col('frequency') == 1).height\n",
    "    \n",
    "    print(\"üéØ CUSTOMER BASE ANALYSIS:\")\n",
    "    print(f\"‚Ä¢ Total Customers: {total_customers:,}\")\n",
    "    print(f\"‚Ä¢ Active Customers: {active_customers:,} ({active_customers/total_customers*100:.1f}%)\")\n",
    "    print(f\"‚Ä¢ Repeat Customers: {repeat_customers:,} ({repeat_customers/total_customers*100:.1f}%)\")\n",
    "    print(f\"‚Ä¢ One-Time Purchasers: {one_time_customers:,} ({one_time_customers/total_customers*100:.1f}%)\")\n",
    "    \n",
    "    # Customer retention insights\n",
    "    retention_rate = (repeat_customers / active_customers) * 100\n",
    "    print(f\"‚Ä¢ Customer Retention Rate: {retention_rate:.1f}%\")\n",
    "\n",
    "if 'monetary' in df.columns:\n",
    "    # Revenue concentration analysis\n",
    "    top_20_pct_threshold = df.select(pl.col('monetary').quantile(0.8)).item()\n",
    "    top_20_pct_customers = df.filter(pl.col('monetary') >= top_20_pct_threshold).height\n",
    "    top_20_pct_revenue = df.filter(pl.col('monetary') >= top_20_pct_threshold).select(pl.col('monetary').sum()).item()\n",
    "    total_revenue = df.select(pl.col('monetary').sum()).item()\n",
    "    \n",
    "    print(\"\\nüí∞ REVENUE CONCENTRATION:\")\n",
    "    print(f\"‚Ä¢ Top 20% Customers: {top_20_pct_customers:,} customers\")\n",
    "    print(f\"‚Ä¢ Top 20% Revenue Share: ¬£{top_20_pct_revenue:,.2f} ({top_20_pct_revenue/total_revenue*100:.1f}% of total)\")\n",
    "    print(f\"‚Ä¢ Average Customer Value: ¬£{total_revenue/total_customers:.2f}\")\n",
    "    print(f\"‚Ä¢ Customer Value Disparity: {df.select(pl.col('monetary').max()).item() / df.select(pl.col('monetary').mean()).item():.1f}x (max vs average)\")\n",
    "\n",
    "# Behavioural insights\n",
    "if available_behavioural:\n",
    "    print(\"\\nüß† BEHAVIOURAL INSIGHTS:\")\n",
    "    \n",
    "    for feature in available_behavioural:\n",
    "        feature_mean = df.select(pl.col(feature).mean()).item()\n",
    "        feature_std = df.select(pl.col(feature).std()).item()\n",
    "        \n",
    "        if 'diversity' in feature:\n",
    "            diversity_level = \"High\" if feature_mean > 2.0 else \"Medium\" if feature_mean > 1.0 else \"Low\"\n",
    "            print(f\"‚Ä¢ Purchase Diversity: {diversity_level} (mean: {feature_mean:.2f})\")\n",
    "            \n",
    "        elif 'sensitivity' in feature:\n",
    "            sensitivity_level = \"High\" if feature_mean > 0.5 else \"Medium\" if feature_mean > 0.2 else \"Low\" \n",
    "            print(f\"‚Ä¢ Price Sensitivity: {sensitivity_level} (mean: {feature_mean:.2f})\")\n",
    "            \n",
    "        elif 'consistency' in feature:\n",
    "            consistency_level = \"High\" if feature_mean > 0.7 else \"Medium\" if feature_mean > 0.4 else \"Low\"\n",
    "            print(f\"‚Ä¢ Style Consistency: {consistency_level} (mean: {feature_mean:.2f})\")\n",
    "\n",
    "# Strategic recommendations\n",
    "print(\"\\nüéØ STRATEGIC RECOMMENDATIONS:\")\n",
    "\n",
    "if 'frequency' in df.columns:\n",
    "    if retention_rate < 50:\n",
    "        print(\"‚Ä¢ üö® PRIORITY: Improve customer retention - current rate is below 50%\")\n",
    "        print(\"  - Implement loyalty programmes\")\n",
    "        print(\"  - Develop personalised re-engagement campaigns\")\n",
    "        print(\"  - Analyse churn patterns for early intervention\")\n",
    "    \n",
    "    if one_time_customers / total_customers > 0.6:\n",
    "        print(\"‚Ä¢ üéØ Focus on converting one-time purchasers to repeat customers\")\n",
    "        print(\"  - Post-purchase follow-up campaigns\")\n",
    "        print(\"  - Personalised product recommendations\")\n",
    "        print(\"  - Time-sensitive offers for second purchase\")\n",
    "\n",
    "if 'monetary' in df.columns and top_20_pct_revenue / total_revenue > 0.6:\n",
    "    print(\"‚Ä¢ üíé Develop VIP programmes for high-value customers\")\n",
    "    print(\"  - Exclusive access to new collections\")\n",
    "    print(\"  - Personal shopping services\")\n",
    "    print(\"  - Premium customer support\")\n",
    "\n",
    "if 'purchase_diversity_score' in df.columns:\n",
    "    diversity_mean = df.select(pl.col('purchase_diversity_score').mean()).item()\n",
    "    if diversity_mean < 1.5:\n",
    "        print(\"‚Ä¢ üõçÔ∏è Encourage cross-category shopping\")\n",
    "        print(\"  - Bundle recommendations\")\n",
    "        print(\"  - Cross-category promotions\")\n",
    "        print(\"  - Style guides and outfit suggestions\")\n",
    "\n",
    "print(\"\\nüìä ANALYTICS RECOMMENDATIONS:\")\n",
    "print(\"‚Ä¢ Implement real-time customer scoring based on RFM analysis\")\n",
    "print(\"‚Ä¢ Set up automated alerts for at-risk high-value customers\")\n",
    "print(\"‚Ä¢ Develop predictive models for customer lifetime value\")\n",
    "print(\"‚Ä¢ Create dynamic segmentation for personalised marketing\")\n",
    "print(\"‚Ä¢ Monitor behavioural feature trends over time\")\n",
    "\n",
    "print(\"\\nüéâ ANALYSIS COMPLETE\")\n",
    "print(f\"‚úÖ Analysed {df.height:,} customer records\")\n",
    "print(f\"‚úÖ Generated insights across {len(correlation_features)} features\")\n",
    "print(f\"‚úÖ Identified customer segments and lifecycle stages\")\n",
    "print(f\"‚úÖ Provided actionable business recommendations\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}