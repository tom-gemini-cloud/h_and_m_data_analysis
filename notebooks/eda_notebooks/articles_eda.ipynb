{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# H&M Article Features - Exploratory Data Analysis\n",
    "\n",
    "This notebook performs comprehensive EDA on the engineered article features dataset to understand:\n",
    "\n",
    "- Product catalogue characteristics and diversity\n",
    "- Feature engineering quality and distributions\n",
    "- Product category patterns and relationships\n",
    "- Text-based feature insights (BERT/TF-IDF embeddings)\n",
    "- Product performance indicators and clustering opportunities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import silhouette_score\n",
    "from scipy import stats\n",
    "from collections import Counter\n",
    "\n",
    "# Add project root to path\n",
    "import sys\n",
    "project_root = Path.cwd().parent.parent  # Go up two levels from notebooks/eda_notebooks/\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "try:\n",
    "    from hnm_data_analysis.exploratory_data_analysis.eda_module import EDAModule\n",
    "    print(\"‚úÖ EDA module imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"‚ö†Ô∏è Could not import EDA module: {e}\")\n",
    "    print(\"Continuing without EDA module...\")\n",
    "\n",
    "# Configure plotting for UK publication standards\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette('viridis')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"üì¶ Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load article features data\n",
    "data_path = \"../../data/features/final/articles_features_final.parquet\"\n",
    "print(f\"Loading article features data from: {data_path}\")\n",
    "\n",
    "try:\n",
    "    df = pl.read_parquet(data_path)\n",
    "    print(f\"‚úÖ Data loaded successfully: {df.shape[0]:,} articles √ó {df.shape[1]} features\")\n",
    "    print(f\"Memory usage: {df.estimated_size('mb'):.1f} MB\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error loading data: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset Structure and Quality Assessment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive dataset overview\n",
    "print(\"üìä Article Features Dataset Overview\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Total articles: {df.height:,}\")\n",
    "print(f\"Total features: {df.width}\")\n",
    "print(f\"Column names: {list(df.columns)}\")\n",
    "print(f\"Data types: {dict(zip(df.columns, [str(dtype) for dtype in df.dtypes]))}\")\n",
    "\n",
    "# Identify feature categories\n",
    "categorical_features = [col for col in df.columns if 'name' in col.lower() or 'group' in col.lower()]\n",
    "numeric_features = df.select(pl.col(pl.NUMERIC_DTYPES)).columns\n",
    "text_embedding_features = [col for col in df.columns if any(prefix in col for prefix in ['bert_', 'tfidf_', 'svd_', 'pca_'])]\n",
    "\n",
    "print(f\"\\nüè∑Ô∏è Feature Categories:\")\n",
    "print(f\"‚Ä¢ Categorical features: {len(categorical_features)} - {categorical_features}\")\n",
    "print(f\"‚Ä¢ Numeric features: {len(numeric_features)}\")\n",
    "print(f\"‚Ä¢ Text embedding features: {len(text_embedding_features)}\")\n",
    "\n",
    "# Display sample data\n",
    "print(\"\\nüîç Sample Article Features:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data quality assessment\n",
    "print(\"üîç Data Quality Assessment\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Missing values analysis\n",
    "quality_summary = []\n",
    "for col in df.columns:\n",
    "    null_count = df.select(pl.col(col).null_count()).item()\n",
    "    null_pct = (null_count / df.height) * 100\n",
    "    \n",
    "    # Additional quality checks\n",
    "    unique_count = df.select(pl.col(col).n_unique()).item()\n",
    "    uniqueness_pct = (unique_count / df.height) * 100\n",
    "    \n",
    "    quality_summary.append({\n",
    "        'Feature': col,\n",
    "        'Missing Count': null_count,\n",
    "        'Missing %': f\"{null_pct:.2f}%\",\n",
    "        'Unique Values': unique_count,\n",
    "        'Uniqueness %': f\"{uniqueness_pct:.1f}%\"\n",
    "    })\n",
    "\n",
    "quality_df = pd.DataFrame(quality_summary)\n",
    "print(quality_df.to_string(index=False))\n",
    "\n",
    "# Check for duplicates\n",
    "duplicate_count = df.height - df.n_unique()\n",
    "print(f\"\\nüîÑ Duplicate article records: {duplicate_count:,}\")\n",
    "\n",
    "# Feature completeness\n",
    "complete_records = df.filter(pl.all_horizontal(pl.all().is_not_null())).height\n",
    "completeness_rate = (complete_records / df.height) * 100\n",
    "print(f\"üìã Complete feature records: {complete_records:,} ({completeness_rate:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Categorical Features Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive categorical features analysis\n",
    "print(\"üè∑Ô∏è Categorical Features Analysis\")\n",
    "\n",
    "# Analyse each categorical feature\n",
    "for feature in categorical_features:\n",
    "    if feature in df.columns:\n",
    "        print(f\"\\nüìä {feature.upper()}:\")\n",
    "        \n",
    "        # Value counts and distribution\n",
    "        value_counts = (\n",
    "            df.group_by(feature)\n",
    "            .count()\n",
    "            .sort('count', descending=True)\n",
    "            .head(10)\n",
    "        )\n",
    "        \n",
    "        total_unique = df.select(pl.col(feature).n_unique()).item()\n",
    "        print(f\"  Total unique values: {total_unique}\")\n",
    "        print(f\"  Top 10 categories:\")\n",
    "        \n",
    "        for row in value_counts.iter_rows(named=True):\n",
    "            percentage = (row['count'] / df.height) * 100\n",
    "            print(f\"    {row[feature]}: {row['count']:,} ({percentage:.1f}%)\")\n",
    "\n",
    "# Create visualisations for categorical features\n",
    "if categorical_features:\n",
    "    n_features = min(len(categorical_features), 4)  # Limit to 4 for display\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    axes = axes.flatten()\n",
    "    fig.suptitle('Categorical Features Distribution', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    for i, feature in enumerate(categorical_features[:n_features]):\n",
    "        if feature in df.columns:\n",
    "            # Get top categories for visualisation\n",
    "            top_categories = (\n",
    "                df.group_by(feature)\n",
    "                .count()\n",
    "                .sort('count', descending=True)\n",
    "                .head(15)  # Top 15 for readability\n",
    "                .to_pandas()\n",
    "            )\n",
    "            \n",
    "            # Create bar plot\n",
    "            axes[i].bar(range(len(top_categories)), top_categories['count'], alpha=0.7)\n",
    "            axes[i].set_title(f'{feature.replace(\"_\", \" \").title()}')\n",
    "            axes[i].set_xlabel('Categories')\n",
    "            axes[i].set_ylabel('Count')\n",
    "            axes[i].tick_params(axis='x', rotation=45)\n",
    "            axes[i].grid(True, alpha=0.3)\n",
    "            \n",
    "            # Set x-axis labels (abbreviated if too long)\n",
    "            labels = [label[:20] + '...' if len(str(label)) > 20 else str(label) \n",
    "                     for label in top_categories[feature]]\n",
    "            axes[i].set_xticks(range(len(top_categories)))\n",
    "            axes[i].set_xticklabels(labels, rotation=45, ha='right')\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for i in range(n_features, 4):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Numeric Features Distribution Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numeric features comprehensive analysis\n",
    "print(\"üìà Numeric Features Distribution Analysis\")\n",
    "\n",
    "# Filter out article_id and other non-feature numeric columns\n",
    "analysis_numeric_features = [col for col in numeric_features \n",
    "                           if col not in ['article_id'] and not col.endswith('_id')]\n",
    "\n",
    "print(f\"Analysing {len(analysis_numeric_features)} numeric features\")\n",
    "\n",
    "if analysis_numeric_features:\n",
    "    # Calculate comprehensive statistics\n",
    "    for feature in analysis_numeric_features[:10]:  # Limit output for readability\n",
    "        if feature in df.columns:\n",
    "            stats_data = df.select([\n",
    "                pl.col(feature).count().alias('count'),\n",
    "                pl.col(feature).mean().alias('mean'),\n",
    "                pl.col(feature).median().alias('median'),\n",
    "                pl.col(feature).std().alias('std'),\n",
    "                pl.col(feature).min().alias('min'),\n",
    "                pl.col(feature).max().alias('max'),\n",
    "                pl.col(feature).quantile(0.25).alias('Q1'),\n",
    "                pl.col(feature).quantile(0.75).alias('Q3'),\n",
    "                pl.col(feature).quantile(0.95).alias('P95'),\n",
    "                pl.col(feature).quantile(0.99).alias('P99')\n",
    "            ]).to_dict(as_series=False)\n",
    "            \n",
    "            print(f\"\\nüéØ {feature.upper()}:\")\n",
    "            for stat, value in stats_data.items():\n",
    "                if stat == 'count':\n",
    "                    print(f\"  {stat}: {value[0]:,}\")\n",
    "                else:\n",
    "                    print(f\"  {stat}: {value[0]:.4f}\")\n",
    "\n",
    "    # Create distribution plots for key numeric features\n",
    "    plot_features = analysis_numeric_features[:6]  # First 6 features\n",
    "    \n",
    "    if plot_features:\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "        axes = axes.flatten()\n",
    "        fig.suptitle('Numeric Features Distribution Analysis', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        for i, feature in enumerate(plot_features):\n",
    "            if feature in df.columns and i < 6:\n",
    "                # Convert to pandas for plotting\n",
    "                feature_data = df.select(feature).to_pandas()[feature].dropna()\n",
    "                \n",
    "                # Create histogram\n",
    "                axes[i].hist(feature_data, bins=50, alpha=0.7, edgecolor='black')\n",
    "                axes[i].set_title(f'{feature.replace(\"_\", \" \").title()}')\n",
    "                axes[i].set_xlabel(feature)\n",
    "                axes[i].set_ylabel('Frequency')\n",
    "                axes[i].grid(True, alpha=0.3)\n",
    "                \n",
    "                # Add statistics annotation\n",
    "                mean_val = feature_data.mean()\n",
    "                std_val = feature_data.std()\n",
    "                axes[i].axvline(mean_val, color='red', linestyle='--', alpha=0.7, label=f'Mean: {mean_val:.3f}')\n",
    "                axes[i].legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "else:\n",
    "    print(\"‚ùå No suitable numeric features found for analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Text Embedding Features Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text embedding features analysis\n",
    "print(\"ü§ñ Text Embedding Features Analysis\")\n",
    "\n",
    "if text_embedding_features:\n",
    "    print(f\"Found {len(text_embedding_features)} text embedding features\")\n",
    "    \n",
    "    # Categorise embedding types\n",
    "    bert_features = [f for f in text_embedding_features if 'bert' in f.lower()]\n",
    "    tfidf_features = [f for f in text_embedding_features if 'tfidf' in f.lower()]\n",
    "    svd_features = [f for f in text_embedding_features if 'svd' in f.lower()]\n",
    "    pca_features = [f for f in text_embedding_features if 'pca' in f.lower()]\n",
    "    \n",
    "    print(f\"\\nüìä Embedding Feature Categories:\")\n",
    "    print(f\"‚Ä¢ BERT features: {len(bert_features)}\")\n",
    "    print(f\"‚Ä¢ TF-IDF features: {len(tfidf_features)}\")\n",
    "    print(f\"‚Ä¢ SVD features: {len(svd_features)}\")\n",
    "    print(f\"‚Ä¢ PCA features: {len(pca_features)}\")\n",
    "    \n",
    "    # Analyse embedding feature statistics\n",
    "    sample_embeddings = text_embedding_features[:20]  # Sample for analysis\n",
    "    \n",
    "    if sample_embeddings:\n",
    "        embedding_stats = []\n",
    "        \n",
    "        for feature in sample_embeddings:\n",
    "            if feature in df.columns:\n",
    "                stats = df.select([\n",
    "                    pl.col(feature).mean().alias('mean'),\n",
    "                    pl.col(feature).std().alias('std'),\n",
    "                    pl.col(feature).min().alias('min'),\n",
    "                    pl.col(feature).max().alias('max')\n",
    "                ]).to_dict(as_series=False)\n",
    "                \n",
    "                embedding_stats.append({\n",
    "                    'Feature': feature,\n",
    "                    'Mean': f\"{stats['mean'][0]:.4f}\",\n",
    "                    'Std': f\"{stats['std'][0]:.4f}\",\n",
    "                    'Range': f\"[{stats['min'][0]:.3f}, {stats['max'][0]:.3f}]\"\n",
    "                })\n",
    "        \n",
    "        embedding_df = pd.DataFrame(embedding_stats)\n",
    "        print(f\"\\nüìà Sample Embedding Statistics:\")\n",
    "        print(embedding_df.head(10).to_string(index=False))\n",
    "    \n",
    "    # Visualise embedding feature distributions\n",
    "    if len(sample_embeddings) >= 4:\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        axes = axes.flatten()\n",
    "        fig.suptitle('Text Embedding Features Distribution', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        for i in range(4):\n",
    "            if i < len(sample_embeddings):\n",
    "                feature = sample_embeddings[i]\n",
    "                if feature in df.columns:\n",
    "                    feature_data = df.select(feature).to_pandas()[feature].dropna()\n",
    "                    \n",
    "                    axes[i].hist(feature_data, bins=50, alpha=0.7, edgecolor='black')\n",
    "                    axes[i].set_title(f'{feature}')\n",
    "                    axes[i].set_xlabel('Value')\n",
    "                    axes[i].set_ylabel('Frequency')\n",
    "                    axes[i].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # Embedding space dimensionality analysis\n",
    "    if len(text_embedding_features) > 10:\n",
    "        print(f\"\\nüîç Embedding Space Analysis:\")\n",
    "        \n",
    "        # Sample embeddings for PCA analysis\n",
    "        embedding_sample = text_embedding_features[:50]  # Use first 50 dimensions\n",
    "        \n",
    "        try:\n",
    "            # Convert to pandas for PCA\n",
    "            embedding_data = df.select(embedding_sample).to_pandas().fillna(0)\n",
    "            \n",
    "            # Perform PCA\n",
    "            pca = PCA(n_components=min(10, len(embedding_sample)))\n",
    "            pca_result = pca.fit_transform(embedding_data)\n",
    "            \n",
    "            # Plot explained variance\n",
    "            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "            \n",
    "            # Explained variance ratio\n",
    "            ax1.bar(range(1, len(pca.explained_variance_ratio_) + 1), pca.explained_variance_ratio_)\n",
    "            ax1.set_title('PCA Explained Variance Ratio')\n",
    "            ax1.set_xlabel('Principal Component')\n",
    "            ax1.set_ylabel('Explained Variance Ratio')\n",
    "            ax1.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Cumulative explained variance\n",
    "            cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "            ax2.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, 'bo-')\n",
    "            ax2.set_title('Cumulative Explained Variance')\n",
    "            ax2.set_xlabel('Number of Components')\n",
    "            ax2.set_ylabel('Cumulative Explained Variance')\n",
    "            ax2.grid(True, alpha=0.3)\n",
    "            ax2.axhline(y=0.95, color='r', linestyle='--', alpha=0.7, label='95% Variance')\n",
    "            ax2.legend()\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            print(f\"  First 5 components explain {cumulative_variance[4]:.1%} of variance\")\n",
    "            print(f\"  95% variance captured by {np.argmax(cumulative_variance >= 0.95) + 1} components\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå Error in PCA analysis: {e}\")\n",
    "            \n",
    "else:\n",
    "    print(\"‚ùå No text embedding features found in the dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Correlation Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive feature correlation analysis\n",
    "print(\"üîó Feature Correlation Analysis\")\n",
    "\n",
    "# Select features for correlation analysis (avoid too many embedding features)\n",
    "correlation_features = []\n",
    "\n",
    "# Add non-embedding numeric features\n",
    "non_embedding_numeric = [f for f in analysis_numeric_features if f not in text_embedding_features]\n",
    "correlation_features.extend(non_embedding_numeric[:10])  # Limit to 10\n",
    "\n",
    "# Add sample of embedding features\n",
    "if text_embedding_features:\n",
    "    correlation_features.extend(text_embedding_features[:20])  # Sample 20 embedding features\n",
    "\n",
    "print(f\"Analysing correlations between {len(correlation_features)} features\")\n",
    "\n",
    "if len(correlation_features) > 1:\n",
    "    # Calculate correlation matrix\n",
    "    corr_data = df.select(correlation_features).to_pandas().fillna(0)\n",
    "    correlation_matrix = corr_data.corr()\n",
    "    \n",
    "    # Create correlation heatmap\n",
    "    fig, ax = plt.subplots(figsize=(14, 12))\n",
    "    \n",
    "    # Create mask for upper triangle\n",
    "    mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "    \n",
    "    # Generate heatmap\n",
    "    sns.heatmap(\n",
    "        correlation_matrix,\n",
    "        mask=mask,\n",
    "        annot=False,  # Too many features for annotations\n",
    "        cmap='RdBu_r',\n",
    "        center=0,\n",
    "        square=True,\n",
    "        cbar_kws={'shrink': 0.8},\n",
    "        ax=ax\n",
    "    )\n",
    "    \n",
    "    ax.set_title('Article Feature Correlation Matrix', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Identify strongest correlations\n",
    "    print(\"\\nüîç Strongest Feature Correlations (|r| > 0.5):\")\n",
    "    strong_correlations = []\n",
    "    \n",
    "    for i in range(len(correlation_matrix.columns)):\n",
    "        for j in range(i+1, len(correlation_matrix.columns)):\n",
    "            corr_value = correlation_matrix.iloc[i, j]\n",
    "            if abs(corr_value) > 0.5:  # Higher threshold for article features\n",
    "                strong_correlations.append({\n",
    "                    'Feature 1': correlation_matrix.columns[i],\n",
    "                    'Feature 2': correlation_matrix.columns[j],\n",
    "                    'Correlation': corr_value\n",
    "                })\n",
    "    \n",
    "    # Sort by absolute correlation strength\n",
    "    strong_correlations.sort(key=lambda x: abs(x['Correlation']), reverse=True)\n",
    "    \n",
    "    if strong_correlations:\n",
    "        for corr in strong_correlations[:15]:  # Top 15\n",
    "            print(f\"  {corr['Feature 1']} ‚Üî {corr['Feature 2']}: {corr['Correlation']:.3f}\")\n",
    "    else:\n",
    "        print(\"  No strong correlations (|r| > 0.5) found between features.\")\n",
    "        \n",
    "    # Feature redundancy analysis\n",
    "    highly_corr_pairs = [corr for corr in strong_correlations if abs(corr['Correlation']) > 0.8]\n",
    "    \n",
    "    if highly_corr_pairs:\n",
    "        print(f\"\\n‚ö†Ô∏è Potentially Redundant Features ({len(highly_corr_pairs)} pairs with |r| > 0.8):\")\n",
    "        for corr in highly_corr_pairs[:10]:\n",
    "            print(f\"  {corr['Feature 1']} ‚Üî {corr['Feature 2']}: {corr['Correlation']:.3f}\")\n",
    "            \n",
    "else:\n",
    "    print(\"‚ùå Insufficient features for correlation analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Article Clustering Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Article clustering analysis for product segmentation\n",
    "print(\"üéØ Article Clustering Analysis\")\n",
    "\n",
    "# Prepare clustering features (use a mix of different feature types)\n",
    "clustering_features = []\n",
    "\n",
    "# Add non-embedding numeric features\n",
    "if non_embedding_numeric:\n",
    "    clustering_features.extend(non_embedding_numeric[:5])\n",
    "\n",
    "# Add embedding features\n",
    "if text_embedding_features:\n",
    "    clustering_features.extend(text_embedding_features[:15])  # Sample embedding features\n",
    "\n",
    "print(f\"Using {len(clustering_features)} features for clustering: {clustering_features[:10]}...\")\n",
    "\n",
    "if len(clustering_features) >= 3:\n",
    "    # Prepare clustering data\n",
    "    cluster_data = df.select(clustering_features).to_pandas().fillna(0)\n",
    "    \n",
    "    # Standardise features\n",
    "    scaler = StandardScaler()\n",
    "    scaled_data = scaler.fit_transform(cluster_data)\n",
    "    \n",
    "    # Determine optimal number of clusters using multiple methods\n",
    "    max_k = min(15, df.height // 100)  # Reasonable upper bound\n",
    "    k_range = range(2, max_k + 1)\n",
    "    \n",
    "    inertias = []\n",
    "    silhouette_scores = []\n",
    "    \n",
    "    print(f\"\\nüîç Evaluating cluster numbers from 2 to {max_k}...\")\n",
    "    \n",
    "    for k in k_range:\n",
    "        # K-means clustering\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "        cluster_labels = kmeans.fit_predict(scaled_data)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        inertias.append(kmeans.inertia_)\n",
    "        \n",
    "        # Silhouette score (sample for large datasets)\n",
    "        if len(scaled_data) > 5000:\n",
    "            sample_indices = np.random.choice(len(scaled_data), 5000, replace=False)\n",
    "            sil_score = silhouette_score(scaled_data[sample_indices], cluster_labels[sample_indices])\n",
    "        else:\n",
    "            sil_score = silhouette_score(scaled_data, cluster_labels)\n",
    "        \n",
    "        silhouette_scores.append(sil_score)\n",
    "    \n",
    "    # Plot clustering evaluation metrics\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Elbow curve\n",
    "    ax1.plot(k_range, inertias, 'bo-')\n",
    "    ax1.set_title('Elbow Method for Optimal K')\n",
    "    ax1.set_xlabel('Number of Clusters')\n",
    "    ax1.set_ylabel('Inertia')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Silhouette scores\n",
    "    ax2.plot(k_range, silhouette_scores, 'ro-')\n",
    "    ax2.set_title('Silhouette Score by Number of Clusters')\n",
    "    ax2.set_xlabel('Number of Clusters')\n",
    "    ax2.set_ylabel('Silhouette Score')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Select optimal K (highest silhouette score)\n",
    "    optimal_k = k_range[np.argmax(silhouette_scores)]\n",
    "    best_silhouette = max(silhouette_scores)\n",
    "    \n",
    "    print(f\"\\nüéØ Optimal number of clusters: {optimal_k} (Silhouette Score: {best_silhouette:.3f})\")\n",
    "    \n",
    "    # Perform final clustering\n",
    "    final_kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "    final_clusters = final_kmeans.fit_predict(scaled_data)\n",
    "    \n",
    "    # Cluster analysis\n",
    "    cluster_df = df.select(['article_id'] + clustering_features).to_pandas()\n",
    "    cluster_df['cluster'] = final_clusters\n",
    "    \n",
    "    print(f\"\\nüìä Article Cluster Analysis (K={optimal_k}):\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    cluster_summary = []\n",
    "    \n",
    "    for cluster_id in range(optimal_k):\n",
    "        cluster_mask = final_clusters == cluster_id\n",
    "        cluster_size = np.sum(cluster_mask)\n",
    "        cluster_percentage = (cluster_size / len(final_clusters)) * 100\n",
    "        \n",
    "        cluster_summary.append({\n",
    "            'Cluster': cluster_id,\n",
    "            'Size': cluster_size,\n",
    "            'Percentage': cluster_percentage\n",
    "        })\n",
    "        \n",
    "        print(f\"\\nüì¶ CLUSTER {cluster_id}: {cluster_size:,} articles ({cluster_percentage:.1f}%)\")\n",
    "        \n",
    "        # Calculate cluster characteristics for non-embedding features\n",
    "        segment_data = cluster_df[cluster_df['cluster'] == cluster_id]\n",
    "        \n",
    "        for feature in non_embedding_numeric[:5]:  # Show top 5 features\n",
    "            if feature in segment_data.columns:\n",
    "                mean_val = segment_data[feature].mean()\n",
    "                overall_mean = cluster_df[feature].mean()\n",
    "                \n",
    "                if not np.isnan(mean_val) and not np.isnan(overall_mean) and overall_mean != 0:\n",
    "                    difference = ((mean_val - overall_mean) / overall_mean) * 100\n",
    "                    \n",
    "                    if abs(difference) > 10:  # Only show significant differences\n",
    "                        direction = \"above\" if difference > 0 else \"below\"\n",
    "                        print(f\"  ‚Ä¢ {feature}: {mean_val:.3f} ({abs(difference):.0f}% {direction} average)\")\n",
    "    \n",
    "    # Visualise clusters using PCA\n",
    "    if len(clustering_features) > 2:\n",
    "        pca = PCA(n_components=2)\n",
    "        pca_data = pca.fit_transform(scaled_data)\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(12, 8))\n",
    "        scatter = ax.scatter(pca_data[:, 0], pca_data[:, 1], c=final_clusters, cmap='viridis', alpha=0.6, s=20)\n",
    "        ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')\n",
    "        ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')\n",
    "        ax.set_title(f'Article Clusters Visualisation (K={optimal_k}, PCA Projection)')\n",
    "        plt.colorbar(scatter, label='Cluster')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"\\nüìà PCA explains {pca.explained_variance_ratio_.sum():.1%} of total variance\")\n",
    "    \n",
    "    # Suggest cluster interpretations based on categorical features\n",
    "    if categorical_features:\n",
    "        print(f\"\\nüè∑Ô∏è Cluster Interpretation (based on categorical features):\")\n",
    "        \n",
    "        for cluster_id in range(min(optimal_k, 5)):  # Show first 5 clusters\n",
    "            cluster_articles = df.filter(pl.col('article_id').is_in(\n",
    "                cluster_df[cluster_df['cluster'] == cluster_id]['article_id'].tolist()\n",
    "            ))\n",
    "            \n",
    "            print(f\"\\n  CLUSTER {cluster_id}:\")\n",
    "            \n",
    "            for cat_feature in categorical_features[:3]:  # Top 3 categorical features\n",
    "                if cat_feature in cluster_articles.columns:\n",
    "                    top_categories = (\n",
    "                        cluster_articles.group_by(cat_feature)\n",
    "                        .count()\n",
    "                        .sort('count', descending=True)\n",
    "                        .head(3)\n",
    "                    )\n",
    "                    \n",
    "                    print(f\"    Top {cat_feature}: \", end=\"\")\n",
    "                    for row in top_categories.iter_rows(named=True):\n",
    "                        pct = (row['count'] / cluster_articles.height) * 100\n",
    "                        print(f\"{row[cat_feature]} ({pct:.0f}%), \", end=\"\")\n",
    "                    print()  # New line\n",
    "    \n",
    "else:\n",
    "    print(\"‚ùå Insufficient features for meaningful clustering analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Interactive Visualisations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interactive visualisations using Plotly\n",
    "print(\"üìä Interactive Article Analytics\")\n",
    "\n",
    "# Interactive categorical feature distribution\n",
    "if categorical_features:\n",
    "    for feature in categorical_features[:2]:  # Show first 2 categorical features\n",
    "        if feature in df.columns:\n",
    "            # Get category counts\n",
    "            category_counts = (\n",
    "                df.group_by(feature)\n",
    "                .count()\n",
    "                .sort('count', descending=True)\n",
    "                .head(20)  # Top 20 categories\n",
    "                .to_pandas()\n",
    "            )\n",
    "            \n",
    "            # Create interactive bar chart\n",
    "            fig = px.bar(\n",
    "                category_counts,\n",
    "                x=feature,\n",
    "                y='count',\n",
    "                title=f'Interactive {feature.replace(\"_\", \" \").title()} Distribution',\n",
    "                hover_data=['count']\n",
    "            )\n",
    "            \n",
    "            fig.update_layout(\n",
    "                xaxis_tickangle=-45,\n",
    "                height=500,\n",
    "                showlegend=False\n",
    "            )\n",
    "            \n",
    "            fig.show()\n",
    "\n",
    "# Interactive correlation heatmap (subset of features)\n",
    "if len(correlation_features) > 2:\n",
    "    # Use subset for interactive visualisation\n",
    "    subset_features = correlation_features[:15]  # First 15 features\n",
    "    corr_subset = df.select(subset_features).to_pandas().fillna(0).corr()\n",
    "    \n",
    "    fig_corr = px.imshow(\n",
    "        corr_subset,\n",
    "        text_auto=True,\n",
    "        aspect=\"auto\",\n",
    "        title=\"Interactive Article Feature Correlation Matrix\",\n",
    "        color_continuous_scale='RdBu_r',\n",
    "        color_continuous_midpoint=0\n",
    "    )\n",
    "    \n",
    "    fig_corr.update_layout(height=600)\n",
    "    fig_corr.show()\n",
    "\n",
    "# Interactive 3D cluster visualisation\n",
    "if 'final_clusters' in locals() and len(clustering_features) >= 3:\n",
    "    # Sample data for performance\n",
    "    sample_size = min(2000, df.height)\n",
    "    sample_indices = np.random.choice(df.height, sample_size, replace=False)\n",
    "    \n",
    "    sample_data = cluster_df.iloc[sample_indices].copy()\n",
    "    \n",
    "    if len(clustering_features) >= 3:\n",
    "        fig_3d = px.scatter_3d(\n",
    "            sample_data,\n",
    "            x=clustering_features[0],\n",
    "            y=clustering_features[1],\n",
    "            z=clustering_features[2],\n",
    "            color='cluster',\n",
    "            title='Interactive 3D Article Clustering View',\n",
    "            hover_data=['article_id'],\n",
    "            color_continuous_scale='viridis'\n",
    "        )\n",
    "        \n",
    "        fig_3d.update_layout(height=600)\n",
    "        fig_3d.show()\n",
    "\n",
    "# Interactive embedding space visualisation (if available)\n",
    "if text_embedding_features and len(text_embedding_features) >= 2:\n",
    "    # Use first two embedding dimensions\n",
    "    embed_sample = df.select(['article_id'] + text_embedding_features[:2]).to_pandas().sample(\n",
    "        min(1000, df.height)\n",
    "    )\n",
    "    \n",
    "    fig_embed = px.scatter(\n",
    "        embed_sample,\n",
    "        x=text_embedding_features[0],\n",
    "        y=text_embedding_features[1],\n",
    "        title='Interactive Embedding Space Visualisation',\n",
    "        hover_data=['article_id'],\n",
    "        opacity=0.6\n",
    "    )\n",
    "    \n",
    "    fig_embed.update_layout(height=500)\n",
    "    fig_embed.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Article Insights and Recommendations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive article insights and recommendations\n",
    "print(\"üí° Article Features Insights and Recommendations\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Basic dataset statistics\n",
    "total_articles = df.height\n",
    "total_features = df.width\n",
    "complete_articles = df.filter(pl.all_horizontal(pl.all().is_not_null())).height\n",
    "\n",
    "print(\"üéØ DATASET OVERVIEW:\")\n",
    "print(f\"‚Ä¢ Total Articles: {total_articles:,}\")\n",
    "print(f\"‚Ä¢ Total Features: {total_features}\")\n",
    "print(f\"‚Ä¢ Complete Records: {complete_articles:,} ({complete_articles/total_articles*100:.1f}%)\")\n",
    "print(f\"‚Ä¢ Feature Categories: {len(categorical_features)} categorical, {len(analysis_numeric_features)} numeric, {len(text_embedding_features)} embeddings\")\n",
    "\n",
    "# Categorical feature diversity\n",
    "if categorical_features:\n",
    "    print(\"\\nüè∑Ô∏è PRODUCT CATALOGUE DIVERSITY:\")\n",
    "    \n",
    "    for feature in categorical_features:\n",
    "        if feature in df.columns:\n",
    "            unique_count = df.select(pl.col(feature).n_unique()).item()\n",
    "            diversity_ratio = unique_count / total_articles\n",
    "            \n",
    "            print(f\"‚Ä¢ {feature.replace('_', ' ').title()}: {unique_count} unique values (diversity: {diversity_ratio:.1%})\")\n",
    "            \n",
    "            # Category concentration analysis\n",
    "            top_category_share = (\n",
    "                df.group_by(feature)\n",
    "                .count()\n",
    "                .sort('count', descending=True)\n",
    "                .head(1)\n",
    "                .select((pl.col('count') / total_articles * 100).alias('percentage'))\n",
    "                .item()\n",
    "            )\n",
    "            \n",
    "            concentration = \"High\" if top_category_share > 50 else \"Medium\" if top_category_share > 25 else \"Low\"\n",
    "            print(f\"  Concentration: {concentration} (top category: {top_category_share:.1f}% of articles)\")\n",
    "\n",
    "# Feature engineering quality assessment\n",
    "if text_embedding_features:\n",
    "    print(\"\\nü§ñ FEATURE ENGINEERING QUALITY:\")\n",
    "    \n",
    "    # Embedding feature statistics\n",
    "    embedding_sample = text_embedding_features[:20]\n",
    "    \n",
    "    if embedding_sample:\n",
    "        # Calculate average variance across embedding dimensions\n",
    "        embedding_data = df.select(embedding_sample).to_pandas().fillna(0)\n",
    "        avg_variance = embedding_data.var().mean()\n",
    "        avg_std = embedding_data.std().mean()\n",
    "        \n",
    "        print(f\"‚Ä¢ Embedding Features: {len(text_embedding_features)} dimensions\")\n",
    "        print(f\"‚Ä¢ Average Variance: {avg_variance:.4f}\")\n",
    "        print(f\"‚Ä¢ Average Standard Deviation: {avg_std:.4f}\")\n",
    "        \n",
    "        # Feature redundancy assessment\n",
    "        if 'strong_correlations' in locals():\n",
    "            high_corr_count = len([c for c in strong_correlations if abs(c['Correlation']) > 0.8])\n",
    "            redundancy_level = \"High\" if high_corr_count > 10 else \"Medium\" if high_corr_count > 5 else \"Low\"\n",
    "            print(f\"‚Ä¢ Feature Redundancy: {redundancy_level} ({high_corr_count} highly correlated pairs)\")\n",
    "\n",
    "# Clustering insights\n",
    "if 'optimal_k' in locals():\n",
    "    print(\"\\nüéØ PRODUCT SEGMENTATION INSIGHTS:\")\n",
    "    print(f\"‚Ä¢ Optimal Clusters: {optimal_k} distinct product segments\")\n",
    "    print(f\"‚Ä¢ Clustering Quality: {best_silhouette:.3f} silhouette score\")\n",
    "    \n",
    "    # Cluster balance analysis\n",
    "    if 'cluster_summary' in locals():\n",
    "        cluster_sizes = [c['Size'] for c in cluster_summary]\n",
    "        size_std = np.std(cluster_sizes)\n",
    "        size_mean = np.mean(cluster_sizes)\n",
    "        balance_ratio = size_std / size_mean\n",
    "        \n",
    "        balance_level = \"Well-balanced\" if balance_ratio < 0.5 else \"Moderately balanced\" if balance_ratio < 1.0 else \"Imbalanced\"\n",
    "        print(f\"‚Ä¢ Cluster Balance: {balance_level} (CV: {balance_ratio:.2f})\")\n",
    "        \n",
    "        largest_cluster = max(cluster_sizes)\n",
    "        smallest_cluster = min(cluster_sizes)\n",
    "        print(f\"‚Ä¢ Size Range: {smallest_cluster:,} - {largest_cluster:,} articles per cluster\")\n",
    "\n",
    "# Strategic recommendations\n",
    "print(\"\\nüéØ STRATEGIC RECOMMENDATIONS:\")\n",
    "\n",
    "# Product catalogue recommendations\n",
    "if categorical_features:\n",
    "    for feature in categorical_features:\n",
    "        if feature in df.columns:\n",
    "            unique_count = df.select(pl.col(feature).n_unique()).item()\n",
    "            \n",
    "            if unique_count < 10:\n",
    "                print(f\"‚Ä¢ üìà Consider expanding {feature.replace('_', ' ')} variety (only {unique_count} categories)\")\n",
    "            elif unique_count > 100:\n",
    "                print(f\"‚Ä¢ üéØ Consider consolidating {feature.replace('_', ' ')} categories ({unique_count} may be too many)\")\n",
    "\n",
    "# Feature engineering recommendations\n",
    "if text_embedding_features:\n",
    "    if len(text_embedding_features) > 100:\n",
    "        print(\"‚Ä¢ üîß Consider dimensionality reduction for embedding features (current: {} dimensions)\".format(len(text_embedding_features)))\n",
    "        print(\"  - Apply PCA or t-SNE for visualisation\")\n",
    "        print(\"  - Use feature selection techniques\")\n",
    "    \n",
    "    if 'high_corr_count' in locals() and high_corr_count > 10:\n",
    "        print(\"‚Ä¢ ‚ö° Address feature redundancy to improve model efficiency\")\n",
    "        print(\"  - Remove highly correlated features\")\n",
    "        print(\"  - Apply regularisation techniques\")\n",
    "\n",
    "# Clustering-based recommendations\n",
    "if 'optimal_k' in locals():\n",
    "    print(f\"‚Ä¢ üõçÔ∏è Leverage {optimal_k} product segments for:\")\n",
    "    print(\"  - Targeted marketing campaigns\")\n",
    "    print(\"  - Inventory management optimisation\")\n",
    "    print(\"  - Product recommendation systems\")\n",
    "    print(\"  - Pricing strategy development\")\n",
    "    \n",
    "    if 'balance_level' in locals() and 'Imbalanced' in balance_level:\n",
    "        print(\"‚Ä¢ ‚öñÔ∏è Address cluster imbalance for better segmentation\")\n",
    "        print(\"  - Investigate dominant clusters\")\n",
    "        print(\"  - Consider alternative clustering algorithms\")\n",
    "        print(\"  - Apply stratified sampling for analysis\")\n",
    "\n",
    "print(\"\\nüìä ANALYTICS RECOMMENDATIONS:\")\n",
    "print(\"‚Ä¢ Implement automated article similarity scoring\")\n",
    "print(\"‚Ä¢ Develop product performance tracking by cluster\")\n",
    "print(\"‚Ä¢ Create embedding-based product search functionality\")\n",
    "print(\"‚Ä¢ Set up monitoring for catalogue diversity metrics\")\n",
    "print(\"‚Ä¢ Build recommendation engines using article features\")\n",
    "\n",
    "print(\"\\nüîç DATA QUALITY RECOMMENDATIONS:\")\n",
    "if complete_articles / total_articles < 0.95:\n",
    "    print(\"‚Ä¢ üö® Improve data completeness - missing values detected\")\n",
    "    print(\"  - Implement data validation pipelines\")\n",
    "    print(\"  - Add imputation strategies for missing features\")\n",
    "\n",
    "print(\"‚Ä¢ Regular feature quality audits\")\n",
    "print(\"‚Ä¢ Monitor embedding feature drift over time\")\n",
    "print(\"‚Ä¢ Validate clustering stability with new data\")\n",
    "\n",
    "print(\"\\nüéâ ANALYSIS COMPLETE\")\n",
    "print(f\"‚úÖ Analysed {total_articles:,} article records\")\n",
    "print(f\"‚úÖ Processed {total_features} engineered features\")\n",
    "print(f\"‚úÖ Identified product segments and patterns\")\n",
    "print(f\"‚úÖ Generated actionable recommendations\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
