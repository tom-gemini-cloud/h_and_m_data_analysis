{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# H&M Article Features - Exploratory Data Analysis\n",
    "\n",
    "This notebook performs comprehensive EDA on the engineered article features dataset to understand:\n",
    "\n",
    "- Product catalogue characteristics and diversity\n",
    "- Feature engineering quality and distributions\n",
    "- Product category patterns and relationships\n",
    "- Text-based feature insights (BERT/TF-IDF embeddings)\n",
    "- Product performance indicators and clustering opportunities\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Data Loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "from datetime import datetime\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA, TruncatedSVD\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.metrics import silhouette_score\n",
    "from scipy import stats\n",
    "from collections import Counter\n",
    "\n",
    "# Add project root to path\n",
    "import sys\n",
    "project_root = Path.cwd().parent.parent  # Go up two levels from notebooks/eda_notebooks/\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "try:\n",
    "    from hnm_data_analysis.exploratory_data_analysis.eda_module import EDAModule\n",
    "    print(\"✅ EDA module imported successfully\")\n",
    "except ImportError as e:\n",
    "    print(f\"⚠️ Could not import EDA module: {e}\")\n",
    "    print(\"Continuing without EDA module...\")\n",
    "\n",
    "# Configure plotting for UK publication standards\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette('viridis')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "plt.rcParams['figure.dpi'] = 100\n",
    "plt.rcParams['font.family'] = 'DejaVu Sans'\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"📦 Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load article features data\n",
    "data_path = \"../../data/features/final/articles_features_final.parquet\"\n",
    "print(f\"Loading article features data from: {data_path}\")\n",
    "\n",
    "try:\n",
    "    df = pl.read_parquet(data_path)\n",
    "    print(f\"✅ Data loaded successfully: {df.shape[0]:,} articles × {df.shape[1]} features\")\n",
    "    print(f\"Memory usage: {df.estimated_size('mb'):.1f} MB\")\n",
    "except Exception as e:\n",
    "    print(f\"❌ Error loading data: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Dataset Structure and Quality Assessment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive dataset overview\n",
    "print(\"📊 Article Features Dataset Overview\")\n",
    "print(\"=\" * 50)\n",
    "print(f\"Shape: {df.shape}\")\n",
    "print(f\"Total articles: {df.height:,}\")\n",
    "print(f\"Total features: {df.width}\")\n",
    "print(f\"Column names: {list(df.columns)}\")\n",
    "print(f\"Data types: {dict(zip(df.columns, [str(dtype) for dtype in df.dtypes]))}\")\n",
    "\n",
    "# Identify feature categories\n",
    "categorical_features = [col for col in df.columns if 'name' in col.lower() or 'group' in col.lower()]\n",
    "numeric_features = df.select(pl.col(pl.NUMERIC_DTYPES)).columns\n",
    "text_embedding_features = [col for col in df.columns if any(prefix in col for prefix in ['bert_', 'tfidf_', 'svd_', 'pca_'])]\n",
    "\n",
    "print(f\"\\n🏷️ Feature Categories:\")\n",
    "print(f\"• Categorical features: {len(categorical_features)} - {categorical_features}\")\n",
    "print(f\"• Numeric features: {len(numeric_features)}\")\n",
    "print(f\"• Text embedding features: {len(text_embedding_features)}\")\n",
    "\n",
    "# Display sample data\n",
    "print(\"\\n🔍 Sample Article Features:\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data quality assessment\n",
    "print(\"🔍 Data Quality Assessment\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "# Missing values analysis\n",
    "quality_summary = []\n",
    "for col in df.columns:\n",
    "    null_count = df.select(pl.col(col).null_count()).item()\n",
    "    null_pct = (null_count / df.height) * 100\n",
    "    \n",
    "    # Additional quality checks\n",
    "    unique_count = df.select(pl.col(col).n_unique()).item()\n",
    "    uniqueness_pct = (unique_count / df.height) * 100\n",
    "    \n",
    "    quality_summary.append({\n",
    "        'Feature': col,\n",
    "        'Missing Count': null_count,\n",
    "        'Missing %': f\"{null_pct:.2f}%\",\n",
    "        'Unique Values': unique_count,\n",
    "        'Uniqueness %': f\"{uniqueness_pct:.1f}%\"\n",
    "    })\n",
    "\n",
    "quality_df = pd.DataFrame(quality_summary)\n",
    "print(quality_df.to_string(index=False))\n",
    "\n",
    "# Check for duplicates\n",
    "duplicate_count = df.height - df.n_unique()\n",
    "print(f\"\\n🔄 Duplicate article records: {duplicate_count:,}\")\n",
    "\n",
    "# Feature completeness\n",
    "complete_records = df.filter(pl.all_horizontal(pl.all().is_not_null())).height\n",
    "completeness_rate = (complete_records / df.height) * 100\n",
    "print(f\"📋 Complete feature records: {complete_records:,} ({completeness_rate:.1f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Categorical Features Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive categorical features analysis\n",
    "print(\"🏷️ Categorical Features Analysis\")\n",
    "\n",
    "# Analyse each categorical feature\n",
    "for feature in categorical_features:\n",
    "    if feature in df.columns:\n",
    "        print(f\"\\n📊 {feature.upper()}:\")\n",
    "        \n",
    "        # Value counts and distribution\n",
    "        value_counts = (\n",
    "            df.group_by(feature)\n",
    "            .count()\n",
    "            .sort('count', descending=True)\n",
    "            .head(10)\n",
    "        )\n",
    "        \n",
    "        total_unique = df.select(pl.col(feature).n_unique()).item()\n",
    "        print(f\"  Total unique values: {total_unique}\")\n",
    "        print(f\"  Top 10 categories:\")\n",
    "        \n",
    "        for row in value_counts.iter_rows(named=True):\n",
    "            percentage = (row['count'] / df.height) * 100\n",
    "            print(f\"    {row[feature]}: {row['count']:,} ({percentage:.1f}%)\")\n",
    "\n",
    "# Create visualisations for categorical features\n",
    "if categorical_features:\n",
    "    n_features = min(len(categorical_features), 4)  # Limit to 4 for display\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    axes = axes.flatten()\n",
    "    fig.suptitle('Categorical Features Distribution', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    for i, feature in enumerate(categorical_features[:n_features]):\n",
    "        if feature in df.columns:\n",
    "            # Get top categories for visualisation\n",
    "            top_categories = (\n",
    "                df.group_by(feature)\n",
    "                .count()\n",
    "                .sort('count', descending=True)\n",
    "                .head(15)  # Top 15 for readability\n",
    "                .to_pandas()\n",
    "            )\n",
    "            \n",
    "            # Create bar plot\n",
    "            axes[i].bar(range(len(top_categories)), top_categories['count'], alpha=0.7)\n",
    "            axes[i].set_title(f'{feature.replace(\"_\", \" \").title()}')\n",
    "            axes[i].set_xlabel('Categories')\n",
    "            axes[i].set_ylabel('Count')\n",
    "            axes[i].tick_params(axis='x', rotation=45)\n",
    "            axes[i].grid(True, alpha=0.3)\n",
    "            \n",
    "            # Set x-axis labels (abbreviated if too long)\n",
    "            labels = [label[:20] + '...' if len(str(label)) > 20 else str(label) \n",
    "                     for label in top_categories[feature]]\n",
    "            axes[i].set_xticks(range(len(top_categories)))\n",
    "            axes[i].set_xticklabels(labels, rotation=45, ha='right')\n",
    "    \n",
    "    # Hide unused subplots\n",
    "    for i in range(n_features, 4):\n",
    "        axes[i].set_visible(False)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Numeric Features Distribution Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Numeric features comprehensive analysis\n",
    "print(\"📈 Numeric Features Distribution Analysis\")\n",
    "\n",
    "# Filter out article_id and other non-feature numeric columns\n",
    "analysis_numeric_features = [col for col in numeric_features \n",
    "                           if col not in ['article_id'] and not col.endswith('_id')]\n",
    "\n",
    "print(f\"Analysing {len(analysis_numeric_features)} numeric features\")\n",
    "\n",
    "if analysis_numeric_features:\n",
    "    # Calculate comprehensive statistics\n",
    "    for feature in analysis_numeric_features[:10]:  # Limit output for readability\n",
    "        if feature in df.columns:\n",
    "            stats_data = df.select([\n",
    "                pl.col(feature).count().alias('count'),\n",
    "                pl.col(feature).mean().alias('mean'),\n",
    "                pl.col(feature).median().alias('median'),\n",
    "                pl.col(feature).std().alias('std'),\n",
    "                pl.col(feature).min().alias('min'),\n",
    "                pl.col(feature).max().alias('max'),\n",
    "                pl.col(feature).quantile(0.25).alias('Q1'),\n",
    "                pl.col(feature).quantile(0.75).alias('Q3'),\n",
    "                pl.col(feature).quantile(0.95).alias('P95'),\n",
    "                pl.col(feature).quantile(0.99).alias('P99')\n",
    "            ]).to_dict(as_series=False)\n",
    "            \n",
    "            print(f\"\\n🎯 {feature.upper()}:\")\n",
    "            for stat, value in stats_data.items():\n",
    "                if stat == 'count':\n",
    "                    print(f\"  {stat}: {value[0]:,}\")\n",
    "                else:\n",
    "                    print(f\"  {stat}: {value[0]:.4f}\")\n",
    "\n",
    "    # Create distribution plots for key numeric features\n",
    "    plot_features = analysis_numeric_features[:6]  # First 6 features\n",
    "    \n",
    "    if plot_features:\n",
    "        fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "        axes = axes.flatten()\n",
    "        fig.suptitle('Numeric Features Distribution Analysis', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        for i, feature in enumerate(plot_features):\n",
    "            if feature in df.columns and i < 6:\n",
    "                # Convert to pandas for plotting\n",
    "                feature_data = df.select(feature).to_pandas()[feature].dropna()\n",
    "                \n",
    "                # Create histogram\n",
    "                axes[i].hist(feature_data, bins=50, alpha=0.7, edgecolor='black')\n",
    "                axes[i].set_title(f'{feature.replace(\"_\", \" \").title()}')\n",
    "                axes[i].set_xlabel(feature)\n",
    "                axes[i].set_ylabel('Frequency')\n",
    "                axes[i].grid(True, alpha=0.3)\n",
    "                \n",
    "                # Add statistics annotation\n",
    "                mean_val = feature_data.mean()\n",
    "                std_val = feature_data.std()\n",
    "                axes[i].axvline(mean_val, color='red', linestyle='--', alpha=0.7, label=f'Mean: {mean_val:.3f}')\n",
    "                axes[i].legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "else:\n",
    "    print(\"❌ No suitable numeric features found for analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Text Embedding Features Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Text embedding features analysis\n",
    "print(\"🤖 Text Embedding Features Analysis\")\n",
    "\n",
    "if text_embedding_features:\n",
    "    print(f\"Found {len(text_embedding_features)} text embedding features\")\n",
    "    \n",
    "    # Categorise embedding types\n",
    "    bert_features = [f for f in text_embedding_features if 'bert' in f.lower()]\n",
    "    tfidf_features = [f for f in text_embedding_features if 'tfidf' in f.lower()]\n",
    "    svd_features = [f for f in text_embedding_features if 'svd' in f.lower()]\n",
    "    pca_features = [f for f in text_embedding_features if 'pca' in f.lower()]\n",
    "    \n",
    "    print(f\"\\n📊 Embedding Feature Categories:\")\n",
    "    print(f\"• BERT features: {len(bert_features)}\")\n",
    "    print(f\"• TF-IDF features: {len(tfidf_features)}\")\n",
    "    print(f\"• SVD features: {len(svd_features)}\")\n",
    "    print(f\"• PCA features: {len(pca_features)}\")\n",
    "    \n",
    "    # Analyse embedding feature statistics\n",
    "    sample_embeddings = text_embedding_features[:20]  # Sample for analysis\n",
    "    \n",
    "    if sample_embeddings:\n",
    "        embedding_stats = []\n",
    "        \n",
    "        for feature in sample_embeddings:\n",
    "            if feature in df.columns:\n",
    "                stats = df.select([\n",
    "                    pl.col(feature).mean().alias('mean'),\n",
    "                    pl.col(feature).std().alias('std'),\n",
    "                    pl.col(feature).min().alias('min'),\n",
    "                    pl.col(feature).max().alias('max')\n",
    "                ]).to_dict(as_series=False)\n",
    "                \n",
    "                embedding_stats.append({\n",
    "                    'Feature': feature,\n",
    "                    'Mean': f\"{stats['mean'][0]:.4f}\",\n",
    "                    'Std': f\"{stats['std'][0]:.4f}\",\n",
    "                    'Range': f\"[{stats['min'][0]:.3f}, {stats['max'][0]:.3f}]\"\n",
    "                })\n",
    "        \n",
    "        embedding_df = pd.DataFrame(embedding_stats)\n",
    "        print(f\"\\n📈 Sample Embedding Statistics:\")\n",
    "        print(embedding_df.head(10).to_string(index=False))\n",
    "    \n",
    "    # Visualise embedding feature distributions\n",
    "    if len(sample_embeddings) >= 4:\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(15, 10))\n",
    "        axes = axes.flatten()\n",
    "        fig.suptitle('Text Embedding Features Distribution', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        for i in range(4):\n",
    "            if i < len(sample_embeddings):\n",
    "                feature = sample_embeddings[i]\n",
    "                if feature in df.columns:\n",
    "                    feature_data = df.select(feature).to_pandas()[feature].dropna()\n",
    "                    \n",
    "                    axes[i].hist(feature_data, bins=50, alpha=0.7, edgecolor='black')\n",
    "                    axes[i].set_title(f'{feature}')\n",
    "                    axes[i].set_xlabel('Value')\n",
    "                    axes[i].set_ylabel('Frequency')\n",
    "                    axes[i].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    # Embedding space dimensionality analysis\n",
    "    if len(text_embedding_features) > 10:\n",
    "        print(f\"\\n🔍 Embedding Space Analysis:\")\n",
    "        \n",
    "        # Sample embeddings for PCA analysis\n",
    "        embedding_sample = text_embedding_features[:50]  # Use first 50 dimensions\n",
    "        \n",
    "        try:\n",
    "            # Convert to pandas for PCA\n",
    "            embedding_data = df.select(embedding_sample).to_pandas().fillna(0)\n",
    "            \n",
    "            # Perform PCA\n",
    "            pca = PCA(n_components=min(10, len(embedding_sample)))\n",
    "            pca_result = pca.fit_transform(embedding_data)\n",
    "            \n",
    "            # Plot explained variance\n",
    "            fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "            \n",
    "            # Explained variance ratio\n",
    "            ax1.bar(range(1, len(pca.explained_variance_ratio_) + 1), pca.explained_variance_ratio_)\n",
    "            ax1.set_title('PCA Explained Variance Ratio')\n",
    "            ax1.set_xlabel('Principal Component')\n",
    "            ax1.set_ylabel('Explained Variance Ratio')\n",
    "            ax1.grid(True, alpha=0.3)\n",
    "            \n",
    "            # Cumulative explained variance\n",
    "            cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "            ax2.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, 'bo-')\n",
    "            ax2.set_title('Cumulative Explained Variance')\n",
    "            ax2.set_xlabel('Number of Components')\n",
    "            ax2.set_ylabel('Cumulative Explained Variance')\n",
    "            ax2.grid(True, alpha=0.3)\n",
    "            ax2.axhline(y=0.95, color='r', linestyle='--', alpha=0.7, label='95% Variance')\n",
    "            ax2.legend()\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            print(f\"  First 5 components explain {cumulative_variance[4]:.1%} of variance\")\n",
    "            print(f\"  95% variance captured by {np.argmax(cumulative_variance >= 0.95) + 1} components\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  ❌ Error in PCA analysis: {e}\")\n",
    "            \n",
    "else:\n",
    "    print(\"❌ No text embedding features found in the dataset.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Feature Correlation Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive feature correlation analysis\n",
    "print(\"🔗 Feature Correlation Analysis\")\n",
    "\n",
    "# Select features for correlation analysis (avoid too many embedding features)\n",
    "correlation_features = []\n",
    "\n",
    "# Add non-embedding numeric features\n",
    "non_embedding_numeric = [f for f in analysis_numeric_features if f not in text_embedding_features]\n",
    "correlation_features.extend(non_embedding_numeric[:10])  # Limit to 10\n",
    "\n",
    "# Add sample of embedding features\n",
    "if text_embedding_features:\n",
    "    correlation_features.extend(text_embedding_features[:20])  # Sample 20 embedding features\n",
    "\n",
    "print(f\"Analysing correlations between {len(correlation_features)} features\")\n",
    "\n",
    "if len(correlation_features) > 1:\n",
    "    # Calculate correlation matrix\n",
    "    corr_data = df.select(correlation_features).to_pandas().fillna(0)\n",
    "    correlation_matrix = corr_data.corr()\n",
    "    \n",
    "    # Create correlation heatmap\n",
    "    fig, ax = plt.subplots(figsize=(14, 12))\n",
    "    \n",
    "    # Create mask for upper triangle\n",
    "    mask = np.triu(np.ones_like(correlation_matrix, dtype=bool))\n",
    "    \n",
    "    # Generate heatmap\n",
    "    sns.heatmap(\n",
    "        correlation_matrix,\n",
    "        mask=mask,\n",
    "        annot=False,  # Too many features for annotations\n",
    "        cmap='RdBu_r',\n",
    "        center=0,\n",
    "        square=True,\n",
    "        cbar_kws={'shrink': 0.8},\n",
    "        ax=ax\n",
    "    )\n",
    "    \n",
    "    ax.set_title('Article Feature Correlation Matrix', fontsize=16, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Identify strongest correlations\n",
    "    print(\"\\n🔍 Strongest Feature Correlations (|r| > 0.5):\")\n",
    "    strong_correlations = []\n",
    "    \n",
    "    for i in range(len(correlation_matrix.columns)):\n",
    "        for j in range(i+1, len(correlation_matrix.columns)):\n",
    "            corr_value = correlation_matrix.iloc[i, j]\n",
    "            if abs(corr_value) > 0.5:  # Higher threshold for article features\n",
    "                strong_correlations.append({\n",
    "                    'Feature 1': correlation_matrix.columns[i],\n",
    "                    'Feature 2': correlation_matrix.columns[j],\n",
    "                    'Correlation': corr_value\n",
    "                })\n",
    "    \n",
    "    # Sort by absolute correlation strength\n",
    "    strong_correlations.sort(key=lambda x: abs(x['Correlation']), reverse=True)\n",
    "    \n",
    "    if strong_correlations:\n",
    "        for corr in strong_correlations[:15]:  # Top 15\n",
    "            print(f\"  {corr['Feature 1']} ↔ {corr['Feature 2']}: {corr['Correlation']:.3f}\")\n",
    "    else:\n",
    "        print(\"  No strong correlations (|r| > 0.5) found between features.\")\n",
    "        \n",
    "    # Feature redundancy analysis\n",
    "    highly_corr_pairs = [corr for corr in strong_correlations if abs(corr['Correlation']) > 0.8]\n",
    "    \n",
    "    if highly_corr_pairs:\n",
    "        print(f\"\\n⚠️ Potentially Redundant Features ({len(highly_corr_pairs)} pairs with |r| > 0.8):\")\n",
    "        for corr in highly_corr_pairs[:10]:\n",
    "            print(f\"  {corr['Feature 1']} ↔ {corr['Feature 2']}: {corr['Correlation']:.3f}\")\n",
    "            \n",
    "else:\n",
    "    print(\"❌ Insufficient features for correlation analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Article Clustering Analysis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Article clustering analysis for product segmentation\n",
    "print(\"🎯 Article Clustering Analysis\")\n",
    "\n",
    "# Prepare clustering features (use a mix of different feature types)\n",
    "clustering_features = []\n",
    "\n",
    "# Add non-embedding numeric features\n",
    "if non_embedding_numeric:\n",
    "    clustering_features.extend(non_embedding_numeric[:5])\n",
    "\n",
    "# Add embedding features\n",
    "if text_embedding_features:\n",
    "    clustering_features.extend(text_embedding_features[:15])  # Sample embedding features\n",
    "\n",
    "print(f\"Using {len(clustering_features)} features for clustering: {clustering_features[:10]}...\")\n",
    "\n",
    "if len(clustering_features) >= 3:\n",
    "    # Prepare clustering data\n",
    "    cluster_data = df.select(clustering_features).to_pandas().fillna(0)\n",
    "    \n",
    "    # Standardise features\n",
    "    scaler = StandardScaler()\n",
    "    scaled_data = scaler.fit_transform(cluster_data)\n",
    "    \n",
    "    # Determine optimal number of clusters using multiple methods\n",
    "    max_k = min(15, df.height // 100)  # Reasonable upper bound\n",
    "    k_range = range(2, max_k + 1)\n",
    "    \n",
    "    inertias = []\n",
    "    silhouette_scores = []\n",
    "    \n",
    "    print(f\"\\n🔍 Evaluating cluster numbers from 2 to {max_k}...\")\n",
    "    \n",
    "    for k in k_range:\n",
    "        # K-means clustering\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "        cluster_labels = kmeans.fit_predict(scaled_data)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        inertias.append(kmeans.inertia_)\n",
    "        \n",
    "        # Silhouette score (sample for large datasets)\n",
    "        if len(scaled_data) > 5000:\n",
    "            sample_indices = np.random.choice(len(scaled_data), 5000, replace=False)\n",
    "            sil_score = silhouette_score(scaled_data[sample_indices], cluster_labels[sample_indices])\n",
    "        else:\n",
    "            sil_score = silhouette_score(scaled_data, cluster_labels)\n",
    "        \n",
    "        silhouette_scores.append(sil_score)\n",
    "    \n",
    "    # Plot clustering evaluation metrics\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n",
    "    \n",
    "    # Elbow curve\n",
    "    ax1.plot(k_range, inertias, 'bo-')\n",
    "    ax1.set_title('Elbow Method for Optimal K')\n",
    "    ax1.set_xlabel('Number of Clusters')\n",
    "    ax1.set_ylabel('Inertia')\n",
    "    ax1.grid(True, alpha=0.3)\n",
    "    \n",
    "    # Silhouette scores\n",
    "    ax2.plot(k_range, silhouette_scores, 'ro-')\n",
    "    ax2.set_title('Silhouette Score by Number of Clusters')\n",
    "    ax2.set_xlabel('Number of Clusters')\n",
    "    ax2.set_ylabel('Silhouette Score')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Select optimal K (highest silhouette score)\n",
    "    optimal_k = k_range[np.argmax(silhouette_scores)]\n",
    "    best_silhouette = max(silhouette_scores)\n",
    "    \n",
    "    print(f\"\\n🎯 Optimal number of clusters: {optimal_k} (Silhouette Score: {best_silhouette:.3f})\")\n",
    "    \n",
    "    # Perform final clustering\n",
    "    final_kmeans = KMeans(n_clusters=optimal_k, random_state=42, n_init=10)\n",
    "    final_clusters = final_kmeans.fit_predict(scaled_data)\n",
    "    \n",
    "    # Cluster analysis\n",
    "    cluster_df = df.select(['article_id'] + clustering_features).to_pandas()\n",
    "    cluster_df['cluster'] = final_clusters\n",
    "    \n",
    "    print(f\"\\n📊 Article Cluster Analysis (K={optimal_k}):\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    cluster_summary = []\n",
    "    \n",
    "    for cluster_id in range(optimal_k):\n",
    "        cluster_mask = final_clusters == cluster_id\n",
    "        cluster_size = np.sum(cluster_mask)\n",
    "        cluster_percentage = (cluster_size / len(final_clusters)) * 100\n",
    "        \n",
    "        cluster_summary.append({\n",
    "            'Cluster': cluster_id,\n",
    "            'Size': cluster_size,\n",
    "            'Percentage': cluster_percentage\n",
    "        })\n",
    "        \n",
    "        print(f\"\\n📦 CLUSTER {cluster_id}: {cluster_size:,} articles ({cluster_percentage:.1f}%)\")\n",
    "        \n",
    "        # Calculate cluster characteristics for non-embedding features\n",
    "        segment_data = cluster_df[cluster_df['cluster'] == cluster_id]\n",
    "        \n",
    "        for feature in non_embedding_numeric[:5]:  # Show top 5 features\n",
    "            if feature in segment_data.columns:\n",
    "                mean_val = segment_data[feature].mean()\n",
    "                overall_mean = cluster_df[feature].mean()\n",
    "                \n",
    "                if not np.isnan(mean_val) and not np.isnan(overall_mean) and overall_mean != 0:\n",
    "                    difference = ((mean_val - overall_mean) / overall_mean) * 100\n",
    "                    \n",
    "                    if abs(difference) > 10:  # Only show significant differences\n",
    "                        direction = \"above\" if difference > 0 else \"below\"\n",
    "                        print(f\"  • {feature}: {mean_val:.3f} ({abs(difference):.0f}% {direction} average)\")\n",
    "    \n",
    "    # Visualise clusters using PCA\n",
    "    if len(clustering_features) > 2:\n",
    "        pca = PCA(n_components=2)\n",
    "        pca_data = pca.fit_transform(scaled_data)\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(12, 8))\n",
    "        scatter = ax.scatter(pca_data[:, 0], pca_data[:, 1], c=final_clusters, cmap='viridis', alpha=0.6, s=20)\n",
    "        ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.1%} variance)')\n",
    "        ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.1%} variance)')\n",
    "        ax.set_title(f'Article Clusters Visualisation (K={optimal_k}, PCA Projection)')\n",
    "        plt.colorbar(scatter, label='Cluster')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"\\n📈 PCA explains {pca.explained_variance_ratio_.sum():.1%} of total variance\")\n",
    "    \n",
    "    # Suggest cluster interpretations based on categorical features\n",
    "    if categorical_features:\n",
    "        print(f\"\\n🏷️ Cluster Interpretation (based on categorical features):\")\n",
    "        \n",
    "        for cluster_id in range(min(optimal_k, 5)):  # Show first 5 clusters\n",
    "            cluster_articles = df.filter(pl.col('article_id').is_in(\n",
    "                cluster_df[cluster_df['cluster'] == cluster_id]['article_id'].tolist()\n",
    "            ))\n",
    "            \n",
    "            print(f\"\\n  CLUSTER {cluster_id}:\")\n",
    "            \n",
    "            for cat_feature in categorical_features[:3]:  # Top 3 categorical features\n",
    "                if cat_feature in cluster_articles.columns:\n",
    "                    top_categories = (\n",
    "                        cluster_articles.group_by(cat_feature)\n",
    "                        .count()\n",
    "                        .sort('count', descending=True)\n",
    "                        .head(3)\n",
    "                    )\n",
    "                    \n",
    "                    print(f\"    Top {cat_feature}: \", end=\"\")\n",
    "                    for row in top_categories.iter_rows(named=True):\n",
    "                        pct = (row['count'] / cluster_articles.height) * 100\n",
    "                        print(f\"{row[cat_feature]} ({pct:.0f}%), \", end=\"\")\n",
    "                    print()  # New line\n",
    "    \n",
    "else:\n",
    "    print(\"❌ Insufficient features for meaningful clustering analysis.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Interactive Visualisations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create interactive visualisations using Plotly\n",
    "print(\"📊 Interactive Article Analytics\")\n",
    "\n",
    "# Interactive categorical feature distribution\n",
    "if categorical_features:\n",
    "    for feature in categorical_features[:2]:  # Show first 2 categorical features\n",
    "        if feature in df.columns:\n",
    "            # Get category counts\n",
    "            category_counts = (\n",
    "                df.group_by(feature)\n",
    "                .count()\n",
    "                .sort('count', descending=True)\n",
    "                .head(20)  # Top 20 categories\n",
    "                .to_pandas()\n",
    "            )\n",
    "            \n",
    "            # Create interactive bar chart\n",
    "            fig = px.bar(\n",
    "                category_counts,\n",
    "                x=feature,\n",
    "                y='count',\n",
    "                title=f'Interactive {feature.replace(\"_\", \" \").title()} Distribution',\n",
    "                hover_data=['count']\n",
    "            )\n",
    "            \n",
    "            fig.update_layout(\n",
    "                xaxis_tickangle=-45,\n",
    "                height=500,\n",
    "                showlegend=False\n",
    "            )\n",
    "            \n",
    "            fig.show()\n",
    "\n",
    "# Interactive correlation heatmap (subset of features)\n",
    "if len(correlation_features) > 2:\n",
    "    # Use subset for interactive visualisation\n",
    "    subset_features = correlation_features[:15]  # First 15 features\n",
    "    corr_subset = df.select(subset_features).to_pandas().fillna(0).corr()\n",
    "    \n",
    "    fig_corr = px.imshow(\n",
    "        corr_subset,\n",
    "        text_auto=True,\n",
    "        aspect=\"auto\",\n",
    "        title=\"Interactive Article Feature Correlation Matrix\",\n",
    "        color_continuous_scale='RdBu_r',\n",
    "        color_continuous_midpoint=0\n",
    "    )\n",
    "    \n",
    "    fig_corr.update_layout(height=600)\n",
    "    fig_corr.show()\n",
    "\n",
    "# Interactive 3D cluster visualisation\n",
    "if 'final_clusters' in locals() and len(clustering_features) >= 3:\n",
    "    # Sample data for performance\n",
    "    sample_size = min(2000, df.height)\n",
    "    sample_indices = np.random.choice(df.height, sample_size, replace=False)\n",
    "    \n",
    "    sample_data = cluster_df.iloc[sample_indices].copy()\n",
    "    \n",
    "    if len(clustering_features) >= 3:\n",
    "        fig_3d = px.scatter_3d(\n",
    "            sample_data,\n",
    "            x=clustering_features[0],\n",
    "            y=clustering_features[1],\n",
    "            z=clustering_features[2],\n",
    "            color='cluster',\n",
    "            title='Interactive 3D Article Clustering View',\n",
    "            hover_data=['article_id'],\n",
    "            color_continuous_scale='viridis'\n",
    "        )\n",
    "        \n",
    "        fig_3d.update_layout(height=600)\n",
    "        fig_3d.show()\n",
    "\n",
    "# Interactive embedding space visualisation (if available)\n",
    "if text_embedding_features and len(text_embedding_features) >= 2:\n",
    "    # Use first two embedding dimensions\n",
    "    embed_sample = df.select(['article_id'] + text_embedding_features[:2]).to_pandas().sample(\n",
    "        min(1000, df.height)\n",
    "    )\n",
    "    \n",
    "    fig_embed = px.scatter(\n",
    "        embed_sample,\n",
    "        x=text_embedding_features[0],\n",
    "        y=text_embedding_features[1],\n",
    "        title='Interactive Embedding Space Visualisation',\n",
    "        hover_data=['article_id'],\n",
    "        opacity=0.6\n",
    "    )\n",
    "    \n",
    "    fig_embed.update_layout(height=500)\n",
    "    fig_embed.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Article Insights and Recommendations\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate comprehensive article insights and recommendations\n",
    "print(\"💡 Article Features Insights and Recommendations\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# Basic dataset statistics\n",
    "total_articles = df.height\n",
    "total_features = df.width\n",
    "complete_articles = df.filter(pl.all_horizontal(pl.all().is_not_null())).height\n",
    "\n",
    "print(\"🎯 DATASET OVERVIEW:\")\n",
    "print(f\"• Total Articles: {total_articles:,}\")\n",
    "print(f\"• Total Features: {total_features}\")\n",
    "print(f\"• Complete Records: {complete_articles:,} ({complete_articles/total_articles*100:.1f}%)\")\n",
    "print(f\"• Feature Categories: {len(categorical_features)} categorical, {len(analysis_numeric_features)} numeric, {len(text_embedding_features)} embeddings\")\n",
    "\n",
    "# Categorical feature diversity\n",
    "if categorical_features:\n",
    "    print(\"\\n🏷️ PRODUCT CATALOGUE DIVERSITY:\")\n",
    "    \n",
    "    for feature in categorical_features:\n",
    "        if feature in df.columns:\n",
    "            unique_count = df.select(pl.col(feature).n_unique()).item()\n",
    "            diversity_ratio = unique_count / total_articles\n",
    "            \n",
    "            print(f\"• {feature.replace('_', ' ').title()}: {unique_count} unique values (diversity: {diversity_ratio:.1%})\")\n",
    "            \n",
    "            # Category concentration analysis\n",
    "            top_category_share = (\n",
    "                df.group_by(feature)\n",
    "                .count()\n",
    "                .sort('count', descending=True)\n",
    "                .head(1)\n",
    "                .select((pl.col('count') / total_articles * 100).alias('percentage'))\n",
    "                .item()\n",
    "            )\n",
    "            \n",
    "            concentration = \"High\" if top_category_share > 50 else \"Medium\" if top_category_share > 25 else \"Low\"\n",
    "            print(f\"  Concentration: {concentration} (top category: {top_category_share:.1f}% of articles)\")\n",
    "\n",
    "# Feature engineering quality assessment\n",
    "if text_embedding_features:\n",
    "    print(\"\\n🤖 FEATURE ENGINEERING QUALITY:\")\n",
    "    \n",
    "    # Embedding feature statistics\n",
    "    embedding_sample = text_embedding_features[:20]\n",
    "    \n",
    "    if embedding_sample:\n",
    "        # Calculate average variance across embedding dimensions\n",
    "        embedding_data = df.select(embedding_sample).to_pandas().fillna(0)\n",
    "        avg_variance = embedding_data.var().mean()\n",
    "        avg_std = embedding_data.std().mean()\n",
    "        \n",
    "        print(f\"• Embedding Features: {len(text_embedding_features)} dimensions\")\n",
    "        print(f\"• Average Variance: {avg_variance:.4f}\")\n",
    "        print(f\"• Average Standard Deviation: {avg_std:.4f}\")\n",
    "        \n",
    "        # Feature redundancy assessment\n",
    "        if 'strong_correlations' in locals():\n",
    "            high_corr_count = len([c for c in strong_correlations if abs(c['Correlation']) > 0.8])\n",
    "            redundancy_level = \"High\" if high_corr_count > 10 else \"Medium\" if high_corr_count > 5 else \"Low\"\n",
    "            print(f\"• Feature Redundancy: {redundancy_level} ({high_corr_count} highly correlated pairs)\")\n",
    "\n",
    "# Clustering insights\n",
    "if 'optimal_k' in locals():\n",
    "    print(\"\\n🎯 PRODUCT SEGMENTATION INSIGHTS:\")\n",
    "    print(f\"• Optimal Clusters: {optimal_k} distinct product segments\")\n",
    "    print(f\"• Clustering Quality: {best_silhouette:.3f} silhouette score\")\n",
    "    \n",
    "    # Cluster balance analysis\n",
    "    if 'cluster_summary' in locals():\n",
    "        cluster_sizes = [c['Size'] for c in cluster_summary]\n",
    "        size_std = np.std(cluster_sizes)\n",
    "        size_mean = np.mean(cluster_sizes)\n",
    "        balance_ratio = size_std / size_mean\n",
    "        \n",
    "        balance_level = \"Well-balanced\" if balance_ratio < 0.5 else \"Moderately balanced\" if balance_ratio < 1.0 else \"Imbalanced\"\n",
    "        print(f\"• Cluster Balance: {balance_level} (CV: {balance_ratio:.2f})\")\n",
    "        \n",
    "        largest_cluster = max(cluster_sizes)\n",
    "        smallest_cluster = min(cluster_sizes)\n",
    "        print(f\"• Size Range: {smallest_cluster:,} - {largest_cluster:,} articles per cluster\")\n",
    "\n",
    "# Strategic recommendations\n",
    "print(\"\\n🎯 STRATEGIC RECOMMENDATIONS:\")\n",
    "\n",
    "# Product catalogue recommendations\n",
    "if categorical_features:\n",
    "    for feature in categorical_features:\n",
    "        if feature in df.columns:\n",
    "            unique_count = df.select(pl.col(feature).n_unique()).item()\n",
    "            \n",
    "            if unique_count < 10:\n",
    "                print(f\"• 📈 Consider expanding {feature.replace('_', ' ')} variety (only {unique_count} categories)\")\n",
    "            elif unique_count > 100:\n",
    "                print(f\"• 🎯 Consider consolidating {feature.replace('_', ' ')} categories ({unique_count} may be too many)\")\n",
    "\n",
    "# Feature engineering recommendations\n",
    "if text_embedding_features:\n",
    "    if len(text_embedding_features) > 100:\n",
    "        print(\"• 🔧 Consider dimensionality reduction for embedding features (current: {} dimensions)\".format(len(text_embedding_features)))\n",
    "        print(\"  - Apply PCA or t-SNE for visualisation\")\n",
    "        print(\"  - Use feature selection techniques\")\n",
    "    \n",
    "    if 'high_corr_count' in locals() and high_corr_count > 10:\n",
    "        print(\"• ⚡ Address feature redundancy to improve model efficiency\")\n",
    "        print(\"  - Remove highly correlated features\")\n",
    "        print(\"  - Apply regularisation techniques\")\n",
    "\n",
    "# Clustering-based recommendations\n",
    "if 'optimal_k' in locals():\n",
    "    print(f\"• 🛍️ Leverage {optimal_k} product segments for:\")\n",
    "    print(\"  - Targeted marketing campaigns\")\n",
    "    print(\"  - Inventory management optimisation\")\n",
    "    print(\"  - Product recommendation systems\")\n",
    "    print(\"  - Pricing strategy development\")\n",
    "    \n",
    "    if 'balance_level' in locals() and 'Imbalanced' in balance_level:\n",
    "        print(\"• ⚖️ Address cluster imbalance for better segmentation\")\n",
    "        print(\"  - Investigate dominant clusters\")\n",
    "        print(\"  - Consider alternative clustering algorithms\")\n",
    "        print(\"  - Apply stratified sampling for analysis\")\n",
    "\n",
    "print(\"\\n📊 ANALYTICS RECOMMENDATIONS:\")\n",
    "print(\"• Implement automated article similarity scoring\")\n",
    "print(\"• Develop product performance tracking by cluster\")\n",
    "print(\"• Create embedding-based product search functionality\")\n",
    "print(\"• Set up monitoring for catalogue diversity metrics\")\n",
    "print(\"• Build recommendation engines using article features\")\n",
    "\n",
    "print(\"\\n🔍 DATA QUALITY RECOMMENDATIONS:\")\n",
    "if complete_articles / total_articles < 0.95:\n",
    "    print(\"• 🚨 Improve data completeness - missing values detected\")\n",
    "    print(\"  - Implement data validation pipelines\")\n",
    "    print(\"  - Add imputation strategies for missing features\")\n",
    "\n",
    "print(\"• Regular feature quality audits\")\n",
    "print(\"• Monitor embedding feature drift over time\")\n",
    "print(\"• Validate clustering stability with new data\")\n",
    "\n",
    "print(\"\\n🎉 ANALYSIS COMPLETE\")\n",
    "print(f\"✅ Analysed {total_articles:,} article records\")\n",
    "print(f\"✅ Processed {total_features} engineered features\")\n",
    "print(f\"✅ Identified product segments and patterns\")\n",
    "print(f\"✅ Generated actionable recommendations\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
