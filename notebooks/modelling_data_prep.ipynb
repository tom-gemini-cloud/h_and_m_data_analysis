{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# H&M Data Modelling\n",
    "\n",
    "This notebook combines the final feature datasets for modelling purposes, removing data quality tracking flags and preparing a clean combined dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "polars.config.Config"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('../')\n",
    "\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Set display options\n",
    "pl.Config.set_tbl_rows(20)\n",
    "pl.Config.set_tbl_cols(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and Examine Datasets\n",
    "\n",
    "Loading the three final datasets:\n",
    "\n",
    "- Articles features (final)\n",
    "- Customers features with clusters\n",
    "- Cleaned transactions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading articles features...\n",
      "Articles features shape: (42298, 14)\n",
      "Articles columns: ['article_id', 'product_type_name', 'product_group_name', 'graphical_appearance_name', 'colour_group_name', 'perceived_colour_value_name', 'perceived_colour_master_name', 'department_name', 'index_name', 'index_group_name']...\n",
      "\n",
      "Loading customers features...\n",
      "Customers features shape: (525075, 26)\n",
      "Customers columns: ['customer_id', 'FN', 'Active', 'club_member_status', 'fashion_news_frequency', 'age', 'postal_code', 'recency', 'frequency', 'monetary']...\n",
      "\n",
      "Loading transactions data...\n",
      "Transactions shape: (3904391, 8)\n",
      "Transactions columns: ['t_dat', 'customer_id', 'article_id', 'price', 'sales_channel_id', 'price_outlier_capped', 'sales_channel_corrected', 'price_percentile_calibrated']\n"
     ]
    }
   ],
   "source": [
    "# Load datasets\n",
    "print(\"Loading articles features...\")\n",
    "articles_df = pl.read_parquet(\"../data/features/final/articles_features_final.parquet\")\n",
    "print(f\"Articles features shape: {articles_df.shape}\")\n",
    "print(f\"Articles columns: {articles_df.columns[:10]}...\")\n",
    "print()\n",
    "\n",
    "print(\"Loading customers features...\")\n",
    "customers_df = pl.read_parquet(\"../data/features/final/customers_features_with_clusters.parquet\")\n",
    "print(f\"Customers features shape: {customers_df.shape}\")\n",
    "print(f\"Customers columns: {customers_df.columns[:10]}...\")\n",
    "print()\n",
    "\n",
    "print(\"Loading transactions data...\")\n",
    "transactions_df = pl.read_parquet(\"../data/cleaned/transactions_cleaned.parquet\")\n",
    "print(f\"Transactions shape: {transactions_df.shape}\")\n",
    "print(f\"Transactions columns: {transactions_df.columns}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examine Data Quality Flags\n",
    "\n",
    "Identify columns that contain data quality tracking information to remove them.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Articles columns:\n",
      "  - article_id\n",
      "  - product_type_name\n",
      "  - product_group_name\n",
      "  - graphical_appearance_name\n",
      "  - colour_group_name\n",
      "  - perceived_colour_value_name\n",
      "  - perceived_colour_master_name\n",
      "  - department_name\n",
      "  - index_name\n",
      "  - index_group_name\n",
      "  - section_name\n",
      "  - garment_group_name\n",
      "  - detail_desc\n",
      "  - bert_cluster\n",
      "\n",
      "Customers columns:\n",
      "  - customer_id\n",
      "  - FN\n",
      "  - Active\n",
      "  - club_member_status\n",
      "  - fashion_news_frequency\n",
      "  - age\n",
      "  - postal_code\n",
      "  - recency\n",
      "  - frequency\n",
      "  - monetary\n",
      "  - purchase_diversity_score\n",
      "  - price_sensitivity_index\n",
      "  - colour_preference_entropy\n",
      "  - style_consistency_score\n",
      "  - dataset_created_at\n",
      "  - created_by\n",
      "  - rfm_cluster\n",
      "  - preference_cluster\n",
      "  - hybrid_cluster\n",
      "  - behavioural_cluster\n",
      "  - rfm_cluster_label\n",
      "  - preference_cluster_label\n",
      "  - hybrid_cluster_label\n",
      "  - behavioural_cluster_label\n",
      "  - clustering_analysis_date\n",
      "  - clustering_version\n",
      "\n",
      "Transactions columns:\n",
      "  - t_dat\n",
      "  - customer_id\n",
      "  - article_id\n",
      "  - price\n",
      "  - sales_channel_id\n",
      "  - price_outlier_capped (potential quality flag)\n",
      "  - sales_channel_corrected\n",
      "  - price_percentile_calibrated\n"
     ]
    }
   ],
   "source": [
    "# Examine column names for data quality flags\n",
    "print(\"Articles columns:\")\n",
    "for col in articles_df.columns:\n",
    "    if any(keyword in col.lower() for keyword in ['flag', 'quality', 'missing', 'outlier', 'imputed', 'cleaned']):\n",
    "        print(f\"  - {col} (potential quality flag)\")\n",
    "    else:\n",
    "        print(f\"  - {col}\")\n",
    "        \n",
    "print(\"\\nCustomers columns:\")\n",
    "for col in customers_df.columns:\n",
    "    if any(keyword in col.lower() for keyword in ['flag', 'quality', 'missing', 'outlier', 'imputed', 'cleaned']):\n",
    "        print(f\"  - {col} (potential quality flag)\")\n",
    "    else:\n",
    "        print(f\"  - {col}\")\n",
    "        \n",
    "print(\"\\nTransactions columns:\")\n",
    "for col in transactions_df.columns:\n",
    "    if any(keyword in col.lower() for keyword in ['flag', 'quality', 'missing', 'outlier', 'imputed', 'cleaned']):\n",
    "        print(f\"  - {col} (potential quality flag)\")\n",
    "    else:\n",
    "        print(f\"  - {col}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove Data Quality Flags\n",
    "\n",
    "Remove any columns that are related to data quality tracking.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No quality/metadata columns found in articles\n",
      "\n",
      "Removed 3 quality/metadata columns from customers:\n",
      "  - dataset_created_at\n",
      "  - clustering_analysis_date\n",
      "  - clustering_version\n",
      "\n",
      "Removed 3 quality/metadata columns from transactions:\n",
      "  - price_outlier_capped\n",
      "  - sales_channel_corrected\n",
      "  - price_percentile_calibrated\n",
      "\n",
      "Cleaned shapes:\n",
      "Articles: (42298, 14)\n",
      "Customers: (525075, 23)\n",
      "Transactions: (3904391, 5)\n"
     ]
    }
   ],
   "source": [
    "# Function to filter out data quality columns\n",
    "def remove_quality_columns(df, dataset_name):\n",
    "    original_cols = df.columns\n",
    "    quality_keywords = ['flag', 'quality', 'missing', 'outlier', 'imputed', 'cleaned', '_qc', '_dq']\n",
    "    \n",
    "    # Additional columns to remove for modelling (metadata and processing indicators)\n",
    "    modelling_exclude_cols = [\n",
    "        'clustering_analysis_date', \n",
    "        'clustering_version',\n",
    "        'dataset_created_at',\n",
    "        'sales_channel_corrected',\n",
    "        'price_percentile_calibrated'\n",
    "    ]\n",
    "    \n",
    "    # Keep columns that don't contain quality tracking keywords or modelling exclusions\n",
    "    cols_to_keep = [\n",
    "        col for col in original_cols \n",
    "        if not any(keyword in col.lower() for keyword in quality_keywords)\n",
    "        and col not in modelling_exclude_cols\n",
    "    ]\n",
    "    \n",
    "    removed_cols = [col for col in original_cols if col not in cols_to_keep]\n",
    "    \n",
    "    if removed_cols:\n",
    "        print(f\"Removed {len(removed_cols)} quality/metadata columns from {dataset_name}:\")\n",
    "        for col in removed_cols:\n",
    "            print(f\"  - {col}\")\n",
    "    else:\n",
    "        print(f\"No quality/metadata columns found in {dataset_name}\")\n",
    "    \n",
    "    return df.select(cols_to_keep)\n",
    "\n",
    "# Remove quality columns from each dataset\n",
    "articles_clean = remove_quality_columns(articles_df, \"articles\")\n",
    "print()\n",
    "customers_clean = remove_quality_columns(customers_df, \"customers\")\n",
    "print()\n",
    "transactions_clean = remove_quality_columns(transactions_df, \"transactions\")\n",
    "\n",
    "print(f\"\\nCleaned shapes:\")\n",
    "print(f\"Articles: {articles_clean.shape}\")\n",
    "print(f\"Customers: {customers_clean.shape}\")\n",
    "print(f\"Transactions: {transactions_clean.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Combine Datasets\n",
    "\n",
    "Join transactions with customer and article features to create the final modelling dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Join key analysis:\n",
      "Transactions customer_id unique count: 525075\n",
      "Customers customer_id unique count: 525075\n",
      "Transactions article_id unique count: 42298\n",
      "Articles article_id unique count: 42298\n",
      "\n",
      "Starting joins...\n",
      "After customer join: (3904391, 27)\n",
      "Final combined shape: (3904391, 40)\n",
      "Total columns: 40\n"
     ]
    }
   ],
   "source": [
    "# Check join keys\n",
    "print(\"Join key analysis:\")\n",
    "print(f\"Transactions customer_id unique count: {transactions_clean['customer_id'].n_unique()}\")\n",
    "print(f\"Customers customer_id unique count: {customers_clean['customer_id'].n_unique()}\")\n",
    "print(f\"Transactions article_id unique count: {transactions_clean['article_id'].n_unique()}\")\n",
    "print(f\"Articles article_id unique count: {articles_clean['article_id'].n_unique()}\")\n",
    "\n",
    "print(\"\\nStarting joins...\")\n",
    "# Join transactions with customers\n",
    "combined_df = transactions_clean.join(\n",
    "    customers_clean,\n",
    "    on=\"customer_id\",\n",
    "    how=\"left\"\n",
    ")\n",
    "print(f\"After customer join: {combined_df.shape}\")\n",
    "\n",
    "# Join with articles\n",
    "combined_df = combined_df.join(\n",
    "    articles_clean,\n",
    "    on=\"article_id\",\n",
    "    how=\"left\"\n",
    ")\n",
    "print(f\"Final combined shape: {combined_df.shape}\")\n",
    "print(f\"Total columns: {len(combined_df.columns)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Quality Check\n",
    "\n",
    "Quick check of the combined dataset for any issues.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined dataset summary:\n",
      "Shape: (3904391, 40)\n",
      "Memory usage: 1445.0 MB\n",
      "\n",
      "Columns with null values (showing first 10):\n",
      "  bert_cluster: 2,460 (0.1%)\n",
      "\n",
      "First 3 rows (first 10 columns):\n",
      "shape: (3, 10)\n",
      "┌───────────┬───────────┬───────────┬───────┬──────────┬─────┬────────┬──────────┬──────────┬──────┐\n",
      "│ t_dat     ┆ customer_ ┆ article_i ┆ price ┆ sales_ch ┆ FN  ┆ Active ┆ club_mem ┆ fashion_ ┆ age  │\n",
      "│ ---       ┆ id        ┆ d         ┆ ---   ┆ annel_id ┆ --- ┆ ---    ┆ ber_stat ┆ news_fre ┆ ---  │\n",
      "│ date      ┆ ---       ┆ ---       ┆ f64   ┆ ---      ┆ f64 ┆ f64    ┆ us       ┆ quency   ┆ f64  │\n",
      "│           ┆ str       ┆ i64       ┆       ┆ i64      ┆     ┆        ┆ ---      ┆ ---      ┆      │\n",
      "│           ┆           ┆           ┆       ┆          ┆     ┆        ┆ cat      ┆ cat      ┆      │\n",
      "╞═══════════╪═══════════╪═══════════╪═══════╪══════════╪═════╪════════╪══════════╪══════════╪══════╡\n",
      "│ 2020-06-2 ┆ 0001b0127 ┆ 844409002 ┆ 14.33 ┆ 1        ┆ 1.0 ┆ 1.0    ┆ ACTIVE   ┆ Regularl ┆ 20.0 │\n",
      "│ 4         ┆ d3e5ff8da ┆           ┆       ┆          ┆     ┆        ┆          ┆ y        ┆      │\n",
      "│           ┆ dcfc6e504 ┆           ┆       ┆          ┆     ┆        ┆          ┆          ┆      │\n",
      "│           ┆ 368…      ┆           ┆       ┆          ┆     ┆        ┆          ┆          ┆      │\n",
      "│ 2020-06-2 ┆ 0001b0127 ┆ 876053003 ┆ 21.35 ┆ 1        ┆ 1.0 ┆ 1.0    ┆ ACTIVE   ┆ Regularl ┆ 20.0 │\n",
      "│ 4         ┆ d3e5ff8da ┆           ┆       ┆          ┆     ┆        ┆          ┆ y        ┆      │\n",
      "│           ┆ dcfc6e504 ┆           ┆       ┆          ┆     ┆        ┆          ┆          ┆      │\n",
      "│           ┆ 368…      ┆           ┆       ┆          ┆     ┆        ┆          ┆          ┆      │\n",
      "│ 2020-06-2 ┆ 000493dd9 ┆ 831269004 ┆ 11.32 ┆ 1        ┆ 0.0 ┆ 0.0    ┆ ACTIVE   ┆ NONE     ┆ 29.0 │\n",
      "│ 4         ┆ fc463df1a ┆           ┆       ┆          ┆     ┆        ┆          ┆          ┆      │\n",
      "│           ┆ cc2081450 ┆           ┆       ┆          ┆     ┆        ┆          ┆          ┆      │\n",
      "│           ┆ c9e…      ┆           ┆       ┆          ┆     ┆        ┆          ┆          ┆      │\n",
      "└───────────┴───────────┴───────────┴───────┴──────────┴─────┴────────┴──────────┴──────────┴──────┘\n"
     ]
    }
   ],
   "source": [
    "# Basic quality check\n",
    "print(\"Combined dataset summary:\")\n",
    "print(f\"Shape: {combined_df.shape}\")\n",
    "print(f\"Memory usage: {combined_df.estimated_size('mb'):.1f} MB\")\n",
    "\n",
    "# Check for null values in key columns\n",
    "null_counts = combined_df.null_count()\n",
    "high_null_cols = [\n",
    "    (col, count) for col, count in zip(null_counts.columns, null_counts.row(0))\n",
    "    if count > 0\n",
    "]\n",
    "\n",
    "if high_null_cols:\n",
    "    print(f\"\\nColumns with null values (showing first 10):\")\n",
    "    for col, count in sorted(high_null_cols, key=lambda x: x[1], reverse=True)[:10]:\n",
    "        percentage = (count / combined_df.shape[0]) * 100\n",
    "        print(f\"  {col}: {count:,} ({percentage:.1f}%)\")\n",
    "else:\n",
    "    print(\"\\nNo null values found\")\n",
    "\n",
    "# Show first few rows\n",
    "print(f\"\\nFirst 3 rows (first 10 columns):\")\n",
    "print(combined_df.select(combined_df.columns[:10]).head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Combined Dataset\n",
    "\n",
    "Create the modelling_data directory and save the combined dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created directory: c:\\Users\\tom\\coding_projects\\data_analytics_projects\\h_and_m_data_analysis\\notebooks\\..\\data\\modelling_data\n",
      "Saving combined dataset to: ..\\data\\modelling_data\\combined_modelling_data.parquet\n",
      "✓ Saved combined dataset: (3904391, 40)\n",
      "File size: 2014.4 MB\n"
     ]
    }
   ],
   "source": [
    "# Create modelling_data directory\n",
    "modelling_dir = Path(\"../data/modelling_data\")\n",
    "modelling_dir.mkdir(exist_ok=True)\n",
    "print(f\"Created directory: {modelling_dir.absolute()}\")\n",
    "\n",
    "# Save combined dataset\n",
    "output_path = modelling_dir / \"combined_modelling_data.parquet\"\n",
    "print(f\"Saving combined dataset to: {output_path}\")\n",
    "\n",
    "combined_df.write_parquet(output_path)\n",
    "print(f\"✓ Saved combined dataset: {combined_df.shape}\")\n",
    "\n",
    "# Verify file size\n",
    "file_size_mb = output_path.stat().st_size / (1024 * 1024)\n",
    "print(f\"File size: {file_size_mb:.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Data Report\n",
    "\n",
    "Create a data report for the combined modelling dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created documentation directory: c:\\Users\\tom\\coding_projects\\data_analytics_projects\\h_and_m_data_analysis\\notebooks\\..\\results\\data_documentation\\modelling\n",
      "Data report generated successfully!\n",
      "Analysed file: c:\\Users\\tom\\coding_projects\\data_analytics_projects\\h_and_m_data_analysis\\notebooks\\..\\data\\modelling_data\\combined_modelling_data.parquet\n",
      "Report saved to: c:\\Users\\tom\\coding_projects\\data_analytics_projects\\h_and_m_data_analysis\\notebooks\\..\\results\\data_documentation\\modelling\\combined_modelling_data_report.md\n",
      "✓ Data report generated: c:\\Users\\tom\\coding_projects\\data_analytics_projects\\h_and_m_data_analysis\\notebooks\\..\\results\\data_documentation\\modelling\\combined_modelling_data_report.md\n"
     ]
    }
   ],
   "source": [
    "# Import data report generator\n",
    "from hnm_data_analysis.data_understanding.data_report_generator import generate_data_report\n",
    "\n",
    "# Create modelling documentation directory\n",
    "modelling_doc_dir = Path(\"../results/data_documentation/modelling\")\n",
    "modelling_doc_dir.mkdir(parents=True, exist_ok=True)\n",
    "print(f\"Created documentation directory: {modelling_doc_dir.absolute()}\")\n",
    "\n",
    "# Generate report for the combined modelling dataset\n",
    "report_output_path = modelling_doc_dir / \"combined_modelling_data_report.md\"\n",
    "report_path = generate_data_report(\n",
    "    str(output_path.absolute()),\n",
    "    output_path=str(report_output_path.absolute())\n",
    ")\n",
    "\n",
    "print(f\"✓ Data report generated: {report_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Combination Summary\n",
    "\n",
    "The combined modelling dataset has been successfully created with:\n",
    "\n",
    "- Transactions data as the base\n",
    "- Customer features and clusters joined\n",
    "- Article features joined\n",
    "- Data quality tracking columns removed\n",
    "- Data report generated\n",
    "\n",
    "The dataset is ready for modelling tasks.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test Split\n",
    "\n",
    "Split the combined modelling dataset into training and test sets for machine learning model development.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading combined modelling dataset...\n",
      "Dataset shape: (3904391, 40)\n",
      "\n",
      "Creating customer-based train/test split...\n",
      "Unique customers: 525,075\n",
      "Training customers: 420,060\n",
      "Test customers: 105,015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tom\\AppData\\Local\\Temp\\ipykernel_26776\\1028922960.py:32: DeprecationWarning: `is_in` with a collection of the same datatype is ambiguous and deprecated.\n",
      "Please use `implode` to return to previous behavior.\n",
      "\n",
      "See https://github.com/pola-rs/polars/issues/22149 for more information.\n",
      "  train_df = combined_df.filter(pl.col(\"customer_id\").is_in(train_customers))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset shape: (3122370, 40)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tom\\AppData\\Local\\Temp\\ipykernel_26776\\1028922960.py:36: DeprecationWarning: `is_in` with a collection of the same datatype is ambiguous and deprecated.\n",
      "Please use `implode` to return to previous behavior.\n",
      "\n",
      "See https://github.com/pola-rs/polars/issues/22149 for more information.\n",
      "  test_df = combined_df.filter(pl.col(\"customer_id\").is_in(test_customers))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataset shape: (782021, 40)\n",
      "Customer overlap between train/test: 0 (should be 0)\n",
      "\n",
      "Split percentages:\n",
      "Training: 80.0% (3,122,370 transactions)\n",
      "Test: 20.0% (782,021 transactions)\n"
     ]
    }
   ],
   "source": [
    "# Load the combined dataset\n",
    "print(\"Loading combined modelling dataset...\")\n",
    "combined_df = pl.read_parquet(\"../data/modelling_data/combined_modelling_data.parquet\")\n",
    "print(f\"Dataset shape: {combined_df.shape}\")\n",
    "\n",
    "# Create a stratified split based on customer_id to ensure customers are not split across train/test\n",
    "# This prevents data leakage where the same customer appears in both training and test sets\n",
    "print(\"\\nCreating customer-based train/test split...\")\n",
    "\n",
    "# Get unique customers\n",
    "unique_customers = combined_df.select(\"customer_id\").unique()\n",
    "print(f\"Unique customers: {unique_customers.shape[0]:,}\")\n",
    "\n",
    "# Create random split of customers (80/20 split)\n",
    "np.random.seed(42)  # For reproducibility\n",
    "customer_indices = np.arange(unique_customers.shape[0])\n",
    "np.random.shuffle(customer_indices)\n",
    "\n",
    "# Split customers 80/20\n",
    "split_idx = int(0.8 * len(customer_indices))\n",
    "train_customer_indices = customer_indices[:split_idx]\n",
    "test_customer_indices = customer_indices[split_idx:]\n",
    "\n",
    "print(f\"Training customers: {len(train_customer_indices):,}\")\n",
    "print(f\"Test customers: {len(test_customer_indices):,}\")\n",
    "\n",
    "# Get customer IDs for each split\n",
    "train_customers = unique_customers[train_customer_indices][\"customer_id\"]\n",
    "test_customers = unique_customers[test_customer_indices][\"customer_id\"]\n",
    "\n",
    "# Create training dataset\n",
    "train_df = combined_df.filter(pl.col(\"customer_id\").is_in(train_customers))\n",
    "print(f\"Training dataset shape: {train_df.shape}\")\n",
    "\n",
    "# Create test dataset  \n",
    "test_df = combined_df.filter(pl.col(\"customer_id\").is_in(test_customers))\n",
    "print(f\"Test dataset shape: {test_df.shape}\")\n",
    "\n",
    "# Verify no customer overlap\n",
    "train_customer_set = set(train_df[\"customer_id\"].unique())\n",
    "test_customer_set = set(test_df[\"customer_id\"].unique())\n",
    "overlap = train_customer_set.intersection(test_customer_set)\n",
    "print(f\"Customer overlap between train/test: {len(overlap)} (should be 0)\")\n",
    "\n",
    "# Show split statistics\n",
    "total_transactions = combined_df.shape[0]\n",
    "train_pct = (train_df.shape[0] / total_transactions) * 100\n",
    "test_pct = (test_df.shape[0] / total_transactions) * 100\n",
    "print(f\"\\nSplit percentages:\")\n",
    "print(f\"Training: {train_pct:.1f}% ({train_df.shape[0]:,} transactions)\")\n",
    "print(f\"Test: {test_pct:.1f}% ({test_df.shape[0]:,} transactions)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Train/Test Datasets\n",
    "\n",
    "Save the training and test datasets to separate files for model development.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving training dataset to: ..\\data\\modelling_data\\train_data.parquet\n",
      "✓ Training dataset saved: (3122370, 40), 1451.1 MB\n",
      "Saving test dataset to: ..\\data\\modelling_data\\test_data.parquet\n",
      "✓ Test dataset saved: (782021, 40), 428.1 MB\n",
      "✓ Customer ID splits saved\n",
      "  - Training customers: ..\\data\\modelling_data\\train_customers.parquet\n",
      "  - Test customers: ..\\data\\modelling_data\\test_customers.parquet\n",
      "\n",
      "=== Train/Test Split Summary ===\n",
      "Original dataset: 3,904,391 transactions, 40 features\n",
      "Training set: 3,122,370 transactions (80.0%)\n",
      "Test set: 782,021 transactions (20.0%)\n",
      "Customer split: 420,060 / 105,015\n",
      "Files saved to: c:\\Users\\tom\\coding_projects\\data_analytics_projects\\h_and_m_data_analysis\\notebooks\\..\\data\\modelling_data\n"
     ]
    }
   ],
   "source": [
    "# Save training dataset\n",
    "train_path = modelling_dir / \"train_data.parquet\"\n",
    "print(f\"Saving training dataset to: {train_path}\")\n",
    "train_df.write_parquet(train_path)\n",
    "train_size_mb = train_path.stat().st_size / (1024 * 1024)\n",
    "print(f\"✓ Training dataset saved: {train_df.shape}, {train_size_mb:.1f} MB\")\n",
    "\n",
    "# Save test dataset\n",
    "test_path = modelling_dir / \"test_data.parquet\"\n",
    "print(f\"Saving test dataset to: {test_path}\")\n",
    "test_df.write_parquet(test_path)\n",
    "test_size_mb = test_path.stat().st_size / (1024 * 1024)\n",
    "print(f\"✓ Test dataset saved: {test_df.shape}, {test_size_mb:.1f} MB\")\n",
    "\n",
    "# Save customer splits for reference\n",
    "train_customers_path = modelling_dir / \"train_customers.parquet\"\n",
    "test_customers_path = modelling_dir / \"test_customers.parquet\"\n",
    "\n",
    "train_customers.to_frame().write_parquet(train_customers_path)\n",
    "test_customers.to_frame().write_parquet(test_customers_path)\n",
    "\n",
    "print(f\"✓ Customer ID splits saved\")\n",
    "print(f\"  - Training customers: {train_customers_path}\")\n",
    "print(f\"  - Test customers: {test_customers_path}\")\n",
    "\n",
    "# Summary\n",
    "print(f\"\\n=== Train/Test Split Summary ===\")\n",
    "print(f\"Original dataset: {combined_df.shape[0]:,} transactions, {combined_df.shape[1]} features\")\n",
    "print(f\"Training set: {train_df.shape[0]:,} transactions ({train_pct:.1f}%)\")\n",
    "print(f\"Test set: {test_df.shape[0]:,} transactions ({test_pct:.1f}%)\")\n",
    "print(f\"Customer split: {len(train_customers):,} / {len(test_customers):,}\")\n",
    "print(f\"Files saved to: {modelling_dir.absolute()}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
