{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Customer Feature Engineering (Memory Optimized)\n",
    "\n",
    "This notebook uses the CustomerFeatureEngineer module to create comprehensive customer features with memory-efficient processing for large datasets. Features are created incrementally and saved to `data/features/final`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure project root is the working directory\n",
    "import os\n",
    "if os.path.basename(os.getcwd()) == 'notebooks':\n",
    "    os.chdir('..')\n",
    "print('Working directory:', os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "import logging\n",
    "import warnings\n",
    "import gc  # For garbage collection\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import our feature engineering module\n",
    "from hnm_data_analysis.feature_engineering.customer_features import CustomerFeatureEngineer\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "print('Imports successful!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check available memory and dataset sizes\n",
    "import psutil\n",
    "\n",
    "def check_memory_status():\n",
    "    memory = psutil.virtual_memory()\n",
    "    print(f\"Available memory: {memory.available / (1024**3):.2f} GB\")\n",
    "    print(f\"Memory usage: {memory.percent}%\")\n",
    "    return memory.available / (1024**3)\n",
    "\n",
    "def check_file_sizes():\n",
    "    files = {\n",
    "        'customers': 'data/cleaned/customers_last_3_months_cleaned.parquet',\n",
    "        'transactions': 'data/cleaned/transactions_last_3_months_cleaned.parquet',\n",
    "        'articles': 'data/cleaned/articles_last_3_months_cleaned.parquet'\n",
    "    }\n",
    "    \n",
    "    for name, path in files.items():\n",
    "        if Path(path).exists():\n",
    "            size_mb = Path(path).stat().st_size / (1024*1024)\n",
    "            print(f\"{name}: {size_mb:.1f} MB\")\n",
    "        else:\n",
    "            print(f\"{name}: FILE NOT FOUND\")\n",
    "\n",
    "print(\"System Memory Status:\")\n",
    "available_gb = check_memory_status()\n",
    "\n",
    "print(\"\\nDataset File Sizes:\")\n",
    "check_file_sizes()\n",
    "\n",
    "# Determine if we need to use sampling\n",
    "USE_SAMPLING = available_gb < 8.0\n",
    "SAMPLE_SIZE = 50000 if USE_SAMPLING else None\n",
    "\n",
    "if USE_SAMPLING:\n",
    "    print(f\"\\n‚ö†Ô∏è Limited memory detected. Using sampling with {SAMPLE_SIZE:,} customers for development.\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ Sufficient memory available for full dataset processing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define data paths and verify files exist\n",
    "customers_path = 'data/cleaned/customers_last_3_months_cleaned.parquet'\n",
    "transactions_path = 'data/cleaned/transactions_last_3_months_cleaned.parquet'\n",
    "articles_path = 'data/cleaned/articles_last_3_months_cleaned.parquet'\n",
    "\n",
    "# Verify files exist\n",
    "for path in [customers_path, transactions_path, articles_path]:\n",
    "    if not Path(path).exists():\n",
    "        raise FileNotFoundError(f\"Required file not found: {path}\")\n",
    "        \n",
    "print('‚úÖ All required data files found!')\n",
    "\n",
    "# Set analysis date\n",
    "analysis_date = datetime(2020, 9, 22)\n",
    "print(f'Analysis date: {analysis_date.date()}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "## Step 1: Create Core Features Using CustomerFeatureEngineer\n",
    "\n",
    "We'll start with the proven CustomerFeatureEngineer module to create RFM analysis, product preferences, demographics, and shopping metrics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize feature engineer\n",
    "feature_engineer = CustomerFeatureEngineer(analysis_date=analysis_date)\n",
    "\n",
    "print('Creating core customer features...')\n",
    "print('This may take a few minutes for large datasets...')\n",
    "\n",
    "# Create core features with memory management\n",
    "try:\n",
    "    customer_features = feature_engineer.create_customer_features(\n",
    "        customers_path=customers_path,\n",
    "        transactions_path=transactions_path,\n",
    "        articles_path=articles_path,\n",
    "        output_dir='data/features/intermediate'\n",
    "    )\n",
    "    \n",
    "    print(f'\\n‚úÖ Core features created successfully!')\n",
    "    print(f'Dataset shape: {customer_features.shape}')\n",
    "    print(f'Memory usage after core features: {psutil.virtual_memory().percent}%')\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå Error creating core features: {e}\")\n",
    "    print(\"This might be due to memory constraints. Try restarting the kernel and running with sampling.\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick examination of core features\n",
    "print('Core Feature Summary:')\n",
    "print(f'Customers: {customer_features.height:,}')\n",
    "print(f'Features: {len(customer_features.columns)}')\n",
    "\n",
    "# Show sample of key features\n",
    "key_columns = ['customer_id', 'age_group', 'rfm_segment', 'total_spent', 'transaction_count']\n",
    "available_columns = [col for col in key_columns if col in customer_features.columns]\n",
    "print(f'\\nSample data:')\n",
    "print(customer_features.select(available_columns).head(3))\n",
    "\n",
    "# Force garbage collection\n",
    "gc.collect()\n",
    "print(f'\\nMemory after cleanup: {psutil.virtual_memory().percent}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## Step 2: Add Complementary Features (Memory Efficient)\n",
    "\n",
    "Now we'll add complementary features in smaller batches to avoid memory issues.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load datasets efficiently for additional features\n",
    "print('Loading datasets for additional feature engineering...')\n",
    "\n",
    "# Load with memory-conscious approach\n",
    "customers = pl.scan_parquet(customers_path).collect()\n",
    "print(f'Customers loaded: {customers.shape}')\n",
    "\n",
    "# Sample transactions if needed for memory management\n",
    "if USE_SAMPLING:\n",
    "    print(f'Sampling transactions for memory efficiency...')\n",
    "    # Get list of customer IDs from our features dataset\n",
    "    customer_ids = customer_features.select('customer_id').to_series().to_list()\n",
    "    \n",
    "    transactions = (\n",
    "        pl.scan_parquet(transactions_path)\n",
    "        .filter(pl.col('customer_id').is_in(customer_ids))\n",
    "        .collect()\n",
    "    )\n",
    "else:\n",
    "    transactions = pl.scan_parquet(transactions_path).collect()\n",
    "\n",
    "print(f'Transactions loaded: {transactions.shape}')\n",
    "\n",
    "# Load articles (smaller dataset)\n",
    "articles = pl.scan_parquet(articles_path).collect()\n",
    "print(f'Articles loaded: {articles.shape}')\n",
    "\n",
    "print(f'Memory after loading: {psutil.virtual_memory().percent}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create temporal features (lightweight)\n",
    "def create_temporal_features(transactions_df):\n",
    "    \"\"\"Create temporal and seasonality features efficiently.\"\"\"\n",
    "    print('Creating temporal features...')\n",
    "    \n",
    "    temporal_features = (\n",
    "        transactions_df\n",
    "        .with_columns([\n",
    "            pl.col('t_dat').dt.month().alias('month'),\n",
    "            pl.col('t_dat').dt.weekday().alias('weekday'),\n",
    "            (pl.col('t_dat').dt.weekday().is_in([6, 7])).alias('is_weekend')\n",
    "        ])\n",
    "        .group_by('customer_id')\n",
    "        .agg([\n",
    "            # Simple temporal metrics\n",
    "            pl.col('month').mode().first().alias('dominant_month'),\n",
    "            pl.col('is_weekend').mean().alias('weekend_ratio'),\n",
    "            pl.col('weekday').std().alias('weekday_consistency'),\n",
    "            \n",
    "            # Purchase timing\n",
    "            (pl.col('t_dat').max() - pl.col('t_dat').min()).dt.total_days().alias('purchase_span')\n",
    "        ])\n",
    "        .with_columns([\n",
    "            # Classifications\n",
    "            pl.when(pl.col('weekend_ratio') > 0.6).then(pl.lit('Weekend_Shopper'))\n",
    "            .when(pl.col('weekend_ratio') < 0.2).then(pl.lit('Weekday_Shopper'))\n",
    "            .otherwise(pl.lit('Mixed_Shopper')).alias('shopping_time_type')\n",
    "        ])\n",
    "    )\n",
    "    \n",
    "    print(f'Temporal features created for {temporal_features.height:,} customers')\n",
    "    return temporal_features\n",
    "\n",
    "temporal_features = create_temporal_features(transactions)\n",
    "gc.collect()  # Clean up memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lifecycle features\n",
    "def create_lifecycle_features(transactions_df, analysis_date):\n",
    "    \"\"\"Create customer lifecycle features efficiently.\"\"\"\n",
    "    print('Creating customer lifecycle features...')\n",
    "    \n",
    "    lifecycle_features = (\n",
    "        transactions_df\n",
    "        .group_by('customer_id')\n",
    "        .agg([\n",
    "            pl.col('t_dat').min().alias('first_purchase'),\n",
    "            pl.col('t_dat').max().alias('last_purchase'),\n",
    "            pl.len().alias('total_purchases'),\n",
    "            pl.col('t_dat').n_unique().alias('active_days')\n",
    "        ])\n",
    "        .with_columns([\n",
    "            (pl.lit(analysis_date).cast(pl.Date) - pl.col('first_purchase')).dt.total_days().alias('tenure_days'),\n",
    "            (pl.lit(analysis_date).cast(pl.Date) - pl.col('last_purchase')).dt.total_days().alias('recency_days'),\n",
    "            (pl.col('total_purchases') / pl.col('active_days')).alias('purchase_intensity')\n",
    "        ])\n",
    "        .with_columns([\n",
    "            # Lifecycle stage\n",
    "            pl.when(pl.col('tenure_days') < 30).then(pl.lit('New'))\n",
    "            .when(pl.col('recency_days') > 60).then(pl.lit('Inactive'))\n",
    "            .when(pl.col('total_purchases') >= 10).then(pl.lit('Loyal'))\n",
    "            .when(pl.col('total_purchases') >= 3).then(pl.lit('Regular'))\n",
    "            .otherwise(pl.lit('Occasional')).alias('lifecycle_stage'),\n",
    "            \n",
    "            # Engagement level\n",
    "            pl.when(pl.col('purchase_intensity') >= 1.5).then(pl.lit('High'))\n",
    "            .when(pl.col('purchase_intensity') >= 0.5).then(pl.lit('Medium'))\n",
    "            .otherwise(pl.lit('Low')).alias('engagement_level')\n",
    "        ])\n",
    "        .select(['customer_id', 'tenure_days', 'recency_days', 'purchase_intensity', \n",
    "                'lifecycle_stage', 'engagement_level'])\n",
    "    )\n",
    "    \n",
    "    print(f'Lifecycle features created for {lifecycle_features.height:,} customers')\n",
    "    return lifecycle_features\n",
    "\n",
    "lifecycle_features = create_lifecycle_features(transactions, analysis_date)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create channel features\n",
    "def create_channel_features(transactions_df):\n",
    "    \"\"\"Create sales channel behavior features.\"\"\"\n",
    "    print('Creating channel behavior features...')\n",
    "    \n",
    "    channel_features = (\n",
    "        transactions_df\n",
    "        .group_by('customer_id')\n",
    "        .agg([\n",
    "            pl.col('sales_channel_id').n_unique().alias('channels_used'),\n",
    "            (pl.col('sales_channel_id') == 1).sum().alias('online_transactions'),\n",
    "            pl.len().alias('total_transactions')\n",
    "        ])\n",
    "        .with_columns([\n",
    "            (pl.col('online_transactions') / pl.col('total_transactions')).alias('online_ratio'),\n",
    "            (pl.col('channels_used') > 1).alias('is_omnichannel')\n",
    "        ])\n",
    "        .with_columns([\n",
    "            pl.when(pl.col('online_ratio') >= 0.8).then(pl.lit('Online_Focused'))\n",
    "            .when(pl.col('online_ratio') <= 0.2).then(pl.lit('Store_Focused'))\n",
    "            .when(pl.col('is_omnichannel')).then(pl.lit('Omnichannel'))\n",
    "            .otherwise(pl.lit('Mixed')).alias('channel_preference')\n",
    "        ])\n",
    "        .select(['customer_id', 'channels_used', 'online_ratio', 'is_omnichannel', 'channel_preference'])\n",
    "    )\n",
    "    \n",
    "    print(f'Channel features created for {channel_features.height:,} customers')\n",
    "    return channel_features\n",
    "\n",
    "channel_features = create_channel_features(transactions)\n",
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## Step 3: Combine Features and Create Final Dataset\n",
    "\n",
    "Now we'll carefully combine all feature sets and create the final clean dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine all features step by step\n",
    "print('Combining all feature sets...')\n",
    "\n",
    "# Start with core features\n",
    "print(f'Starting with core features: {customer_features.shape}')\n",
    "final_features = customer_features\n",
    "\n",
    "# Add temporal features\n",
    "print('Adding temporal features...')\n",
    "final_features = final_features.join(temporal_features, on='customer_id', how='left')\n",
    "print(f'After temporal: {final_features.shape}')\n",
    "\n",
    "# Add lifecycle features  \n",
    "print('Adding lifecycle features...')\n",
    "final_features = final_features.join(lifecycle_features, on='customer_id', how='left')\n",
    "print(f'After lifecycle: {final_features.shape}')\n",
    "\n",
    "# Add channel features\n",
    "print('Adding channel features...')\n",
    "final_features = final_features.join(channel_features, on='customer_id', how='left')\n",
    "print(f'After channel: {final_features.shape}')\n",
    "\n",
    "print(f'\\n‚úÖ All features combined!')\n",
    "print(f'Final dataset shape: {final_features.shape}')\n",
    "print(f'Total features: {len(final_features.columns) - 1}')  # Subtract customer_id\n",
    "\n",
    "# Clean up intermediate variables\n",
    "del temporal_features, lifecycle_features, channel_features\n",
    "del transactions, customers, articles\n",
    "gc.collect()\n",
    "print(f'Memory after cleanup: {psutil.virtual_memory().percent}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up the dataset - remove cleaning flags and handle nulls\n",
    "print('Cleaning up final dataset...')\n",
    "\n",
    "# Check for cleaning flag columns to remove\n",
    "cleaning_flags = [col for col in final_features.columns if col.endswith('_imputed') or col.endswith('_corrected')]\n",
    "other_cleanup_cols = ['data_completeness_score'] if 'data_completeness_score' in final_features.columns else []\n",
    "\n",
    "columns_to_remove = cleaning_flags + other_cleanup_cols\n",
    "print(f'Removing columns: {columns_to_remove}')\n",
    "\n",
    "if columns_to_remove:\n",
    "    final_features_clean = final_features.drop(columns_to_remove)\n",
    "else:\n",
    "    final_features_clean = final_features\n",
    "\n",
    "print(f'After cleanup: {final_features_clean.shape}')\n",
    "\n",
    "# Handle null values efficiently\n",
    "print('Handling null values...')\n",
    "null_counts = final_features_clean.null_count()\n",
    "has_nulls = False\n",
    "for col in null_counts.columns:\n",
    "    null_count = null_counts.select(pl.col(col)).item()\n",
    "    if null_count > 0:\n",
    "        print(f'  {col}: {null_count:,} nulls')\n",
    "        has_nulls = True\n",
    "\n",
    "if has_nulls:\n",
    "    # Fill nulls strategically\n",
    "    final_features_clean = final_features_clean.with_columns([\n",
    "        # Numeric columns get 0\n",
    "        pl.all().fill_null(0).exclude([\"customer_id\", pl.Utf8, pl.Categorical])\n",
    "    ])\n",
    "    \n",
    "    # String/categorical columns get 'Unknown'\n",
    "    string_cols = [col for col in final_features_clean.columns \n",
    "                   if col != 'customer_id' and final_features_clean.schema[col] in [pl.Utf8, pl.Categorical]]\n",
    "    \n",
    "    for col in string_cols:\n",
    "        final_features_clean = final_features_clean.with_columns([\n",
    "            pl.col(col).fill_null('Unknown')\n",
    "        ])\n",
    "    \n",
    "    print('‚úÖ Null values handled')\n",
    "else:\n",
    "    print('‚úÖ No null values found')\n",
    "\n",
    "print(f'Final clean dataset: {final_features_clean.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display final dataset summary\n",
    "print('=== FINAL CUSTOMER FEATURES DATASET SUMMARY ===')\n",
    "print(f'Dataset shape: {final_features_clean.shape}')\n",
    "print(f'Customers: {final_features_clean.height:,}')\n",
    "print(f'Features: {len(final_features_clean.columns) - 1}')\n",
    "\n",
    "print('\\nSample of key features:')\n",
    "sample_cols = ['customer_id', 'age_group', 'rfm_segment', 'lifecycle_stage', \n",
    "               'channel_preference', 'shopping_time_type']\n",
    "available_sample_cols = [col for col in sample_cols if col in final_features_clean.columns]\n",
    "print(final_features_clean.select(available_sample_cols).head(5))\n",
    "\n",
    "print('\\nFeature types:')\n",
    "feature_types = {}\n",
    "for col in final_features_clean.columns:\n",
    "    if col != 'customer_id':\n",
    "        dtype = str(final_features_clean.schema[col])\n",
    "        feature_types[dtype] = feature_types.get(dtype, 0) + 1\n",
    "\n",
    "for dtype, count in feature_types.items():\n",
    "    print(f'  {dtype}: {count} features')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "## Step 4: Save Final Dataset\n",
    "\n",
    "Save the comprehensive customer features to the final location with documentation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "output_dir = Path('data/features/final')\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Define output paths\n",
    "parquet_path = output_dir / 'customer_features_final.parquet'\n",
    "csv_path = output_dir / 'customer_features_final.csv'\n",
    "\n",
    "print(f'Saving final customer features dataset...')\n",
    "print(f'Parquet: {parquet_path}')\n",
    "print(f'CSV: {csv_path}')\n",
    "\n",
    "try:\n",
    "    # Save as Parquet (primary format)\n",
    "    final_features_clean.write_parquet(parquet_path)\n",
    "    print('‚úÖ Parquet file saved successfully')\n",
    "    \n",
    "    # Save as CSV (for compatibility)\n",
    "    final_features_clean.write_csv(csv_path)\n",
    "    print('‚úÖ CSV file saved successfully')\n",
    "    \n",
    "    # Display file sizes\n",
    "    parquet_size = parquet_path.stat().st_size / (1024*1024)  # MB\n",
    "    csv_size = csv_path.stat().st_size / (1024*1024)  # MB\n",
    "    \n",
    "    print(f'\\nFile sizes:')\n",
    "    print(f'  Parquet: {parquet_size:.2f} MB')\n",
    "    print(f'  CSV: {csv_size:.2f} MB')\n",
    "    print(f'  Compression ratio: {csv_size/parquet_size:.1f}x')\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f'‚ùå Error saving files: {e}')\n",
    "    # Try saving just Parquet if CSV fails due to memory\n",
    "    try:\n",
    "        final_features_clean.write_parquet(parquet_path)\n",
    "        print('‚úÖ Parquet file saved (CSV skipped due to memory constraints)')\n",
    "    except Exception as e2:\n",
    "        print(f'‚ùå Critical error: {e2}')\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature documentation\n",
    "print('Creating feature documentation...')\n",
    "\n",
    "documentation_path = output_dir / 'customer_features_documentation.txt'\n",
    "\n",
    "with open(documentation_path, 'w', encoding='utf-8') as f:\n",
    "    f.write('CUSTOMER FEATURES DATASET DOCUMENTATION\\n')\n",
    "    f.write('='*50 + '\\n\\n')\n",
    "    f.write(f'Generated: {datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")}\\n')\n",
    "    f.write(f'Analysis Date: {analysis_date.strftime(\"%Y-%m-%d\")}\\n')\n",
    "    f.write(f'Dataset Shape: {final_features_clean.shape}\\n')\n",
    "    f.write(f'Total Features: {len(final_features_clean.columns) - 1}\\n')\n",
    "    \n",
    "    if USE_SAMPLING:\n",
    "        f.write(f'Note: Dataset created with sampling due to memory constraints\\n')\n",
    "    \n",
    "    f.write('\\n\\nFEATURE CATEGORIES:\\n')\n",
    "    f.write('-' * 20 + '\\n')\n",
    "    \n",
    "    categories = {\n",
    "        'Demographics': ['age', 'age_group', 'life_stage', 'club_member', 'fashion_engagement'],\n",
    "        'RFM Analysis': ['recency', 'frequency', 'monetary', 'rfm_score', 'rfm_segment'],\n",
    "        'Product Preferences': ['preferred_', 'diversity', 'color', 'department'],\n",
    "        'Shopping Behavior': ['transaction_count', 'total_spent', 'avg_', 'purchase_'],\n",
    "        'Temporal Patterns': ['month', 'weekend', 'weekday', 'seasonal'],\n",
    "        'Customer Lifecycle': ['lifecycle_stage', 'engagement_level', 'tenure', 'recency'],\n",
    "        'Channel Behavior': ['channel', 'online', 'omnichannel', 'store']\n",
    "    }\n",
    "    \n",
    "    for category, keywords in categories.items():\n",
    "        matching_cols = [col for col in final_features_clean.columns \n",
    "                        if col != 'customer_id' and any(kw in col.lower() for kw in keywords)]\n",
    "        f.write(f'{category}: {len(matching_cols)} features\\n')\n",
    "        if matching_cols[:3]:  # Show first 3 examples\n",
    "            f.write(f'  Examples: {\", \".join(matching_cols[:3])}\\n')\n",
    "        f.write('\\n')\n",
    "    \n",
    "    f.write('\\nALL FEATURES:\\n')\n",
    "    f.write('-' * 15 + '\\n')\n",
    "    for col in sorted(final_features_clean.columns):\n",
    "        if col != 'customer_id':\n",
    "            f.write(f'{col}\\n')\n",
    "    \n",
    "    f.write('\\n\\nUSAGE NOTES:\\n')\n",
    "    f.write('-' * 12 + '\\n')\n",
    "    f.write('- Ready for customer segmentation and clustering analysis\\n')\n",
    "    f.write('- Suitable for ML algorithms (K-means, DBSCAN, etc.)\\n')\n",
    "    f.write('- All cleaning flags removed for production use\\n')\n",
    "    f.write('- Null values handled appropriately\\n')\n",
    "    f.write('- Both numeric and categorical features included\\n')\n",
    "\n",
    "print(f'üìù Documentation saved to: {documentation_path}')\n",
    "\n",
    "# Final success message\n",
    "print('\\nüéâ Customer feature engineering completed successfully!')\n",
    "print(f'\\nFinal dataset ready for analysis: {parquet_path}')\n",
    "print(f'Features created: {len(final_features_clean.columns) - 1}')\n",
    "print(f'Customers processed: {final_features_clean.height:,}')\n",
    "\n",
    "if USE_SAMPLING:\n",
    "    print('\\n‚ö†Ô∏è Note: This was a sampled run due to memory constraints.')\n",
    "    print('For full dataset processing, ensure sufficient memory is available.')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
