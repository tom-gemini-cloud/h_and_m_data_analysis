{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# H&M Data Preprocessing and Cleaning with Polars\n",
    "\n",
    "This notebook performs comprehensive preprocessing and cleaning of the H&M dataset using Polars for high-performance data processing, including:\n",
    "\n",
    "- Duplicate removal\n",
    "- Missing value handling (per user specifications)\n",
    "- Outlier detection and handling\n",
    "- Data validation\n",
    "- Export to Parquet format\n",
    "\n",
    "## Missing Value Handling Strategy\n",
    "\n",
    "- **t_dat**: Interpolate based on customer patterns\n",
    "- **customer_id/article_id**: Drop rows with missing IDs\n",
    "- **price**: Fill with median price for that article_id\n",
    "- **sales_channel_id**: Fill with mode\n",
    "- **FN**: Fill with 0\n",
    "- **Active**: Fill with \"UNKNOWN\"\n",
    "- **club_member_status**: Fill with \"INACTIVE\"\n",
    "- **fashion_news_frequency**: Fill with \"NONE\"\n",
    "- **age**: Fill with median age\n",
    "- **postal_code**: Fill with \"UNKNOWN\"\n",
    "- **Categorical fields**: Fill with \"UNKNOWN\"\n",
    "- **Numerical codes**: Fill with 0\n",
    "- **detail_desc**: Fill with \"NO_DESCRIPTION\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polars version: 1.32.0\n",
      "Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from datetime import datetime, timedelta\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure Polars\n",
    "pl.Config.set_streaming_chunk_size(100_000)\n",
    "\n",
    "print(f\"Polars version: {pl.__version__}\")\n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading raw data files...\n",
      "Loading transactions from data\\raw\\transactions_train.csv ...\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "File not found: data\\raw\\transactions_train.csv\nPlease ensure the dataset is available at the specified path.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 13\u001b[39m\n\u001b[32m     11\u001b[39m transactions_path = Path(data_path) / \u001b[33m\"\u001b[39m\u001b[33mtransactions_train.csv\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mLoading transactions from \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtransactions_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m ...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m13\u001b[39m transactions_df = \u001b[43m_safe_read_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtransactions_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtry_parse_dates\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     15\u001b[39m \u001b[38;5;66;03m# Load customers data\u001b[39;00m\n\u001b[32m     16\u001b[39m customers_path = Path(data_path) / \u001b[33m\"\u001b[39m\u001b[33mcustomers.csv\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 6\u001b[39m, in \u001b[36m_safe_read_csv\u001b[39m\u001b[34m(path, **kwargs)\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_safe_read_csv\u001b[39m(path, **kwargs):\n\u001b[32m      5\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m Path(path).exists():\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFile not found: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m      7\u001b[39m                                 \u001b[33m\"\u001b[39m\u001b[33mPlease ensure the dataset is available at the specified path.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      8\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m pl.read_csv(path, **kwargs)\n",
      "\u001b[31mFileNotFoundError\u001b[39m: File not found: data\\raw\\transactions_train.csv\nPlease ensure the dataset is available at the specified path."
     ]
    }
   ],
   "source": [
    "print(\"Loading raw data files...\")\n",
    "\n",
    "# Robust data loading with existence checks and clear error messages\n",
    "def _safe_read_csv(path, **kwargs):\n",
    "    if not Path(path).exists():\n",
    "        raise FileNotFoundError(f\"File not found: {path}\\n\"\n",
    "                                \"Please ensure the dataset is available at the specified path.\")\n",
    "    return pl.read_csv(path, **kwargs)\n",
    "\n",
    "# Load transactions data\n",
    "transactions_path = Path(data_path) / \"transactions_train.csv\"\n",
    "print(f\"Loading transactions from {transactions_path} ...\")\n",
    "transactions_df = _safe_read_csv(transactions_path, try_parse_dates=True)\n",
    "\n",
    "# Load customers data\n",
    "customers_path = Path(data_path) / \"customers.csv\"\n",
    "print(f\"Loading customers from {customers_path} ...\")\n",
    "customers_df = _safe_read_csv(customers_path, try_parse_dates=True)\n",
    "\n",
    "# Load articles data\n",
    "articles_path = Path(data_path) / \"articles.csv\"\n",
    "print(f\"Loading articles from {articles_path} ...\")\n",
    "articles_df = _safe_read_csv(articles_path, try_parse_dates=True)\n",
    "\n",
    "print(f\"Transactions: {transactions_df.height:,} rows, {transactions_df.width} columns\")\n",
    "print(f\"Customers: {customers_df.height:,} rows, {customers_df.width} columns\")\n",
    "print(f\"Articles: {articles_df.height:,} rows, {articles_df.width} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Loading raw data files...\")\n",
    "\n",
    "# Robust data loading with existence checks and clear error messages\n",
    "\n",
    "def _safe_read_csv(path, **kwargs):\n",
    "    if not Path(path).exists():\n",
    "        raise FileNotFoundError(f\"File not found: {path}\\n\"\n",
    "                                \"Please ensure the dataset is available at the specified path.\")\n",
    "    return pl.read_csv(path, **kwargs)\n",
    "\n",
    "# Load transactions data\n",
    "transactions_path = Path(data_path) / \"transactions_train.csv\"\n",
    "print(f\"Loading transactions from {transactions_path} ...\")\n",
    "transactions_df = _safe_read_csv(transactions_path, try_parse_dates=True)\n",
    "\n",
    "# Load customers data\n",
    "customers_path = Path(data_path) / \"customers.csv\"\n",
    "print(f\"Loading customers from {customers_path} ...\")\n",
    "customers_df = _safe_read_csv(customers_path, try_parse_dates=True)\n",
    "\n",
    "# Load articles data\n",
    "articles_path = Path(data_path) / \"articles.csv\"\n",
    "print(f\"Loading articles from {articles_path} ...\")\n",
    "articles_df = _safe_read_csv(articles_path, try_parse_dates=True)\n",
    "\n",
    "print(f\"Transactions: {transactions_df.height:,} rows, {transactions_df.width} columns\")\n",
    "print(f\"Customers: {customers_df.height:,} rows, {customers_df.width} columns\")\n",
    "print(f\"Articles: {articles_df.height:,} rows, {articles_df.width} columns\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display basic info about each dataset\n",
    "print(\"=== TRANSACTIONS SCHEMA ===\")\n",
    "print(transactions_df.dtypes)\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(transactions_df.head(3))\n",
    "\n",
    "print(\"\\n=== CUSTOMERS SCHEMA ===\")\n",
    "print(customers_df.dtypes)\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(customers_df.head(3))\n",
    "\n",
    "print(\"\\n=== ARTICLES SCHEMA ===\")\n",
    "print(articles_df.dtypes)\n",
    "print(\"\\nFirst few rows:\")\n",
    "print(articles_df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Quality Assessment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def assess_data_quality(df, dataset_name):\n",
    "    \"\"\"\n",
    "    Assess data quality including missing values, duplicates, and basic statistics\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== DATA QUALITY ASSESSMENT: {dataset_name.upper()} ===\")\n",
    "    \n",
    "    total_rows = df.height\n",
    "    print(f\"Total rows: {total_rows:,}\")\n",
    "    \n",
    "    # Check for missing values\n",
    "    print(\"\\n--- Missing Values ---\")\n",
    "    null_counts = df.null_count()\n",
    "    \n",
    "    for column in df.columns:\n",
    "        null_count = null_counts.select(column).item()\n",
    "        null_pct = (null_count / total_rows) * 100\n",
    "        print(f\"{column}: {null_count:,} ({null_pct:.2f}%)\")\n",
    "    \n",
    "    # Check for duplicates\n",
    "    print(\"\\n--- Duplicate Analysis ---\")\n",
    "    distinct_rows = df.unique().height\n",
    "    duplicate_count = total_rows - distinct_rows\n",
    "    print(f\"Duplicate rows: {duplicate_count:,} ({(duplicate_count/total_rows)*100:.2f}%)\")\n",
    "    \n",
    "    return null_counts, duplicate_count\n",
    "\n",
    "# Assess each dataset\n",
    "transactions_quality = assess_data_quality(transactions_df, \"transactions\")\n",
    "customers_quality = assess_data_quality(customers_df, \"customers\")\n",
    "articles_quality = assess_data_quality(articles_df, \"articles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Duplicate Removal\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_duplicates(df, dataset_name):\n",
    "    \"\"\"\n",
    "    Remove duplicate rows from dataset\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== REMOVING DUPLICATES: {dataset_name.upper()} ===\")\n",
    "    \n",
    "    original_count = df.height\n",
    "    df_deduplicated = df.unique()\n",
    "    final_count = df_deduplicated.height\n",
    "    \n",
    "    removed_count = original_count - final_count\n",
    "    print(f\"Original rows: {original_count:,}\")\n",
    "    print(f\"After deduplication: {final_count:,}\")\n",
    "    print(f\"Removed duplicates: {removed_count:,} ({(removed_count/original_count)*100:.2f}%)\")\n",
    "    \n",
    "    return df_deduplicated\n",
    "\n",
    "# Remove duplicates from all datasets\n",
    "transactions_clean = remove_duplicates(transactions_df, \"transactions\")\n",
    "customers_clean = remove_duplicates(customers_df, \"customers\")\n",
    "articles_clean = remove_duplicates(articles_df, \"articles\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Missing Value Handling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_transactions_missing_values(df):\n",
    "    \"\"\"\n",
    "    Handle missing values in transactions dataset according to user specifications\n",
    "    \"\"\"\n",
    "    print(\"\\n=== HANDLING MISSING VALUES: TRANSACTIONS ===\")\n",
    "    \n",
    "    original_count = df.height\n",
    "    \n",
    "    # 1. Drop rows with missing customer_id or article_id\n",
    "    df = df.filter(\n",
    "        (pl.col(\"customer_id\").is_not_null()) & \n",
    "        (pl.col(\"customer_id\") != \"\") &\n",
    "        (pl.col(\"article_id\").is_not_null()) & \n",
    "        (pl.col(\"article_id\") != \"\")\n",
    "    )\n",
    "    \n",
    "    after_id_drop = df.height\n",
    "    print(f\"Dropped {original_count - after_id_drop:,} rows with missing customer_id or article_id\")\n",
    "    \n",
    "    # 2. Fill missing prices with median price for that article_id\n",
    "    # Calculate median price per article_id\n",
    "    article_median_prices = (\n",
    "        df.filter(pl.col(\"price\").is_not_null())\n",
    "        .group_by(\"article_id\")\n",
    "        .agg(pl.col(\"price\").median().alias(\"median_price\"))\n",
    "    )\n",
    "    \n",
    "    # Join and fill missing prices\n",
    "    df = df.join(article_median_prices, on=\"article_id\", how=\"left\")\n",
    "    df = df.with_columns(\n",
    "        pl.when(pl.col(\"price\").is_null())\n",
    "        .then(pl.col(\"median_price\"))\n",
    "        .otherwise(pl.col(\"price\"))\n",
    "        .alias(\"price\")\n",
    "    ).drop(\"median_price\")\n",
    "    \n",
    "    # 3. Fill missing sales_channel_id with mode\n",
    "    mode_channel = (\n",
    "        df.filter(pl.col(\"sales_channel_id\").is_not_null())\n",
    "        .group_by(\"sales_channel_id\")\n",
    "        .agg(pl.count().alias(\"count\"))\n",
    "        .sort(\"count\", descending=True)\n",
    "        .select(\"sales_channel_id\")\n",
    "        .item(0, 0)\n",
    "    )\n",
    "    \n",
    "    df = df.with_columns(\n",
    "        pl.when(pl.col(\"sales_channel_id\").is_null())\n",
    "        .then(pl.lit(mode_channel))\n",
    "        .otherwise(pl.col(\"sales_channel_id\"))\n",
    "        .alias(\"sales_channel_id\")\n",
    "    )\n",
    "    \n",
    "    # 4. Handle missing t_dat (interpolate based on customer patterns)\n",
    "    # Calculate median date per customer (convert to days since epoch for median calculation)\n",
    "    customer_median_dates = (\n",
    "        df.filter(pl.col(\"t_dat\").is_not_null())\n",
    "        .with_columns(\n",
    "            pl.col(\"t_dat\").str.to_date(\"%Y-%m-%d\").dt.epoch(\"d\").alias(\"date_days\")\n",
    "        )\n",
    "        .group_by(\"customer_id\")\n",
    "        .agg(pl.col(\"date_days\").median().alias(\"median_date_days\"))\n",
    "    )\n",
    "    \n",
    "    # Join and fill missing dates\n",
    "    df = df.join(customer_median_dates, on=\"customer_id\", how=\"left\")\n",
    "    df = df.with_columns(\n",
    "        pl.when(pl.col(\"t_dat\").is_null())\n",
    "        .then(\n",
    "            pl.from_epoch(pl.col(\"median_date_days\") * 24 * 60 * 60, \"s\")\n",
    "            .dt.strftime(\"%Y-%m-%d\")\n",
    "        )\n",
    "        .otherwise(pl.col(\"t_dat\"))\n",
    "        .alias(\"t_dat\")\n",
    "    ).drop(\"median_date_days\")\n",
    "    \n",
    "    final_count = df.height\n",
    "    print(f\"Final transaction count: {final_count:,}\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "transactions_clean = handle_transactions_missing_values(transactions_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_customers_missing_values(df):\n",
    "    \"\"\"\n",
    "    Handle missing values in customers dataset according to user specifications\n",
    "    \"\"\"\n",
    "    print(\"\\n=== HANDLING MISSING VALUES: CUSTOMERS ===\")\n",
    "    \n",
    "    # Calculate median age for imputation\n",
    "    median_age = df.filter(pl.col(\"age\").is_not_null()).select(pl.col(\"age\").median()).item()\n",
    "    print(f\"Median age for imputation: {median_age}\")\n",
    "    \n",
    "    # Apply missing value handling rules\n",
    "    df = df.with_columns([\n",
    "        # FN: Fill with 0\n",
    "        pl.when((pl.col(\"FN\").is_null()) | (pl.col(\"FN\") == \"\"))\n",
    "        .then(pl.lit(0))\n",
    "        .otherwise(pl.col(\"FN\"))\n",
    "        .alias(\"FN\"),\n",
    "        \n",
    "        # Active: Fill with \"UNKNOWN\"\n",
    "        pl.when((pl.col(\"Active\").is_null()) | (pl.col(\"Active\") == \"\"))\n",
    "        .then(pl.lit(\"UNKNOWN\"))\n",
    "        .otherwise(pl.col(\"Active\"))\n",
    "        .alias(\"Active\"),\n",
    "        \n",
    "        # club_member_status: Fill with \"INACTIVE\"\n",
    "        pl.when((pl.col(\"club_member_status\").is_null()) | (pl.col(\"club_member_status\") == \"\"))\n",
    "        .then(pl.lit(\"INACTIVE\"))\n",
    "        .otherwise(pl.col(\"club_member_status\"))\n",
    "        .alias(\"club_member_status\"),\n",
    "        \n",
    "        # fashion_news_frequency: Fill with \"NONE\"\n",
    "        pl.when((pl.col(\"fashion_news_frequency\").is_null()) | (pl.col(\"fashion_news_frequency\") == \"\"))\n",
    "        .then(pl.lit(\"NONE\"))\n",
    "        .otherwise(pl.col(\"fashion_news_frequency\"))\n",
    "        .alias(\"fashion_news_frequency\"),\n",
    "        \n",
    "        # age: Fill with median age\n",
    "        pl.when(pl.col(\"age\").is_null())\n",
    "        .then(pl.lit(median_age))\n",
    "        .otherwise(pl.col(\"age\"))\n",
    "        .alias(\"age\"),\n",
    "        \n",
    "        # postal_code: Fill with \"UNKNOWN\"\n",
    "        pl.when((pl.col(\"postal_code\").is_null()) | (pl.col(\"postal_code\") == \"\"))\n",
    "        .then(pl.lit(\"UNKNOWN\"))\n",
    "        .otherwise(pl.col(\"postal_code\"))\n",
    "        .alias(\"postal_code\")\n",
    "    ])\n",
    "    \n",
    "    print(\"Customer missing values handled successfully\")\n",
    "    return df\n",
    "\n",
    "customers_clean = handle_customers_missing_values(customers_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def handle_articles_missing_values(df):\n",
    "    \"\"\"\n",
    "    Handle missing values in articles dataset according to user specifications\n",
    "    \"\"\"\n",
    "    print(\"\\n=== HANDLING MISSING VALUES: ARTICLES ===\")\n",
    "    \n",
    "    original_count = df.height\n",
    "    \n",
    "    # Drop rows with missing article_id\n",
    "    df = df.filter(\n",
    "        (pl.col(\"article_id\").is_not_null()) & (pl.col(\"article_id\") != \"\")\n",
    "    )\n",
    "    after_id_drop = df.height\n",
    "    print(f\"Dropped {original_count - after_id_drop:,} rows with missing article_id\")\n",
    "    \n",
    "    # Handle categorical fields - fill with \"UNKNOWN\"\n",
    "    categorical_fields = [\n",
    "        \"product_code\", \"prod_name\", \"product_type_name\", \"product_group_name\",\n",
    "        \"graphical_appearance_name\", \"colour_group_name\", \"perceived_colour_value_name\",\n",
    "        \"perceived_colour_master_name\", \"department_name\", \"index_name\",\n",
    "        \"index_group_name\", \"section_name\", \"garment_group_name\", \"index_code\"\n",
    "    ]\n",
    "    \n",
    "    # Handle numerical code fields - fill with 0\n",
    "    numerical_fields = [\n",
    "        \"product_type_no\", \"graphical_appearance_no\", \"colour_group_code\",\n",
    "        \"perceived_colour_value_id\", \"perceived_colour_master_id\", \"department_no\",\n",
    "        \"index_group_no\", \"section_no\", \"garment_group_no\"\n",
    "    ]\n",
    "    \n",
    "    # Build expressions for all columns\n",
    "    expressions = []\n",
    "    \n",
    "    for col_name in df.columns:\n",
    "        if col_name in categorical_fields:\n",
    "            expressions.append(\n",
    "                pl.when((pl.col(col_name).is_null()) | (pl.col(col_name) == \"\"))\n",
    "                .then(pl.lit(\"UNKNOWN\"))\n",
    "                .otherwise(pl.col(col_name))\n",
    "                .alias(col_name)\n",
    "            )\n",
    "        elif col_name in numerical_fields:\n",
    "            expressions.append(\n",
    "                pl.when(pl.col(col_name).is_null())\n",
    "                .then(pl.lit(0))\n",
    "                .otherwise(pl.col(col_name))\n",
    "                .alias(col_name)\n",
    "            )\n",
    "        elif col_name == \"detail_desc\":\n",
    "            expressions.append(\n",
    "                pl.when((pl.col(col_name).is_null()) | (pl.col(col_name) == \"\"))\n",
    "                .then(pl.lit(\"NO_DESCRIPTION\"))\n",
    "                .otherwise(pl.col(col_name))\n",
    "                .alias(col_name)\n",
    "            )\n",
    "        else:\n",
    "            expressions.append(pl.col(col_name))\n",
    "    \n",
    "    df = df.with_columns(expressions)\n",
    "    \n",
    "    final_count = df.height\n",
    "    print(f\"Final article count: {final_count:,}\")\n",
    "    print(\"Article missing values handled successfully\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "articles_clean = handle_articles_missing_values(articles_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Outlier Detection and Handling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_and_handle_outliers(df, dataset_name, numerical_columns):\n",
    "    \"\"\"\n",
    "    Detect and handle outliers using IQR method with capping\n",
    "    \"\"\"\n",
    "    print(f\"\\n=== OUTLIER DETECTION AND HANDLING: {dataset_name.upper()} ===\")\n",
    "    \n",
    "    for col_name in numerical_columns:\n",
    "        if col_name in df.columns:\n",
    "            print(f\"\\nProcessing outliers for {col_name}:\")\n",
    "            \n",
    "            # Calculate quartiles\n",
    "            quartiles = df.select([\n",
    "                pl.col(col_name).quantile(0.25).alias(\"q1\"),\n",
    "                pl.col(col_name).quantile(0.75).alias(\"q3\")\n",
    "            ]).to_dict(as_series=False)\n",
    "            \n",
    "            q1, q3 = quartiles[\"q1\"][0], quartiles[\"q3\"][0]\n",
    "            iqr = q3 - q1\n",
    "            lower_bound = q1 - 1.5 * iqr\n",
    "            upper_bound = q3 + 1.5 * iqr\n",
    "            \n",
    "            print(f\"  Q1: {q1}, Q3: {q3}, IQR: {iqr}\")\n",
    "            print(f\"  Lower bound: {lower_bound}, Upper bound: {upper_bound}\")\n",
    "            \n",
    "            # Count outliers\n",
    "            outlier_count = df.filter(\n",
    "                (pl.col(col_name) < lower_bound) | (pl.col(col_name) > upper_bound)\n",
    "            ).height\n",
    "            \n",
    "            print(f\"  Outliers detected: {outlier_count:,}\")\n",
    "            \n",
    "            # Cap outliers instead of removing them\n",
    "            df = df.with_columns(\n",
    "                pl.when(pl.col(col_name) < lower_bound)\n",
    "                .then(pl.lit(lower_bound))\n",
    "                .when(pl.col(col_name) > upper_bound)\n",
    "                .then(pl.lit(upper_bound))\n",
    "                .otherwise(pl.col(col_name))\n",
    "                .alias(col_name)\n",
    "            )\n",
    "            \n",
    "            print(f\"  Outliers capped to bounds\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Handle outliers for each dataset\n",
    "transactions_clean = detect_and_handle_outliers(\n",
    "    transactions_clean, \"transactions\", [\"price\"]\n",
    ")\n",
    "\n",
    "customers_clean = detect_and_handle_outliers(\n",
    "    customers_clean, \"customers\", [\"age\"]\n",
    ")\n",
    "\n",
    "articles_clean = detect_and_handle_outliers(\n",
    "    articles_clean, \"articles\", \n",
    "    [\"product_type_no\", \"graphical_appearance_no\", \"colour_group_code\",\n",
    "     \"perceived_colour_value_id\", \"perceived_colour_master_id\", \"department_no\",\n",
    "     \"index_group_no\", \"section_no\", \"garment_group_no\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Validation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_cleaned_data(transactions_df, customers_df, articles_df):\n",
    "    \"\"\"\n",
    "    Perform comprehensive data validation on cleaned datasets\n",
    "    \"\"\"\n",
    "    print(\"\\n=== DATA VALIDATION ===\")\n",
    "    \n",
    "    validation_results = {}\n",
    "    \n",
    "    # 1. Check for remaining null values\n",
    "    print(\"\\n--- Checking for remaining null values ---\")\n",
    "    datasets = [(transactions_df, \"transactions\"), (customers_df, \"customers\"), (articles_df, \"articles\")]\n",
    "    \n",
    "    for df, name in datasets:\n",
    "        null_counts = df.null_count()\n",
    "        has_nulls = False\n",
    "        \n",
    "        for col_name in df.columns:\n",
    "            null_count = null_counts.select(col_name).item()\n",
    "            if null_count > 0:\n",
    "                if not has_nulls:\n",
    "                    print(f\"{name}: Found nulls in columns:\")\n",
    "                    has_nulls = True\n",
    "                print(f\"  {col_name}: {null_count:,} nulls\")\n",
    "        \n",
    "        if not has_nulls:\n",
    "            print(f\"{name}: No null values found ✓\")\n",
    "        \n",
    "        validation_results[f\"{name}_has_nulls\"] = has_nulls\n",
    "    \n",
    "    # 2. Check data consistency\n",
    "    print(\"\\n--- Data consistency checks ---\")\n",
    "    \n",
    "    # Check if all customer_ids in transactions exist in customers\n",
    "    transaction_customers = transactions_df.select(\"customer_id\").unique()\n",
    "    customer_ids = customers_df.select(\"customer_id\").unique()\n",
    "    \n",
    "    missing_customers = transaction_customers.join(\n",
    "        customer_ids, on=\"customer_id\", how=\"anti\"\n",
    "    ).height\n",
    "    print(f\"Customers in transactions but not in customers table: {missing_customers:,}\")\n",
    "    validation_results[\"missing_customers\"] = missing_customers\n",
    "    \n",
    "    # Check if all article_ids in transactions exist in articles\n",
    "    transaction_articles = transactions_df.select(\"article_id\").unique()\n",
    "    article_ids = articles_df.select(\"article_id\").unique()\n",
    "    \n",
    "    missing_articles = transaction_articles.join(\n",
    "        article_ids, on=\"article_id\", how=\"anti\"\n",
    "    ).height\n",
    "    print(f\"Articles in transactions but not in articles table: {missing_articles:,}\")\n",
    "    validation_results[\"missing_articles\"] = missing_articles\n",
    "    \n",
    "    # 3. Check data ranges\n",
    "    print(\"\\n--- Data range validation ---\")\n",
    "    \n",
    "    # Age validation\n",
    "    age_stats = customers_df.select([\n",
    "        pl.col(\"age\").min().alias(\"min_age\"),\n",
    "        pl.col(\"age\").max().alias(\"max_age\"),\n",
    "        pl.col(\"age\").mean().alias(\"avg_age\")\n",
    "    ]).to_dict(as_series=False)\n",
    "    \n",
    "    print(f\"Age range: {age_stats['min_age'][0]:.0f} - {age_stats['max_age'][0]:.0f} (avg: {age_stats['avg_age'][0]:.1f})\")\n",
    "    \n",
    "    # Price validation\n",
    "    price_stats = transactions_df.select([\n",
    "        pl.col(\"price\").min().alias(\"min_price\"),\n",
    "        pl.col(\"price\").max().alias(\"max_price\"),\n",
    "        pl.col(\"price\").mean().alias(\"avg_price\")\n",
    "    ]).to_dict(as_series=False)\n",
    "    \n",
    "    print(f\"Price range: {price_stats['min_price'][0]:.4f} - {price_stats['max_price'][0]:.4f} (avg: {price_stats['avg_price'][0]:.4f})\")\n",
    "    \n",
    "    # Date validation\n",
    "    date_stats = transactions_df.select([\n",
    "        pl.col(\"t_dat\").min().alias(\"min_date\"),\n",
    "        pl.col(\"t_dat\").max().alias(\"max_date\")\n",
    "    ]).to_dict(as_series=False)\n",
    "    \n",
    "    print(f\"Date range: {date_stats['min_date'][0]} - {date_stats['max_date'][0]}\")\n",
    "    \n",
    "    validation_results[\"age_stats\"] = age_stats\n",
    "    validation_results[\"price_stats\"] = price_stats\n",
    "    validation_results[\"date_stats\"] = date_stats\n",
    "    \n",
    "    print(\"\\n=== VALIDATION COMPLETE ===\")\n",
    "    return validation_results\n",
    "\n",
    "validation_results = validate_cleaned_data(transactions_clean, customers_clean, articles_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Final Data Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_final_summary(transactions_df, customers_df, articles_df):\n",
    "    \"\"\"\n",
    "    Generate final summary of cleaned datasets\n",
    "    \"\"\"\n",
    "    print(\"\\n=== FINAL CLEANED DATA SUMMARY ===\")\n",
    "    \n",
    "    datasets = [\n",
    "        (transactions_df, \"Transactions\"),\n",
    "        (customers_df, \"Customers\"), \n",
    "        (articles_df, \"Articles\")\n",
    "    ]\n",
    "    \n",
    "    for df, name in datasets:\n",
    "        print(f\"\\n--- {name} ---\")\n",
    "        print(f\"Rows: {df.height:,}\")\n",
    "        print(f\"Columns: {df.width}\")\n",
    "        print(f\"Columns: {', '.join(df.columns)}\")\n",
    "        \n",
    "        # Show sample data\n",
    "        print(f\"\\nSample data:\")\n",
    "        print(df.head(3))\n",
    "\n",
    "generate_final_summary(transactions_clean, customers_clean, articles_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Export to Parquet Format\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_cleaned_data(transactions_df, customers_df, articles_df, output_path=\"data/processed/\"):\n",
    "    \"\"\"\n",
    "    Save cleaned datasets as Parquet files\n",
    "    \"\"\"\n",
    "    print(\"\\n=== SAVING CLEANED DATA ===\")\n",
    "    \n",
    "    # Create output directory if it doesn't exist\n",
    "    Path(output_path).mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    try:\n",
    "        # Save transactions\n",
    "        print(\"Saving transactions...\")\n",
    "        transactions_df.write_parquet(f\"{output_path}transactions_cleaned.parquet\")\n",
    "        print(f\"✓ Transactions saved to {output_path}transactions_cleaned.parquet\")\n",
    "        \n",
    "        # Save customers\n",
    "        print(\"Saving customers...\")\n",
    "        customers_df.write_parquet(f\"{output_path}customers_cleaned.parquet\")\n",
    "        print(f\"✓ Customers saved to {output_path}customers_cleaned.parquet\")\n",
    "        \n",
    "        # Save articles\n",
    "        print(\"Saving articles...\")\n",
    "        articles_df.write_parquet(f\"{output_path}articles_cleaned.parquet\")\n",
    "        print(f\"✓ Articles saved to {output_path}articles_cleaned.parquet\")\n",
    "        \n",
    "        print(\"\\n=== ALL DATA SAVED SUCCESSFULLY ===\")\n",
    "        \n",
    "        # Display file sizes\n",
    "        print(\"\\n--- File Sizes ---\")\n",
    "        for filename in [\"transactions_cleaned.parquet\", \"customers_cleaned.parquet\", \"articles_cleaned.parquet\"]:\n",
    "            filepath = Path(output_path) / filename\n",
    "            if filepath.exists():\n",
    "                size_mb = filepath.stat().st_size / (1024 * 1024)\n",
    "                print(f\"{filename}: {size_mb:.2f} MB\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error saving data: {str(e)}\")\n",
    "        raise\n",
    "\n",
    "save_cleaned_data(transactions_clean, customers_clean, articles_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Performance Metrics and Summary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_performance_metrics():\n",
    "    \"\"\"\n",
    "    Display final performance metrics and summary\n",
    "    \"\"\"\n",
    "    print(\"\\n=== PERFORMANCE METRICS ===\")\n",
    "    \n",
    "    print(f\"Polars version: {pl.__version__}\")\n",
    "    print(f\"Streaming chunk size: {pl.Config.get_streaming_chunk_size()}\")\n",
    "    \n",
    "    print(\"\\n=== PREPROCESSING COMPLETE ===\")\n",
    "    print(\"All datasets have been successfully:\")\n",
    "    print(\"✓ Loaded with Polars for high performance\")\n",
    "    print(\"✓ Deduplicated\")\n",
    "    print(\"✓ Missing values handled per specifications\")\n",
    "    print(\"✓ Outliers processed using IQR capping\")\n",
    "    print(\"✓ Data validated for consistency\")\n",
    "    print(\"✓ Saved as optimized Parquet files\")\n",
    "    \n",
    "    print(\"\\n=== NEXT STEPS ===\")\n",
    "    print(\"Your cleaned datasets are ready for:\")\n",
    "    print(\"• Feature engineering\")\n",
    "    print(\"• Customer segmentation analysis\")\n",
    "    print(\"• Recommendation system development\")\n",
    "    print(\"• Statistical modeling and machine learning\")\n",
    "\n",
    "display_performance_metrics()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
