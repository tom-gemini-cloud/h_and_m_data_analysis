{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": "# H&M Recommendation System Model Evaluation\n\nThis notebook evaluates the trained recommendation models from the modelling phase.\n\n## Evaluation Approaches\n1. **Model Loading** - Load pre-trained models\n2. **Performance Metrics** - Evaluate recommendation quality\n3. **Comparative Analysis** - Compare different model approaches\n4. **Business Impact Assessment** - Evaluate practical utility\n\n## Models to Evaluate\n- Collaborative Filtering (SVD-based)\n- Content-Based Filtering (TF-IDF similarity)\n- Purchase Prediction (Classification models)\n- Hybrid Recommender (Combined approach)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": "import sys\nimport os\nsys.path.append('../')\n\nimport polars as pl\nimport pandas as pd\nimport numpy as np\nfrom pathlib import Path\nimport json\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Import H&M data modelling modules\nfrom hnm_data_analysis.data_modelling import (\n    CollaborativeFilteringModel,\n    ContentBasedFilteringModel,\n    PurchasePredictionModel,\n    HybridRecommenderModel\n)\n\n# Visualisation\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n\n# Set display options\npl.Config.set_tbl_rows(10)\npl.Config.set_tbl_cols(15)\npd.set_option('display.max_columns', 20)\npd.set_option('display.max_rows', 10)\n\nprint(\"Libraries imported successfully\")\nprint(f\"Current working directory: {os.getcwd()}\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-2",
   "metadata": {},
   "source": [
    "## Load Test Data\n",
    "\n",
    "Load the test dataset for evaluation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "print(\"Loading test dataset...\")\n",
    "\n",
    "test_df = pl.read_parquet(\"../data/modelling_data/test_data.parquet\")\n",
    "test_pd = test_df.to_pandas()\n",
    "\n",
    "print(f\"Test data shape: {test_df.shape}\")\n",
    "print(f\"Test customers: {test_df['customer_id'].n_unique():,}\")\n",
    "print(f\"Test articles: {test_df['article_id'].n_unique():,}\")\n",
    "\n",
    "# Sample for faster evaluation if needed\n",
    "USE_SAMPLE = True\n",
    "SAMPLE_SIZE = 10000\n",
    "\n",
    "if USE_SAMPLE and len(test_pd) > SAMPLE_SIZE:\n",
    "    print(f\"\\nSampling {SAMPLE_SIZE:,} test transactions for evaluation...\")\n",
    "    test_pd = test_pd.sample(n=SAMPLE_SIZE, random_state=42).reset_index(drop=True)\n",
    "    print(f\"Sampled test data shape: {test_pd.shape}\")\n",
    "\n",
    "print(\"\\nTest data loaded successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "## Load Trained Models\n",
    "\n",
    "Load the models trained in the modelling phase.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "models_dir = Path(\"../models\")\n",
    "\n",
    "print(\"Checking for trained models...\")\n",
    "\n",
    "# Check if models directory exists\n",
    "if not models_dir.exists():\n",
    "    print(f\"‚ùå Models directory not found: {models_dir}\")\n",
    "    print(\"Please run the modelling notebook first to train and save models.\")\n",
    "    loaded_models = {}\n",
    "else:\n",
    "    # Define model files to look for\n",
    "    model_files = {\n",
    "        'collaborative_filtering': models_dir / 'collaborative_filtering_model.pkl',\n",
    "        'content_based': models_dir / 'content_based_filtering_model.pkl',\n",
    "        'purchase_prediction': models_dir / 'purchase_prediction_model.pkl',\n",
    "        'hybrid': models_dir / 'hybrid_recommender_model.pkl'\n",
    "    }\n",
    "\n",
    "    loaded_models = {}\n",
    "    \n",
    "    print(\"Scanning for available model files...\")\n",
    "    \n",
    "    for model_name, model_path in model_files.items():\n",
    "        if model_path.exists():\n",
    "            print(f\"‚úÖ Found {model_name} model file\")\n",
    "            try:\n",
    "                if model_name == 'collaborative_filtering':\n",
    "                    model = CollaborativeFilteringModel()\n",
    "                    model.load_model(str(model_path))\n",
    "                elif model_name == 'content_based':\n",
    "                    model = ContentBasedFilteringModel()\n",
    "                    model.load_model(str(model_path))\n",
    "                elif model_name == 'purchase_prediction':\n",
    "                    model = PurchasePredictionModel()\n",
    "                    model.load_model(str(model_path))\n",
    "                elif model_name == 'hybrid':\n",
    "                    model = HybridRecommenderModel()\n",
    "                    model.load_model(str(model_path))\n",
    "                \n",
    "                loaded_models[model_name] = model\n",
    "                print(f\"‚úÖ Successfully loaded {model_name} model\")\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Failed to load {model_name} model: {e}\")\n",
    "        else:\n",
    "            print(f\"‚ö†Ô∏è  {model_name} model file not found: {model_path.name}\")\n",
    "\n",
    "print(f\"\\nüìä Total models loaded: {len(loaded_models)}\")\n",
    "\n",
    "if loaded_models:\n",
    "    print(\"Available models for evaluation:\")\n",
    "    for model_name in loaded_models.keys():\n",
    "        print(f\"  - {model_name.replace('_', ' ').title()}\")\n",
    "        \n",
    "    # Load model summary if available\n",
    "    summary_path = models_dir / 'model_summary.json'\n",
    "    if summary_path.exists():\n",
    "        try:\n",
    "            with open(summary_path, 'r') as f:\n",
    "                model_summary = json.load(f)\n",
    "            print(\"\\nüìã Model training summary found:\")\n",
    "            for model_name, info in model_summary.items():\n",
    "                if any(model_name.lower().replace(' ', '_') in key for key in loaded_models.keys()):\n",
    "                    print(f\"  {model_name}: {len(info)} properties\")\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è  Could not load model summary: {e}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è  Model summary file not found\")\n",
    "else:\n",
    "    print(\"‚ùå No models available for evaluation!\")\n",
    "    print(\"Please run the modelling notebook first to train models.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "## Recommendation Quality Evaluation\n",
    "\n",
    "Evaluate the quality of recommendations from each model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": "print(\"=== Recommendation Quality Evaluation ===\\n\")\n\nif not loaded_models:\n    print(\"‚ùå No models available for evaluation.\")\nelse:\n    # Get customers that exist in training data for fair evaluation\n    print(\"Finding customers from training data for evaluation...\")\n    \n    # Try to get training customers from one of the loaded models\n    training_customers = None\n    for model_name, model in loaded_models.items():\n        if hasattr(model, 'all_customers') and model.all_customers is not None:\n            training_customers = model.all_customers[:20]  # Use first 20 training customers\n            print(f\"Found {len(training_customers)} training customers from {model_name} model\")\n            break\n    \n    # Fallback: use test customers but expect some failures\n    if training_customers is None:\n        print(\"‚ö†Ô∏è  Could not access training customers, using test customers (expect some failures)\")\n        training_customers = test_pd['customer_id'].unique()[:10]\n    \n    evaluation_results = {}\n    \n    for model_name, model in loaded_models.items():\n        print(f\"üìä Evaluating {model_name.replace('_', ' ').title()} Model...\")\n        \n        if model_name == 'purchase_prediction':\n            # Purchase prediction evaluation\n            try:\n                model_scores = model.get_model_scores()\n                print(f\"  Purchase prediction performance:\")\n                for alg_name, scores in model_scores.items():\n                    print(f\"    {alg_name}: F1={scores['f1_score']:.3f}, AUC={scores['auc_roc']:.3f}\")\n                continue\n            except Exception as e:\n                print(f\"  ‚ùå Error evaluating purchase prediction: {e}\")\n                continue\n        \n        # Recommendation evaluation\n        recommendation_results = []\n        successful_customers = []\n        \n        for customer_id in training_customers:\n            try:\n                recommendations = model.get_recommendations(customer_id, n_recommendations=5)\n                \n                if recommendations:\n                    avg_score = np.mean([score for _, score in recommendations])\n                    recommendation_results.append({\n                        'customer_id': customer_id,\n                        'num_recommendations': len(recommendations),\n                        'avg_score': avg_score,\n                        'recommendations': recommendations\n                    })\n                    successful_customers.append(customer_id)\n                    \n            except Exception as e:\n                # Only show first few errors to avoid spam\n                if len(successful_customers) < 3:\n                    print(f\"    ‚ö†Ô∏è  Error for customer {str(customer_id)[:10]}...: {str(e)[:40]}...\")\n        \n        if recommendation_results:\n            avg_score = np.mean([r['avg_score'] for r in recommendation_results])\n            coverage = len(recommendation_results) / len(training_customers)\n            \n            evaluation_results[model_name] = {\n                'average_score': avg_score,\n                'coverage': coverage,\n                'successful_recommendations': len(recommendation_results),\n                'total_tested': len(training_customers)\n            }\n            \n            print(f\"  ‚úÖ Average score: {avg_score:.4f}\")\n            print(f\"  ‚úÖ Coverage: {coverage:.2%} ({len(recommendation_results)}/{len(training_customers)})\") \n            \n            # Store a successful customer for sample display\n            if successful_customers:\n                globals()[f'{model_name}_sample_customer'] = successful_customers[0]\n        else:\n            print(f\"  ‚ùå No successful recommendations generated\")\n        \n        print()\n    \n    print(\"‚úÖ Recommendation evaluation complete\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "## Sample Recommendations Display\n",
    "\n",
    "Display sample recommendations from each model for comparison.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Sample Recommendations Comparison ===\\n\")\n",
    "\n",
    "if not loaded_models:\n",
    "    print(\"‚ùå No models available for recommendation display.\")\n",
    "else:\n",
    "    recommendation_models = {name: model for name, model in loaded_models.items() \n",
    "                           if name != 'purchase_prediction'}\n",
    "    \n",
    "    if not recommendation_models:\n",
    "        print(\"‚ö†Ô∏è  No recommendation models available (only classification models loaded)\")\n",
    "    else:\n",
    "        # Try to find a customer that works for at least one model\n",
    "        sample_customer = None\n",
    "        \n",
    "        # First, try to use stored successful customers from evaluation\n",
    "        for model_name in recommendation_models.keys():\n",
    "            stored_customer_var = f'{model_name}_sample_customer'\n",
    "            if stored_customer_var in globals():\n",
    "                sample_customer = globals()[stored_customer_var]\n",
    "                print(f\"Using successful customer from {model_name} evaluation\")\n",
    "                break\n",
    "        \n",
    "        # Fallback: try to find any customer that works\n",
    "        if sample_customer is None:\n",
    "            print(\"Looking for a customer that works with the models...\")\n",
    "            # Try customers from the first available model's training data\n",
    "            for model_name, model in recommendation_models.items():\n",
    "                if hasattr(model, 'all_customers') and model.all_customers is not None:\n",
    "                    for test_customer in model.all_customers[:5]:\n",
    "                        try:\n",
    "                            test_recs = model.get_recommendations(test_customer, n_recommendations=1)\n",
    "                            if test_recs:\n",
    "                                sample_customer = test_customer\n",
    "                                print(f\"Found working customer: {str(sample_customer)[:20]}...\")\n",
    "                                break\n",
    "                        except:\n",
    "                            continue\n",
    "                    if sample_customer:\n",
    "                        break\n",
    "        \n",
    "        if sample_customer is None:\n",
    "            print(\"‚ùå Could not find a customer that works with any model\")\n",
    "            print(\"This suggests the models may not have been trained properly or have no training data\")\n",
    "        else:\n",
    "            print(f\"üéØ Sample recommendations for Customer: {str(sample_customer)[:20]}...\\\\n\")\n",
    "            \n",
    "            for model_name, model in recommendation_models.items():\n",
    "                print(f\"**{model_name.replace('_', ' ').title()} Model:**\")\n",
    "                \n",
    "                try:\n",
    "                    recommendations = model.get_recommendations(sample_customer, n_recommendations=5)\n",
    "                    \n",
    "                    if recommendations:\n",
    "                        for i, (article_id, score) in enumerate(recommendations, 1):\n",
    "                            print(f\"  {i}. Article {article_id}: Score {score:.4f}\")\n",
    "                    else:\n",
    "                        print(\"  ‚ö†Ô∏è  No recommendations available for this customer\")\n",
    "                        \n",
    "                except Exception as e:\n",
    "                    print(f\"  ‚ùå Error: {str(e)[:50]}...\")\n",
    "                \n",
    "                print()\n",
    "\n",
    "print(\"Sample recommendations display complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": "## Model Performance Visualisation\n\nCreate visualisations comparing model performance."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": "print(\"=== Model Performance Visualisation ===\\n\")\n\nif evaluation_results:\n    # Create comparison dataframe\n    comparison_df = pd.DataFrame(evaluation_results).T\n    comparison_df = comparison_df.reset_index().rename(columns={'index': 'model'})\n    \n    print(\"Model Performance Summary:\")\n    display(comparison_df)\n    \n    # Create visualisations\n    fig, axes = plt.subplots(1, 2, figsize=(15, 6))\n    \n    # Average Score Comparison\n    axes[0].bar(comparison_df['model'], comparison_df['average_score'])\n    axes[0].set_title('Average Recommendation Score by Model')\n    axes[0].set_xlabel('Model')\n    axes[0].set_ylabel('Average Score')\n    axes[0].tick_params(axis='x', rotation=45)\n    \n    # Coverage Comparison\n    axes[1].bar(comparison_df['model'], comparison_df['coverage'])\n    axes[1].set_title('Customer Coverage by Model')\n    axes[1].set_xlabel('Model')\n    axes[1].set_ylabel('Coverage Rate')\n    axes[1].tick_params(axis='x', rotation=45)\n    axes[1].set_ylim(0, 1)\n    \n    plt.tight_layout()\n    \n    # Save the visualisation to results/modelling\n    fig_path = \"../results/modelling/model_performance_comparison.png\"\n    plt.savefig(fig_path, dpi=300, bbox_inches='tight')\n    print(f\"üìä Performance comparison chart saved to: {fig_path}\")\n    plt.show()\n    \n    # Interactive plotly visualisation\n    fig_plotly = make_subplots(\n        rows=1, cols=2,\n        subplot_titles=('Average Score', 'Coverage Rate')\n    )\n    \n    fig_plotly.add_trace(\n        go.Bar(x=comparison_df['model'], y=comparison_df['average_score'], name='Avg Score'),\n        row=1, col=1\n    )\n    \n    fig_plotly.add_trace(\n        go.Bar(x=comparison_df['model'], y=comparison_df['coverage'], name='Coverage'),\n        row=1, col=2\n    )\n    \n    fig_plotly.update_layout(\n        title_text=\"Model Performance Comparison\",\n        showlegend=False,\n        height=500\n    )\n    \n    # Save interactive plot as HTML\n    html_path = \"../results/modelling/model_performance_interactive.html\"\n    fig_plotly.write_html(html_path)\n    print(f\"üìä Interactive chart saved to: {html_path}\")\n    \n    fig_plotly.show()\n\nelse:\n    print(\"No evaluation results to visualise\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-12",
   "metadata": {},
   "source": [
    "## Business Impact Assessment\n",
    "\n",
    "Assess the practical business value of different recommendation approaches.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-13",
   "metadata": {},
   "outputs": [],
   "source": "print(\"=== Business Impact Assessment ===\\n\")\n\n# Model characteristics assessment\nmodel_assessment = {\n    'Collaborative Filtering': {\n        'strengths': ['Discovers user preferences', 'Good for cross-selling', 'Handles new products well'],\n        'weaknesses': ['Cold start problem', 'Sparsity issues', 'Computational complexity'],\n        'business_use': 'Personalised homepage recommendations, email campaigns'\n    },\n    'Content-Based Filtering': {\n        'strengths': ['No cold start problem', 'Transparent recommendations', 'Domain knowledge integration'],\n        'weaknesses': ['Limited diversity', 'Requires rich content features', 'Over-specialisation'],\n        'business_use': 'Product detail page recommendations, similar item suggestions'\n    },\n    'Purchase Prediction': {\n        'strengths': ['Direct business metric', 'Probability scores', 'Feature interpretability'],\n        'weaknesses': ['Requires negative sampling', 'Class imbalance', 'Complex feature engineering'],\n        'business_use': 'Inventory planning, targeted promotions, customer segmentation'\n    },\n    'Hybrid Recommender': {\n        'strengths': ['Combines multiple approaches', 'Balanced recommendations', 'Higher coverage'],\n        'weaknesses': ['Increased complexity', 'Parameter tuning', 'Computational overhead'],\n        'business_use': 'Primary recommendation engine, A/B testing baseline'\n    }\n}\n\nfor model_name, assessment in model_assessment.items():\n    if any(model_name.lower().replace(' ', '_') in loaded_name for loaded_name in loaded_models.keys()):\n        print(f\"**{model_name}**\")\n        print(f\"  Strengths: {', '.join(assessment['strengths'])}\")\n        print(f\"  Weaknesses: {', '.join(assessment['weaknesses'])}\")\n        print(f\"  Business Use Cases: {assessment['business_use']}\")\n        print()\n\n# Recommendations for deployment\nprint(\"**Deployment Recommendations:**\")\nprint(\"1. **Hybrid Model**: Primary recommendation engine for balanced performance\")\nprint(\"2. **Content-Based**: Quick recommendations for new users/products\")\nprint(\"3. **Collaborative Filtering**: Discover cross-category preferences\")\nprint(\"4. **Purchase Prediction**: Business analytics and inventory planning\")\nprint(\"\\n**Next Steps:**\")\nprint(\"- A/B testing with real customers\")\nprint(\"- Online evaluation metrics (CTR, conversion rate)\")\nprint(\"- Model retraining pipeline\")\nprint(\"- Real-time inference optimisation\")"
  },
  {
   "cell_type": "markdown",
   "id": "cell-14",
   "metadata": {},
   "source": [
    "## Evaluation Summary\n",
    "\n",
    "Final summary of model evaluation results and recommendations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": "# Generate model evaluation summary report\nprint(\"=== Model Evaluation Summary ===\\n\")\n\nreport_content = []\nreport_content.append(\"# Model Evaluation Summary Report\\n\")\nreport_content.append(f\"**Generated:** {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\")\nreport_content.append(f\"**Models Evaluated:** {len(loaded_models)}\")\nreport_content.append(f\"**Test Dataset Size:** {len(test_pd):,} transactions\")\n\n# Get the number of customers tested from evaluation results or use default\ncustomers_tested = 0\nif evaluation_results:\n    # Get from any model's results\n    for model_results in evaluation_results.values():\n        if 'total_tested' in model_results:\n            customers_tested = model_results['total_tested']\n            break\n\nreport_content.append(f\"**Sample Customers Tested:** {customers_tested}\")\n\n# Console output\nprint(f\"**Models Evaluated:** {len(loaded_models)}\")\nprint(f\"**Test Dataset Size:** {len(test_pd):,} transactions\")\nprint(f\"**Sample Customers Tested:** {customers_tested}\")\n\nif evaluation_results:\n    report_content.append(\"\\n## Performance Rankings\")\n    print(\"\\n**Performance Rankings:**\")\n    \n    # Filter out non-recommendation models for ranking\n    rec_results = {name: results for name, results in evaluation_results.items() \n                   if 'average_score' in results}\n    \n    if rec_results:\n        # Rank by average score\n        score_ranking = sorted(rec_results.items(), \n                              key=lambda x: x[1]['average_score'], reverse=True)\n        \n        report_content.append(\"\\n### By Average Recommendation Score:\")\n        print(\"\\nBy Average Recommendation Score:\")\n        for i, (model_name, results) in enumerate(score_ranking, 1):\n            model_title = model_name.replace('_', ' ').title()\n            line = f\"  {i}. {model_title}: {results['average_score']:.4f}\"\n            report_content.append(line)\n            print(line)\n        \n        # Rank by coverage\n        coverage_ranking = sorted(rec_results.items(), \n                                 key=lambda x: x[1]['coverage'], reverse=True)\n        \n        report_content.append(\"\\n### By Customer Coverage:\")\n        print(\"\\nBy Customer Coverage:\")\n        for i, (model_name, results) in enumerate(coverage_ranking, 1):\n            model_title = model_name.replace('_', ' ').title()\n            line = f\"  {i}. {model_title}: {results['coverage']:.2%}\"\n            report_content.append(line)\n            print(line)\n    else:\n        report_content.append(\"\\n‚ö†Ô∏è  No recommendation models evaluated for ranking\")\n        print(\"\\n‚ö†Ô∏è  No recommendation models evaluated for ranking\")\n\nreport_content.append(\"\\n## Key Findings:\")\nprint(\"\\n**Key Findings:**\")\nif len(loaded_models) > 0:\n    successful_models = len([name for name, results in evaluation_results.items() \n                           if results.get('coverage', 0) > 0 or 'best_f1_score' in results])\n    findings = [\n        f\"- {successful_models}/{len(loaded_models)} models working successfully\"\n    ]\n    \n    if any('average_score' in results for results in evaluation_results.values()):\n        findings.append(\"- Recommendation models generating suggestions\")\n    if any('best_f1_score' in results for results in evaluation_results.values()):\n        findings.append(\"- Classification models trained and scored\")\n    \n    findings.append(\"- Models ready for production deployment and A/B testing\")\n    \n    for finding in findings:\n        report_content.append(finding)\n        print(finding)\nelse:\n    finding = \"- No models available - training required\"\n    report_content.append(finding)\n    print(finding)\n\n# Add detailed results table if available\nif evaluation_results:\n    report_content.append(\"\\n## Detailed Results\")\n    report_content.append(\"\\n| Model | Average Score | Coverage | Successful Recs | Total Tested |\")\n    report_content.append(\"|-------|---------------|----------|-----------------|--------------|\")\n    \n    for model_name, results in evaluation_results.items():\n        if 'average_score' in results:\n            model_title = model_name.replace('_', ' ').title()\n            avg_score = results['average_score']\n            coverage = results['coverage']\n            successful = results['successful_recommendations']\n            total = results['total_tested']\n            report_content.append(f\"| {model_title} | {avg_score:.4f} | {coverage:.2%} | {successful} | {total} |\")\n\nreport_content.append(f\"\\nüéâ **Model evaluation completed successfully!** üéâ\")\n\n# Save report to results/modelling directory\nimport os\nos.makedirs(\"../results/modelling\", exist_ok=True)\n\nreport_path = \"../results/modelling/model_evaluation_summary.md\"\nwith open(report_path, 'w', encoding='utf-8') as f:\n    f.write('\\n'.join(report_content))\n\nprint(f\"\\nüìÅ Report saved to: {report_path}\")\nprint(\"\\nüéâ **Model evaluation completed successfully!** üéâ\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}