{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Combined Article Clustering\n\nThis notebook clusters H&M articles using combined features:\n- SVD (LSA) embeddings derived from TF‑IDF of `detail_desc` (200 dimensions)\n- Categorical features from cleaned articles metadata (24 dimensions)\n\nThe combined approach provides both semantic similarity from text descriptions and categorical similarity from product attributes.\n\n- Loads combined features (SVD + categorical) from the feature engineering pipeline\n- Finds an optimal number of clusters via the elbow method\n- Performs clustering and interprets clusters using product metadata\n- Visualises clusters with PCA and t‑SNE\n\nAll identifiers, comments, and Markdown use UK spellings."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports and configuration\n",
    "import os\n",
    "import sys\n",
    "import polars as pl\n",
    "\n",
    "# Add project root to path\n",
    "sys.path.append('../') \n",
    "\n",
    "from hnm_data_analysis.clustering.article_clustering import ArticleClusterer, ClusteringConfig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Paths\n# Updated to use combined features (SVD + categorical)\nBASE_DIR = PROJECT_ROOT\nDATA_DIR = os.path.join(BASE_DIR, \"data\")\nPROCESSED_DIR = os.path.join(DATA_DIR, \"processed\")\nFEATURES_DIR = os.path.join(DATA_DIR, \"processed\", \"features\")\nRESULTS_DIR = os.path.join(BASE_DIR, \"results\", \"combined_clustering\")\n\n# Use combined features (SVD embeddings + categorical features)\nFEATURES_PATH = os.path.join(FEATURES_DIR, \"combined_features.parquet\")\nARTICLES_PATH = os.path.join(PROCESSED_DIR, \"articles_last_3_months.parquet\")\n\nos.makedirs(RESULTS_DIR, exist_ok=True)\nprint(\"FEATURES_PATH:\", FEATURES_PATH)\nprint(\"ARTICLES_PATH:\", ARTICLES_PATH)\nprint(\"RESULTS_DIR:\", RESULTS_DIR)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Check for combined features (preferred) or fallback to SVD-only\nARTICLE_IDS_PATH = None\nFEATURES_PATH_TO_USE = None\n\ncombined_path = FEATURES_PATH\nsvd_path = os.path.join(FEATURES_DIR, \"svd_embeddings.parquet\")\ntfidf_path = os.path.join(FEATURES_DIR, \"tfidf_features.npz\")\nindex_path = os.path.join(FEATURES_DIR, \"article_id_index.csv\")\n\nif os.path.exists(combined_path):\n    FEATURES_PATH_TO_USE = combined_path\n    print(\"Using combined features (SVD + categorical):\", FEATURES_PATH_TO_USE)\nelif os.path.exists(svd_path):\n    FEATURES_PATH_TO_USE = svd_path\n    print(\"Using SVD embeddings only:\", FEATURES_PATH_TO_USE)\nelif os.path.exists(tfidf_path) and os.path.exists(index_path):\n    FEATURES_PATH_TO_USE = tfidf_path\n    ARTICLE_IDS_PATH = index_path\n    print(\"Using TF-IDF sparse matrix (will densify):\", FEATURES_PATH_TO_USE)\n    print(\"Article IDs index:\", ARTICLE_IDS_PATH)\nelse:\n    raise FileNotFoundError(\n        \"No feature files found. Generate combined, SVD or TF-IDF features first.\"\n    )"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) Load features and prepare clusterer\n",
    "clusterer = ArticleClusterer(\n",
    "    features_path=FEATURES_PATH_TO_USE,\n",
    "    article_ids_path=ARTICLE_IDS_PATH,\n",
    "    articles_metadata_path=ARTICLES_PATH\n",
    ")\n",
    "\n",
    "clusterer.load_features()\n",
    "clusterer.load_articles_metadata()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 2) Find optimal k via elbow (K-means inertia) on combined features\noptimal_k, scores = clusterer.find_optimal_k(k_range=(3, 20), algorithm=\"kmeans\")\nprint(\"Optimal k:\", optimal_k)\n# Scores can be large; preview first few rounded entries\npreview = {k: (round(v, 2) if isinstance(v, (int, float)) else v) for k, v in list(scores.items())[:5]}\nprint(\"Scores preview:\", preview)"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Cluster with K-means using the selected k\n",
    "config = ClusteringConfig(\n",
    "    algorithm=\"kmeans\",\n",
    "    n_clusters=optimal_k,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "results = clusterer.cluster(config)\n",
    "\n",
    "# 5) Interpret clusters\n",
    "summaries = clusterer.interpret_clusters()\n",
    "print(\"\\nCluster summaries (truncated):\")\n",
    "for cid, s in list(summaries.items())[:5]:\n",
    "    print(f\"Cluster {cid}: size={s['size']} ({s['percentage']:.1f}%)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) Visualisations\n",
    "clusterer.visualise_clusters(method=\"pca\", save_path=f\"{RESULTS_DIR}/clusters_pca.png\")\n",
    "clusterer.visualise_clusters(method=\"tsne\", save_path=f\"{RESULTS_DIR}/clusters_tsne.png\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7) Save results\n",
    "clusterer.save_results(RESULTS_DIR)\n",
    "print(\"Saved results to:\", RESULTS_DIR)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}