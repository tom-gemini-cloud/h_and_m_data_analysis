{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# H&M Exploratory Data Analysis (EDA) with Polars\n",
    "\n",
    "This notebook provides an exploratory analysis of the H&M dataset structure and quality using Polars for high-performance data processing. The analysis includes data loading, integration, and quality assessment across transactions, customers, and articles datasets.\n",
    "\n",
    "## Overview\n",
    "\n",
    "- **Transaction Data**: Customer purchase history with temporal patterns\n",
    "- **Customer Data**: Demographic and preference information\n",
    "- **Articles Data**: Product metadata and categorisation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Setup\n",
    "\n",
    "Import Polars and other necessary libraries for data processing, analysis, and visualisation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polars version: 0.20.31\n",
      "Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import time\n",
    "from typing import Tuple, List, Dict, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure Polars for optimal performance\n",
    "try:\n",
    "    pl.Config.set_streaming_chunk_size(10000)\n",
    "    # Some Polars versions may not have all config options\n",
    "    if hasattr(pl.Config, 'set_table_width'):\n",
    "        pl.Config.set_table_width(120)\n",
    "except AttributeError:\n",
    "    # Gracefully handle missing config options in older versions\n",
    "    pass\n",
    "\n",
    "print(f\"Polars version: {pl.__version__}\")\n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading Configuration\n",
    "\n",
    "Set up file paths and configuration for data loading. With 128GB RAM, we can process the full dataset efficiently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loading Configuration:\n",
      "• Data directory: ../data\n",
      "• Using full dataset: True\n",
      "• Sample fraction: 100%\n",
      "• Random seed: 42\n"
     ]
    }
   ],
   "source": [
    "# Configuration parameters\n",
    "data_dir = '../data'\n",
    "use_full_dataset = True  # With 128GB RAM, we can handle the full dataset\n",
    "sample_fraction = 0.1 if not use_full_dataset else 1.0\n",
    "random_seed = 42\n",
    "\n",
    "# Set up file paths\n",
    "transactions_path = os.path.join(data_dir, 'raw', 'transactions_train.csv')\n",
    "customers_path = os.path.join(data_dir, 'raw', 'customers.csv')\n",
    "articles_path = os.path.join(data_dir, 'raw', 'articles.csv')\n",
    "\n",
    "print(\"Data Loading Configuration:\")\n",
    "print(f\"• Data directory: {data_dir}\")\n",
    "print(f\"• Using full dataset: {use_full_dataset}\")\n",
    "print(f\"• Sample fraction: {sample_fraction*100:.0f}%\")\n",
    "print(f\"• Random seed: {random_seed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Transactions Data\n",
    "\n",
    "Load transaction data using Polars' optimised CSV reader. Polars automatically infers schema and optimises memory usage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loading Configuration:\n",
      "• Data directory: ../data\n",
      "• Using full dataset: False\n",
      "• Sample fraction: 10%\n",
      "• Random seed: 42\n",
      "\n",
      "Path verification:\n",
      "• Transactions file exists: True\n",
      "• Customers file exists: True\n",
      "• Articles file exists: True\n"
     ]
    }
   ],
   "source": [
    "# Configuration parameters\n",
    "data_dir = '../data'  # Corrected path - use relative path from notebooks directory\n",
    "use_full_dataset = False  # Start with sample for testing, change to True for full dataset\n",
    "sample_fraction = 0.1 if not use_full_dataset else 1.0\n",
    "random_seed = 42\n",
    "\n",
    "# Set up file paths\n",
    "transactions_path = os.path.join(data_dir, 'raw', 'transactions_train.csv')\n",
    "customers_path = os.path.join(data_dir, 'raw', 'customers.csv')\n",
    "articles_path = os.path.join(data_dir, 'raw', 'articles.csv')\n",
    "\n",
    "print(\"Data Loading Configuration:\")\n",
    "print(f\"• Data directory: {data_dir}\")\n",
    "print(f\"• Using full dataset: {use_full_dataset}\")\n",
    "print(f\"• Sample fraction: {sample_fraction*100:.0f}%\")\n",
    "print(f\"• Random seed: {random_seed}\")\n",
    "\n",
    "# Verify paths exist\n",
    "print(f\"\\nPath verification:\")\n",
    "print(f\"• Transactions file exists: {os.path.exists(transactions_path)}\")\n",
    "print(f\"• Customers file exists: {os.path.exists(customers_path)}\")\n",
    "print(f\"• Articles file exists: {os.path.exists(articles_path)}\")\n",
    "\n",
    "if not all(os.path.exists(p) for p in [transactions_path, customers_path, articles_path]):\n",
    "    print(f\"\\nTroubleshooting:\")\n",
    "    print(f\"• Current working directory: {os.getcwd()}\")\n",
    "    print(f\"• Data directory contents: {os.listdir(data_dir) if os.path.exists(data_dir) else 'Not found'}\")\n",
    "    if os.path.exists(os.path.join(data_dir, 'raw')):\n",
    "        print(f\"• Raw directory contents: {os.listdir(os.path.join(data_dir, 'raw'))[:10]}\")  # Show first 10 files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Customer Data\n",
    "\n",
    "Load customer demographic and preference data using Polars' efficient processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "• Loading customer data...\n",
      "  ✓ Customers: 1,371,980 records loaded in 0.23 seconds\n",
      "  ✓ Memory usage: 215.0 MB\n",
      "  - Schema:\n",
      "OrderedDict([('customer_id', String), ('FN', Float64), ('Active', Float64), ('club_member_status', String), ('fashion_news_frequency', String), ('age', Int64), ('postal_code', String)])\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n• Loading customer data...\")\n",
    "start_time = time.time()\n",
    "\n",
    "if not os.path.exists(customers_path):\n",
    "    raise FileNotFoundError(f\"Customers file not found: {customers_path}\")\n",
    "\n",
    "df_customers = pl.read_csv(customers_path)\n",
    "customer_count = df_customers.height\n",
    "load_time = time.time() - start_time\n",
    "\n",
    "print(f\"  ✓ Customers: {customer_count:,} records loaded in {load_time:.2f} seconds\")\n",
    "print(f\"  ✓ Memory usage: {df_customers.estimated_size('mb'):.1f} MB\")\n",
    "print(\"  - Schema:\")\n",
    "print(df_customers.schema)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "• Loading transactions data with Polars...\n",
      "  Loading 10% sample...\n",
      "  ✓ Transactions: 3,178,832 records loaded in 22.81 seconds\n",
      "  ✓ Memory usage: 491.1 MB\n",
      "  - Schema:\n",
      "OrderedDict([('t_dat', String), ('customer_id', String), ('article_id', Int64), ('price', Float64), ('sales_channel_id', Int64)])\n",
      "\n",
      "First 3 rows:\n",
      "shape: (3, 5)\n",
      "┌────────────┬─────────────────────────────────┬────────────┬──────────┬──────────────────┐\n",
      "│ t_dat      ┆ customer_id                     ┆ article_id ┆ price    ┆ sales_channel_id │\n",
      "│ ---        ┆ ---                             ┆ ---        ┆ ---      ┆ ---              │\n",
      "│ str        ┆ str                             ┆ i64        ┆ f64      ┆ i64              │\n",
      "╞════════════╪═════════════════════════════════╪════════════╪══════════╪══════════════════╡\n",
      "│ 2019-05-02 ┆ 1e6db9e9e42595b9bb7f2076d7dd11… ┆ 600886001  ┆ 0.023949 ┆ 1                │\n",
      "│ 2019-04-26 ┆ d72800d8455ca7f23189c39daaee30… ┆ 594161006  ┆ 0.027102 ┆ 2                │\n",
      "│ 2020-03-07 ┆ 900fe1bfbca0a18fab380fb96896c6… ┆ 721266004  ┆ 0.038119 ┆ 2                │\n",
      "└────────────┴─────────────────────────────────┴────────────┴──────────┴──────────────────┘\n"
     ]
    }
   ],
   "source": [
    "print(\"• Loading transactions data with Polars...\")\n",
    "start_time = time.time()\n",
    "\n",
    "if not os.path.exists(transactions_path):\n",
    "    raise FileNotFoundError(f\"Transaction file not found: {transactions_path}\")\n",
    "\n",
    "try:\n",
    "    # Load transactions data efficiently with Polars\n",
    "    if use_full_dataset:\n",
    "        print(\"  Loading full dataset (this may take a moment for large files)...\")\n",
    "        df_transactions = pl.read_csv(transactions_path)\n",
    "    else:\n",
    "        print(f\"  Loading {sample_fraction*100:.0f}% sample...\")\n",
    "        # For compatibility with different Polars versions, read then sample\n",
    "        df_transactions = pl.read_csv(transactions_path).sample(fraction=sample_fraction, seed=random_seed)\n",
    "\n",
    "    transaction_count = df_transactions.height\n",
    "    load_time = time.time() - start_time\n",
    "\n",
    "    print(f\"  ✓ Transactions: {transaction_count:,} records loaded in {load_time:.2f} seconds\")\n",
    "    print(f\"  ✓ Memory usage: {df_transactions.estimated_size('mb'):.1f} MB\")\n",
    "    print(\"  - Schema:\")\n",
    "    print(df_transactions.schema)\n",
    "    \n",
    "    # Show a quick sample\n",
    "    print(f\"\\nFirst 3 rows:\")\n",
    "    print(df_transactions.head(3))\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading transactions data: {e}\")\n",
    "    print(\"This might be due to:\")\n",
    "    print(\"1. Large file size - try setting use_full_dataset = False\")\n",
    "    print(\"2. Memory constraints - reduce sample_fraction\")\n",
    "    print(\"3. File corruption - check the CSV file\")\n",
    "    \n",
    "    # If memory is an issue, try with smaller sample\n",
    "    print(\"\\nTrying with smaller sample (1%)...\")\n",
    "    try:\n",
    "        df_transactions = pl.read_csv(transactions_path).sample(fraction=0.01, seed=random_seed)\n",
    "        transaction_count = df_transactions.height\n",
    "        load_time = time.time() - start_time\n",
    "        print(f\"  ✓ Emergency sample: {transaction_count:,} records loaded\")\n",
    "        print(\"  Note: Using 1% sample due to memory constraints\")\n",
    "    except Exception as e2:\n",
    "        print(f\"  Failed with smaller sample too: {e2}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load Articles Data\n",
    "\n",
    "Load product/article information and metadata with detailed product categorisation including product types, groups, colours, and other attributes.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "• Loading articles data...\n",
      "  ✓ Articles: 105,542 records loaded in 0.07 seconds\n",
      "  ✓ Memory usage: 36.4 MB\n",
      "  - Schema:\n",
      "OrderedDict([('article_id', Int64), ('product_code', Int64), ('prod_name', String), ('product_type_no', Int64), ('product_type_name', String), ('product_group_name', String), ('graphical_appearance_no', Int64), ('graphical_appearance_name', String), ('colour_group_code', Int64), ('colour_group_name', String), ('perceived_colour_value_id', Int64), ('perceived_colour_value_name', String), ('perceived_colour_master_id', Int64), ('perceived_colour_master_name', String), ('department_no', Int64), ('department_name', String), ('index_code', String), ('index_name', String), ('index_group_no', Int64), ('index_group_name', String), ('section_no', Int64), ('section_name', String), ('garment_group_no', Int64), ('garment_group_name', String), ('detail_desc', String)])\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n• Loading articles data...\")\n",
    "start_time = time.time()\n",
    "\n",
    "if not os.path.exists(articles_path):\n",
    "    raise FileNotFoundError(f\"Articles file not found: {articles_path}\")\n",
    "\n",
    "df_articles = pl.read_csv(articles_path)\n",
    "article_count = df_articles.height\n",
    "load_time = time.time() - start_time\n",
    "\n",
    "print(f\"  ✓ Articles: {article_count:,} records loaded in {load_time:.2f} seconds\")\n",
    "print(f\"  ✓ Memory usage: {df_articles.estimated_size('mb'):.1f} MB\")\n",
    "print(\"  - Schema:\")\n",
    "print(df_articles.schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Dataset Integration\n",
    "\n",
    "Integrate transaction, customer, and article datasets using Polars' optimised join operations. This creates a unified view of customer interactions with products for comprehensive analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating integrated customer interaction dataset...\n",
      "✓ Integrated dataset created with 3,178,832 transaction records\n",
      "✓ Integration completed in 1.27 seconds\n",
      "✓ Total memory usage: 1828.6 MB\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating integrated customer interaction dataset...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Perform left joins to preserve all transaction records - Polars optimises joins automatically\n",
    "df_integrated = (\n",
    "    df_transactions\n",
    "    .join(df_customers, on=\"customer_id\", how=\"left\")\n",
    "    .join(df_articles, on=\"article_id\", how=\"left\")\n",
    ")\n",
    "\n",
    "# Calculate dataset statistics efficiently\n",
    "integration_stats = {\n",
    "    'total_records': df_integrated.height,\n",
    "    'unique_customers': df_integrated['customer_id'].n_unique(),\n",
    "    'unique_articles': df_integrated['article_id'].n_unique()\n",
    "}\n",
    "\n",
    "integration_time = time.time() - start_time\n",
    "\n",
    "print(f\"✓ Integrated dataset created with {integration_stats['total_records']:,} transaction records\")\n",
    "print(f\"✓ Integration completed in {integration_time:.2f} seconds\")\n",
    "print(f\"✓ Total memory usage: {df_integrated.estimated_size('mb'):.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Sample Data Preview\n",
    "\n",
    "Display a sample of the integrated dataset to understand the structure and verify successful integration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of integrated dataset:\n",
      "shape: (10, 7)\n",
      "┌─────────────────┬────────────┬──────────┬────────────────┬─────┬────────────────┬────────────────┐\n",
      "│ customer_id     ┆ article_id ┆ price    ┆ sales_channel_ ┆ age ┆ club_member_st ┆ product_type_n │\n",
      "│ ---             ┆ ---        ┆ ---      ┆ id             ┆ --- ┆ atus           ┆ ame            │\n",
      "│ str             ┆ i64        ┆ f64      ┆ ---            ┆ i64 ┆ ---            ┆ ---            │\n",
      "│                 ┆            ┆          ┆ i64            ┆     ┆ str            ┆ str            │\n",
      "╞═════════════════╪════════════╪══════════╪════════════════╪═════╪════════════════╪════════════════╡\n",
      "│ 1e6db9e9e42595b ┆ 600886001  ┆ 0.023949 ┆ 1              ┆ 45  ┆ ACTIVE         ┆ Swimwear       │\n",
      "│ 9bb7f2076d7dd11 ┆            ┆          ┆                ┆     ┆                ┆ bottom         │\n",
      "│ …               ┆            ┆          ┆                ┆     ┆                ┆                │\n",
      "│ d72800d8455ca7f ┆ 594161006  ┆ 0.027102 ┆ 2              ┆ 49  ┆ ACTIVE         ┆ Sandals        │\n",
      "│ 23189c39daaee30 ┆            ┆          ┆                ┆     ┆                ┆                │\n",
      "│ …               ┆            ┆          ┆                ┆     ┆                ┆                │\n",
      "│ 900fe1bfbca0a18 ┆ 721266004  ┆ 0.038119 ┆ 2              ┆ 40  ┆ ACTIVE         ┆ Hoodie         │\n",
      "│ fab380fb96896c6 ┆            ┆          ┆                ┆     ┆                ┆                │\n",
      "│ …               ┆            ┆          ┆                ┆     ┆                ┆                │\n",
      "│ 569db89380ddfd2 ┆ 749098022  ┆ 0.013542 ┆ 2              ┆ 23  ┆ ACTIVE         ┆ Sweater        │\n",
      "│ 80e66195dafdccd ┆            ┆          ┆                ┆     ┆                ┆                │\n",
      "│ …               ┆            ┆          ┆                ┆     ┆                ┆                │\n",
      "│ 51805853a3c5c51 ┆ 488012002  ┆ 0.01439  ┆ 1              ┆ 48  ┆ ACTIVE         ┆ Socks          │\n",
      "│ 56087534b287af3 ┆            ┆          ┆                ┆     ┆                ┆                │\n",
      "│ …               ┆            ┆          ┆                ┆     ┆                ┆                │\n",
      "│ be80fe3d2ce8ed8 ┆ 560270007  ┆ 0.050831 ┆ 1              ┆ 46  ┆ ACTIVE         ┆ Hoodie         │\n",
      "│ c020b2bf5f684df ┆            ┆          ┆                ┆     ┆                ┆                │\n",
      "│ …               ┆            ┆          ┆                ┆     ┆                ┆                │\n",
      "│ fcf15559cadaee7 ┆ 642105002  ┆ 0.011847 ┆ 2              ┆ 53  ┆ ACTIVE         ┆ Blouse         │\n",
      "│ 43849cfa7dd1652 ┆            ┆          ┆                ┆     ┆                ┆                │\n",
      "│ …               ┆            ┆          ┆                ┆     ┆                ┆                │\n",
      "│ 524cf19c299dcea ┆ 554450014  ┆ 0.021339 ┆ 2              ┆ 35  ┆ ACTIVE         ┆ Trousers       │\n",
      "│ 677a37a0aa343e6 ┆            ┆          ┆                ┆     ┆                ┆                │\n",
      "│ …               ┆            ┆          ┆                ┆     ┆                ┆                │\n",
      "│ 63f2d0db5658bda ┆ 690933005  ┆ 0.033881 ┆ 2              ┆ 28  ┆ ACTIVE         ┆ Trousers       │\n",
      "│ 70f552280637dd4 ┆            ┆          ┆                ┆     ┆                ┆                │\n",
      "│ …               ┆            ┆          ┆                ┆     ┆                ┆                │\n",
      "│ d5e90225ef3e994 ┆ 160442010  ┆ 0.013542 ┆ 2              ┆ 45  ┆ ACTIVE         ┆ Socks          │\n",
      "│ 299fb5f20f42d40 ┆            ┆          ┆                ┆     ┆                ┆                │\n",
      "│ …               ┆            ┆          ┆                ┆     ┆                ┆                │\n",
      "└─────────────────┴────────────┴──────────┴────────────────┴─────┴────────────────┴────────────────┘\n"
     ]
    }
   ],
   "source": [
    "# Show sample of integrated data\n",
    "print(\"Sample of integrated dataset:\")\n",
    "sample_cols = [\"customer_id\", \"article_id\", \"price\", \"sales_channel_id\", \"age\", \"club_member_status\", \"product_type_name\"]\n",
    "available_cols = [col for col in sample_cols if col in df_integrated.columns]\n",
    "\n",
    "display_df = df_integrated.select(available_cols).head(10)\n",
    "print(display_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Dataset Structure Analysis\n",
    "\n",
    "Analyse the integrated dataset structure including customer and product diversity, date ranges, and feature categorisation for the H&M dataset.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Structure Analysis:\n",
      "• Total unique customers: 821,920\n",
      "• Total unique articles: 87,121\n",
      "• Date range: 2018-09-20 to 2020-09-22\n",
      "\n",
      "Dataset Features by Category:\n",
      "  Customer Demographics: customer_id, age, club_member_status, fashion_news_frequency\n",
      "  Transaction Behaviour: t_dat, price, sales_channel_id\n",
      "  Product Information: article_id, product_type_name, product_group_name, colour_group_name\n",
      "  Customer Preferences: garment_group_name, index_name, section_name\n"
     ]
    }
   ],
   "source": [
    "print(f\"Dataset Structure Analysis:\")\n",
    "print(f\"• Total unique customers: {integration_stats['unique_customers']:,}\")\n",
    "print(f\"• Total unique articles: {integration_stats['unique_articles']:,}\")\n",
    "\n",
    "# Check if date column exists and show range\n",
    "if 't_dat' in df_integrated.columns:\n",
    "    date_stats = df_integrated.select([\n",
    "        pl.col('t_dat').min().alias('min_date'),\n",
    "        pl.col('t_dat').max().alias('max_date')\n",
    "    ]).to_pandas().iloc[0]\n",
    "    print(f\"• Date range: {date_stats['min_date']} to {date_stats['max_date']}\")\n",
    "\n",
    "# Create feature categories for H&M dataset\n",
    "all_columns = df_integrated.columns\n",
    "feature_categories = {\n",
    "    'Customer Demographics': [col for col in ['customer_id', 'age', 'club_member_status', 'fashion_news_frequency'] if col in all_columns],\n",
    "    'Transaction Behaviour': [col for col in ['t_dat', 'price', 'sales_channel_id'] if col in all_columns],\n",
    "    'Product Information': [col for col in ['article_id', 'product_type_name', 'product_group_name', 'colour_group_name'] if col in all_columns],\n",
    "    'Customer Preferences': [col for col in ['garment_group_name', 'index_name', 'section_name'] if col in all_columns]\n",
    "}\n",
    "\n",
    "print(f\"\\nDataset Features by Category:\")\n",
    "for category, features in feature_categories.items():\n",
    "    if features:  # Only show categories with available features\n",
    "        print(f\"  {category}: {', '.join(features)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Data Quality Assessment\n",
    "\n",
    "Perform comprehensive data quality assessment using Polars' efficient operations to identify missing values, duplicates, and statistical patterns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysing existing data quality issues in H&M dataset...\n",
      "\n",
      "• Missing Values Analysis:\n",
      "  FN: 1,821,007 (57.29%)\n",
      "  Active: 1,841,453 (57.93%)\n",
      "  club_member_status: 6,179 (0.19%)\n",
      "  fashion_news_frequency: 14,216 (0.45%)\n",
      "  age: 14,079 (0.44%)\n",
      "  detail_desc: 11,480 (0.36%)\n"
     ]
    }
   ],
   "source": [
    "print(\"Analysing existing data quality issues in H&M dataset...\")\n",
    "\n",
    "# Check for missing values efficiently with Polars\n",
    "print(f\"\\n• Missing Values Analysis:\")\n",
    "\n",
    "# Get null counts for all columns in one operation\n",
    "null_counts = df_integrated.null_count()\n",
    "total_records = df_integrated.height\n",
    "\n",
    "missing_summary = []\n",
    "for col_name in df_integrated.columns:\n",
    "    missing_count = null_counts[col_name][0]\n",
    "    if missing_count > 0:\n",
    "        missing_percentage = (missing_count / total_records) * 100\n",
    "        missing_summary.append((col_name, missing_count, missing_percentage))\n",
    "        print(f\"  {col_name}: {missing_count:,} ({missing_percentage:.2f}%)\")\n",
    "\n",
    "if not missing_summary:\n",
    "    print(\"  ✓ No missing values found in dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Duplicate Analysis\n",
    "\n",
    "Analyse the dataset for duplicate records to understand data integrity using Polars' efficient duplicate detection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "• Duplicate Analysis:\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Total records: 3,178,832\n",
      "  Unique records: 3,141,398\n",
      "  Duplicate records: 37,434\n"
     ]
    }
   ],
   "source": [
    "# Analyse duplicates efficiently\n",
    "print(f\"\\n• Duplicate Analysis:\")\n",
    "total_records = df_integrated.height\n",
    "unique_records = df_integrated.unique().height\n",
    "duplicate_count = total_records - unique_records\n",
    "\n",
    "print(f\"  Total records: {total_records:,}\")\n",
    "print(f\"  Unique records: {unique_records:,}\")\n",
    "print(f\"  Duplicate records: {duplicate_count:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Price Distribution Analysis\n",
    "\n",
    "Analyse price distribution to identify potential outliers and understand the pricing structure of H&M products using Polars' statistical functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "• Price Distribution Analysis:\n",
      "  count: 3178832.00\n",
      "  mean: 0.03\n",
      "  std: 0.02\n",
      "  min: 0.00\n",
      "  25%: 0.02\n",
      "  50%: 0.03\n",
      "  75%: 0.03\n",
      "  max: 0.51\n"
     ]
    }
   ],
   "source": [
    "# Check for potential outliers in price data\n",
    "if 'price' in df_integrated.columns:\n",
    "    print(f\"\\n• Price Distribution Analysis:\")\n",
    "    \n",
    "    # Get comprehensive price statistics in one operation\n",
    "    price_stats = df_integrated.select([\n",
    "        pl.col('price').count().alias('count'),\n",
    "        pl.col('price').mean().alias('mean'),\n",
    "        pl.col('price').std().alias('std'),\n",
    "        pl.col('price').min().alias('min'),\n",
    "        pl.col('price').quantile(0.25).alias('25%'),\n",
    "        pl.col('price').quantile(0.5).alias('50%'),\n",
    "        pl.col('price').quantile(0.75).alias('75%'),\n",
    "        pl.col('price').max().alias('max')\n",
    "    ]).to_pandas().iloc[0]\n",
    "    \n",
    "    for stat_name, value in price_stats.items():\n",
    "        print(f\"  {stat_name}: {value:.2f}\")\n",
    "else:\n",
    "    print(f\"\\n• No price column found in dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Performance Summary\n",
    "\n",
    "Display processing performance metrics of Polars' efficiency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Data Exploration Complete\n",
      "  - Transaction records: 3,178,832\n",
      "  - Integrated records: 3,178,832\n",
      "  - Unique customers: 821,920\n",
      "  - Unique articles: 87,121\n",
      "  - Total memory usage: 2571.0 MB\n",
      "  - Processing framework: Polars 0.20.31\n",
      "\n",
      "============================================================\n",
      "DATA EXPLORATION COMPLETED SUCCESSFULLY\n",
      "============================================================\n",
      "Integrated dataset available as 'df_integrated'\n",
      "Quality report available as 'quality_report'\n"
     ]
    }
   ],
   "source": [
    "# Compile quality assessment results\n",
    "quality_report = {\n",
    "    'missing_summary': missing_summary,\n",
    "    'duplicate_count': duplicate_count,\n",
    "    'total_records': total_records\n",
    "}\n",
    "\n",
    "# Calculate total memory usage\n",
    "total_memory_mb = (\n",
    "    df_transactions.estimated_size('mb') + \n",
    "    df_customers.estimated_size('mb') + \n",
    "    df_articles.estimated_size('mb') + \n",
    "    df_integrated.estimated_size('mb')\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Data Exploration Complete\")\n",
    "print(f\"  - Transaction records: {transaction_count:,}\")\n",
    "print(f\"  - Integrated records: {integration_stats['total_records']:,}\")\n",
    "print(f\"  - Unique customers: {integration_stats['unique_customers']:,}\")\n",
    "print(f\"  - Unique articles: {integration_stats['unique_articles']:,}\")\n",
    "print(f\"  - Total memory usage: {total_memory_mb:.1f} MB\")\n",
    "print(f\"  - Processing framework: Polars {pl.__version__}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DATA EXPLORATION COMPLETED SUCCESSFULLY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Integrated dataset available as 'df_integrated'\")\n",
    "print(f\"Quality report available as 'quality_report'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 14. Save Results\n",
    "\n",
    "Save the integrated dataset and results for use in preprocessing and further analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Integrated dataset saved as Parquet file\n"
     ]
    }
   ],
   "source": [
    "# Save integrated dataset for preprocessing\n",
    "output_dir = os.path.join(data_dir, 'processed')\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save as Parquet for efficient storage\n",
    "df_integrated.write_parquet(os.path.join(output_dir, 'hm_integrated_dataset.parquet'))\n",
    "print(f\"✓ Integrated dataset saved as Parquet file\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
