{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# H&M Personalised Fashion Recommendations Data Understanding\n",
    "\n",
    "This notebook provides an exploratory analysis of the H&M dataset structure and quality using Polars for high-performance data processing. The analysis includes data loading, integration, and quality assessment across transactions, customers, and articles datasets.\n",
    "\n",
    "## Dataset Description\n",
    "\n",
    "**Challenge notes from Kaggle**: For this challenge you are given the purchase history of customers across time, along with supporting metadata. Your challenge is to predict what articles each customer will purchase in the 7-day period immediately after the training data ends. Customer who did not make any purchase during that time are excluded from the scoring.\n",
    "\n",
    "### Files\n",
    "\n",
    "- **images/** - a folder of images corresponding to each article_id; images are placed in subfolders starting with the first three digits of the article_id; note, not all article_id values have a corresponding image.\n",
    "- **articles.csv** - detailed metadata for each article_id available for purchase\n",
    "- **customers.csv** - metadata for each customer_id in dataset\n",
    "- **sample_submission.csv** - a sample submission file in the correct format\n",
    "- **transactions_train.csv** - the training data, consisting of the purchases each customer for each date, as well as additional information. Duplicate rows correspond to multiple purchases of the same item. Your task is to predict the article_ids each customer will purchase during the 7-day period immediately after the training data period.\n",
    "\n",
    "[H&M Personalized Fashion Recommendations Kaggle Competition](https://www.kaggle.com/competitions/h-and-m-personalized-fashion-recommendations)\n",
    "\n",
    "## Overview\n",
    "\n",
    "- **Transaction Data**: Customer purchase history with temporal patterns\n",
    "- **Customer Data**: Demographic and preference information\n",
    "- **Articles Data**: Product metadata and categorisation\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Setup\n",
    "\n",
    "Import Polars and other necessary libraries for data processing, analysis, and visualisation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Polars version: 1.32.0\n",
      "Libraries imported successfully\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import time\n",
    "from typing import Tuple, List, Dict, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(f\"Polars version: {pl.__version__}\")\n",
    "print(\"Libraries imported successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading Configuration\n",
    "\n",
    "Set up file paths and configuration for data loading.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loading Configuration:\n",
      "• Data directory: ../data\n",
      "• Using full dataset: False\n",
      "• Sample fraction: 10%\n",
      "• Random seed: 42\n"
     ]
    }
   ],
   "source": [
    "# Configuration parameters\n",
    "data_dir = '../data'\n",
    "use_full_dataset = False\n",
    "sample_fraction = 0.1 if not use_full_dataset else 1.0\n",
    "random_seed = 42\n",
    "\n",
    "# Set up file paths\n",
    "transactions_path = os.path.join(data_dir, 'raw', 'transactions_train.csv')\n",
    "customers_path = os.path.join(data_dir, 'raw', 'customers.csv')\n",
    "articles_path = os.path.join(data_dir, 'raw', 'articles.csv')\n",
    "\n",
    "print(\"Data Loading Configuration:\")\n",
    "print(f\"• Data directory: {data_dir}\")\n",
    "print(f\"• Using full dataset: {use_full_dataset}\")\n",
    "print(f\"• Sample fraction: {sample_fraction*100:.0f}%\")\n",
    "print(f\"• Random seed: {random_seed}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Check Dataset Existance\n",
    "\n",
    "Check that the datasets exist\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data Loading Configuration:\n",
      "• Data directory: ../data\n",
      "• Using full dataset: False\n",
      "• Sample fraction: 10%\n",
      "• Random seed: 42\n",
      "\n",
      "Path verification:\n",
      "• Transactions file exists: True\n",
      "• Customers file exists: True\n",
      "• Articles file exists: True\n"
     ]
    }
   ],
   "source": [
    "# Configuration parameters\n",
    "data_dir = '../data'\n",
    "use_full_dataset = False\n",
    "sample_fraction = 0.1 if not use_full_dataset else 1.0\n",
    "random_seed = 42\n",
    "\n",
    "# Set up file paths\n",
    "transactions_path = os.path.join(data_dir, 'raw', 'transactions_train.csv')\n",
    "customers_path = os.path.join(data_dir, 'raw', 'customers.csv')\n",
    "articles_path = os.path.join(data_dir, 'raw', 'articles.csv')\n",
    "\n",
    "print(\"Data Loading Configuration:\")\n",
    "print(f\"• Data directory: {data_dir}\")\n",
    "print(f\"• Using full dataset: {use_full_dataset}\")\n",
    "print(f\"• Sample fraction: {sample_fraction*100:.0f}%\")\n",
    "print(f\"• Random seed: {random_seed}\")\n",
    "\n",
    "# Verify paths exist\n",
    "print(f\"\\nPath verification:\")\n",
    "print(f\"• Transactions file exists: {os.path.exists(transactions_path)}\")\n",
    "print(f\"• Customers file exists: {os.path.exists(customers_path)}\")\n",
    "print(f\"• Articles file exists: {os.path.exists(articles_path)}\")\n",
    "\n",
    "if not all(os.path.exists(p) for p in [transactions_path, customers_path, articles_path]):\n",
    "    print(f\"\\nTroubleshooting:\")\n",
    "    print(f\"• Current working directory: {os.getcwd()}\")\n",
    "    print(f\"• Data directory contents: {os.listdir(data_dir) if os.path.exists(data_dir) else 'Not found'}\")\n",
    "    if os.path.exists(os.path.join(data_dir, 'raw')):\n",
    "        print(f\"• Raw directory contents: {os.listdir(os.path.join(data_dir, 'raw'))[:10]}\")  # Show first 10 files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Load Customer Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "• Loading customer data...\n",
      "  ✓ Customers: 1,371,980 records loaded in 0.46 seconds\n",
      "  ✓ Memory usage: 215.0 MB\n",
      "  - Schema:\n",
      "Schema([('customer_id', String), ('FN', Float64), ('Active', Float64), ('club_member_status', String), ('fashion_news_frequency', String), ('age', Int64), ('postal_code', String)])\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n• Loading customer data...\")\n",
    "start_time = time.time()\n",
    "\n",
    "if not os.path.exists(customers_path):\n",
    "    raise FileNotFoundError(f\"Customers file not found: {customers_path}\")\n",
    "\n",
    "df_customers = pl.read_csv(customers_path)\n",
    "customer_count = df_customers.height\n",
    "load_time = time.time() - start_time\n",
    "\n",
    "print(f\"  ✓ Customers: {customer_count:,} records loaded in {load_time:.2f} seconds\")\n",
    "print(f\"  ✓ Memory usage: {df_customers.estimated_size('mb'):.1f} MB\")\n",
    "print(\"  - Schema:\")\n",
    "print(df_customers.schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Load Transactions Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "• Loading transactions data with Polars...\n",
      "  Loading 10% sample...\n",
      "  ✓ Transactions: 3,178,832 records loaded in 10.40 seconds\n",
      "  ✓ Memory usage: 297.1 MB\n",
      "  - Schema:\n",
      "Schema([('t_dat', String), ('customer_id', String), ('article_id', Int64), ('price', Float64), ('sales_channel_id', Int64)])\n",
      "\n",
      "First 5 rows:\n",
      "shape: (5, 5)\n",
      "┌────────────┬─────────────────────────────────┬────────────┬──────────┬──────────────────┐\n",
      "│ t_dat      ┆ customer_id                     ┆ article_id ┆ price    ┆ sales_channel_id │\n",
      "│ ---        ┆ ---                             ┆ ---        ┆ ---      ┆ ---              │\n",
      "│ str        ┆ str                             ┆ i64        ┆ f64      ┆ i64              │\n",
      "╞════════════╪═════════════════════════════════╪════════════╪══════════╪══════════════════╡\n",
      "│ 2020-05-18 ┆ 242e34dcafc554396c79c976e08459… ┆ 874916005  ┆ 0.014322 ┆ 2                │\n",
      "│ 2019-05-12 ┆ 14de0945a699eb27c5b24c69077a1f… ┆ 751260002  ┆ 0.016932 ┆ 2                │\n",
      "│ 2020-09-08 ┆ 8d7b5cd5a125ed04d08acbd977f320… ┆ 714790024  ┆ 0.050831 ┆ 2                │\n",
      "│ 2020-02-12 ┆ b30b65fae12005cdae2ecd6f7dab75… ┆ 767834001  ┆ 0.008458 ┆ 2                │\n",
      "│ 2020-04-29 ┆ a2944495b894d4bb505b8ede9a27d5… ┆ 766495009  ┆ 0.045746 ┆ 2                │\n",
      "└────────────┴─────────────────────────────────┴────────────┴──────────┴──────────────────┘\n"
     ]
    }
   ],
   "source": [
    "print(\"• Loading transactions data with Polars...\")\n",
    "start_time = time.time()\n",
    "\n",
    "if not os.path.exists(transactions_path):\n",
    "    raise FileNotFoundError(f\"Transaction file not found: {transactions_path}\")\n",
    "\n",
    "try:\n",
    "    # Load transactions data with Polars\n",
    "    if use_full_dataset:\n",
    "        print(\"  Loading full dataset (this may take a moment for large files)...\")\n",
    "        df_transactions = pl.read_csv(transactions_path)\n",
    "    else:\n",
    "        print(f\"  Loading {sample_fraction*100:.0f}% sample...\")\n",
    "        # For compatibility with different Polars versions, read then sample\n",
    "        df_transactions = pl.read_csv(transactions_path).sample(fraction=sample_fraction, seed=random_seed)\n",
    "\n",
    "    transaction_count = df_transactions.height\n",
    "    load_time = time.time() - start_time\n",
    "\n",
    "    print(f\"  ✓ Transactions: {transaction_count:,} records loaded in {load_time:.2f} seconds\")\n",
    "    print(f\"  ✓ Memory usage: {df_transactions.estimated_size('mb'):.1f} MB\")\n",
    "    print(\"  - Schema:\")\n",
    "    print(df_transactions.schema)\n",
    "    \n",
    "    # Show a sample\n",
    "    print(f\"\\nFirst 5 rows:\")\n",
    "    print(df_transactions.head(5))\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading transactions data: {e}\")\n",
    "    print(\"This might be due to:\")\n",
    "    print(\"1. Large file size - try setting use_full_dataset = False\")\n",
    "    print(\"2. Memory constraints - reduce sample_fraction\")\n",
    "    print(\"3. File corruption - check the CSV file\")\n",
    "    \n",
    "    # If memory is an issue, try with smaller sample\n",
    "    print(\"\\nTrying with smaller sample (1%)...\")\n",
    "    try:\n",
    "        df_transactions = pl.read_csv(transactions_path).sample(fraction=0.01, seed=random_seed)\n",
    "        transaction_count = df_transactions.height\n",
    "        load_time = time.time() - start_time\n",
    "        print(f\"  ✓ Emergency sample: {transaction_count:,} records loaded\")\n",
    "        print(\"  Note: Using 1% sample due to memory constraints\")\n",
    "    except Exception as e2:\n",
    "        print(f\"  Failed with smaller sample too: {e2}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Load Articles Data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "• Loading articles data...\n",
      "  ✓ Articles: 105,542 records loaded in 0.06 seconds\n",
      "  ✓ Memory usage: 36.4 MB\n",
      "  - Schema:\n",
      "Schema([('article_id', Int64), ('product_code', Int64), ('prod_name', String), ('product_type_no', Int64), ('product_type_name', String), ('product_group_name', String), ('graphical_appearance_no', Int64), ('graphical_appearance_name', String), ('colour_group_code', Int64), ('colour_group_name', String), ('perceived_colour_value_id', Int64), ('perceived_colour_value_name', String), ('perceived_colour_master_id', Int64), ('perceived_colour_master_name', String), ('department_no', Int64), ('department_name', String), ('index_code', String), ('index_name', String), ('index_group_no', Int64), ('index_group_name', String), ('section_no', Int64), ('section_name', String), ('garment_group_no', Int64), ('garment_group_name', String), ('detail_desc', String)])\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n• Loading articles data...\")\n",
    "start_time = time.time()\n",
    "\n",
    "if not os.path.exists(articles_path):\n",
    "    raise FileNotFoundError(f\"Articles file not found: {articles_path}\")\n",
    "\n",
    "df_articles = pl.read_csv(articles_path)\n",
    "article_count = df_articles.height\n",
    "load_time = time.time() - start_time\n",
    "\n",
    "print(f\"  ✓ Articles: {article_count:,} records loaded in {load_time:.2f} seconds\")\n",
    "print(f\"  ✓ Memory usage: {df_articles.estimated_size('mb'):.1f} MB\")\n",
    "print(\"  - Schema:\")\n",
    "print(df_articles.schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Dataset Integration\n",
    "\n",
    "Integrate transaction, customer, and article datasets using Polars join operations. Please note that this denormalises the data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating integrated customer interaction dataset...\n",
      "✓ Integrated dataset created with 3,178,832 transaction records\n",
      "✓ Integration completed in 1.03 seconds\n",
      "✓ Total memory usage: 1634.7 MB\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating integrated customer interaction dataset...\")\n",
    "start_time = time.time()\n",
    "\n",
    "# Perform left joins to preserve all transaction records\n",
    "df_integrated = (\n",
    "    df_transactions\n",
    "    .join(df_customers, on=\"customer_id\", how=\"left\")\n",
    "    .join(df_articles, on=\"article_id\", how=\"left\")\n",
    ")\n",
    "\n",
    "# Calculate dataset statistics efficiently\n",
    "integration_stats = {\n",
    "    'total_records': df_integrated.height,\n",
    "    'unique_customers': df_integrated['customer_id'].n_unique(),\n",
    "    'unique_articles': df_integrated['article_id'].n_unique()\n",
    "}\n",
    "\n",
    "integration_time = time.time() - start_time\n",
    "\n",
    "print(f\"✓ Integrated dataset created with {integration_stats['total_records']:,} transaction records\")\n",
    "print(f\"✓ Integration completed in {integration_time:.2f} seconds\")\n",
    "print(f\"✓ Total memory usage: {df_integrated.estimated_size('mb'):.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Integrated Data Preview\n",
    "\n",
    "Display a sample of the integrated dataset to understand the structure and verify successful integration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample of integrated dataset:\n",
      "shape: (10, 7)\n",
      "┌─────────────────┬────────────┬──────────┬────────────────┬─────┬────────────────┬────────────────┐\n",
      "│ customer_id     ┆ article_id ┆ price    ┆ sales_channel_ ┆ age ┆ club_member_st ┆ product_type_n │\n",
      "│ ---             ┆ ---        ┆ ---      ┆ id             ┆ --- ┆ atus           ┆ ame            │\n",
      "│ str             ┆ i64        ┆ f64      ┆ ---            ┆ i64 ┆ ---            ┆ ---            │\n",
      "│                 ┆            ┆          ┆ i64            ┆     ┆ str            ┆ str            │\n",
      "╞═════════════════╪════════════╪══════════╪════════════════╪═════╪════════════════╪════════════════╡\n",
      "│ 242e34dcafc5543 ┆ 874916005  ┆ 0.014322 ┆ 2              ┆ 54  ┆ ACTIVE         ┆ Sunglasses     │\n",
      "│ 96c79c976e08459 ┆            ┆          ┆                ┆     ┆                ┆                │\n",
      "│ …               ┆            ┆          ┆                ┆     ┆                ┆                │\n",
      "│ 14de0945a699eb2 ┆ 751260002  ┆ 0.016932 ┆ 2              ┆ 57  ┆ PRE-CREATE     ┆ Shorts         │\n",
      "│ 7c5b24c69077a1f ┆            ┆          ┆                ┆     ┆                ┆                │\n",
      "│ …               ┆            ┆          ┆                ┆     ┆                ┆                │\n",
      "│ 8d7b5cd5a125ed0 ┆ 714790024  ┆ 0.050831 ┆ 2              ┆ 24  ┆ ACTIVE         ┆ Trousers       │\n",
      "│ 4d08acbd977f320 ┆            ┆          ┆                ┆     ┆                ┆                │\n",
      "│ …               ┆            ┆          ┆                ┆     ┆                ┆                │\n",
      "│ b30b65fae12005c ┆ 767834001  ┆ 0.008458 ┆ 2              ┆ 23  ┆ ACTIVE         ┆ Vest top       │\n",
      "│ dae2ecd6f7dab75 ┆            ┆          ┆                ┆     ┆                ┆                │\n",
      "│ …               ┆            ┆          ┆                ┆     ┆                ┆                │\n",
      "│ a2944495b894d4b ┆ 766495009  ┆ 0.045746 ┆ 2              ┆ 33  ┆ ACTIVE         ┆ Skirt          │\n",
      "│ b505b8ede9a27d5 ┆            ┆          ┆                ┆     ┆                ┆                │\n",
      "│ …               ┆            ┆          ┆                ┆     ┆                ┆                │\n",
      "│ cb71c7a2a28f378 ┆ 785060001  ┆ 0.033881 ┆ 2              ┆ 23  ┆ ACTIVE         ┆ Pyjama set     │\n",
      "│ 102cf4ceb839ad0 ┆            ┆          ┆                ┆     ┆                ┆                │\n",
      "│ …               ┆            ┆          ┆                ┆     ┆                ┆                │\n",
      "│ 26ed5afcdefaa39 ┆ 630116023  ┆ 0.016932 ┆ 2              ┆ 51  ┆ ACTIVE         ┆ Leggings/Tight │\n",
      "│ 670172d9a9cf3ef ┆            ┆          ┆                ┆     ┆                ┆ s              │\n",
      "│ …               ┆            ┆          ┆                ┆     ┆                ┆                │\n",
      "│ 845059419cc5d9e ┆ 829611001  ┆ 0.006763 ┆ 1              ┆ 39  ┆ ACTIVE         ┆ Hair/alice     │\n",
      "│ 609a5aa7d8fd7d4 ┆            ┆          ┆                ┆     ┆                ┆ band           │\n",
      "│ …               ┆            ┆          ┆                ┆     ┆                ┆                │\n",
      "│ bc9589fa44d035f ┆ 399223032  ┆ 0.028797 ┆ 1              ┆ 50  ┆ ACTIVE         ┆ Trousers       │\n",
      "│ f98abd368db21da ┆            ┆          ┆                ┆     ┆                ┆                │\n",
      "│ …               ┆            ┆          ┆                ┆     ┆                ┆                │\n",
      "│ 24207b6b3af2fd8 ┆ 903924001  ┆ 0.022017 ┆ 1              ┆ 19  ┆ ACTIVE         ┆ Earring        │\n",
      "│ 5e028395f4c9d36 ┆            ┆          ┆                ┆     ┆                ┆                │\n",
      "│ …               ┆            ┆          ┆                ┆     ┆                ┆                │\n",
      "└─────────────────┴────────────┴──────────┴────────────────┴─────┴────────────────┴────────────────┘\n",
      "  - Schema:\n",
      "Schema([('t_dat', String), ('customer_id', String), ('article_id', Int64), ('price', Float64), ('sales_channel_id', Int64), ('FN', Float64), ('Active', Float64), ('club_member_status', String), ('fashion_news_frequency', String), ('age', Int64), ('postal_code', String), ('product_code', Int64), ('prod_name', String), ('product_type_no', Int64), ('product_type_name', String), ('product_group_name', String), ('graphical_appearance_no', Int64), ('graphical_appearance_name', String), ('colour_group_code', Int64), ('colour_group_name', String), ('perceived_colour_value_id', Int64), ('perceived_colour_value_name', String), ('perceived_colour_master_id', Int64), ('perceived_colour_master_name', String), ('department_no', Int64), ('department_name', String), ('index_code', String), ('index_name', String), ('index_group_no', Int64), ('index_group_name', String), ('section_no', Int64), ('section_name', String), ('garment_group_no', Int64), ('garment_group_name', String), ('detail_desc', String)])\n"
     ]
    }
   ],
   "source": [
    "# Show sample of integrated data\n",
    "print(\"Sample of integrated dataset:\")\n",
    "sample_cols = [\"customer_id\", \"article_id\", \"price\", \"sales_channel_id\", \"age\", \"club_member_status\", \"product_type_name\"]\n",
    "available_cols = [col for col in sample_cols if col in df_integrated.columns]\n",
    "\n",
    "display_df = df_integrated.select(available_cols).head(10)\n",
    "print(display_df)\n",
    "\n",
    "# Print integrated data schema\n",
    "print(\"  - Schema:\")\n",
    "print(df_integrated.schema)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Integrated Dataset Structure Analysis\n",
    "\n",
    "Analyse the integrated dataset structure including customer and product diversity and date ranges.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Structure Analysis:\n",
      "• Total unique customers: 822,211\n",
      "• Total unique articles: 86,988\n",
      "• Date range: 2018-09-20 to 2020-09-22\n"
     ]
    }
   ],
   "source": [
    "print(f\"Dataset Structure Analysis:\")\n",
    "print(f\"• Total unique customers: {integration_stats['unique_customers']:,}\")\n",
    "print(f\"• Total unique articles: {integration_stats['unique_articles']:,}\")\n",
    "\n",
    "# Check if date column exists and show range\n",
    "if 't_dat' in df_integrated.columns:\n",
    "    date_stats = df_integrated.select([\n",
    "        pl.col('t_dat').min().alias('min_date'),\n",
    "        pl.col('t_dat').max().alias('max_date')\n",
    "    ]).to_pandas().iloc[0]\n",
    "    print(f\"• Date range: {date_stats['min_date']} to {date_stats['max_date']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Data Quality Assessment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analysing existing data quality issues in H&M dataset...\n",
      "\n",
      "• Missing Values Analysis:\n",
      "  FN: 1,821,934 (57.31%)\n",
      "  Active: 1,842,236 (57.95%)\n",
      "  club_member_status: 6,226 (0.20%)\n",
      "  fashion_news_frequency: 14,299 (0.45%)\n",
      "  age: 14,156 (0.45%)\n",
      "  detail_desc: 11,458 (0.36%)\n"
     ]
    }
   ],
   "source": [
    "print(\"Analysing existing data quality issues in H&M dataset...\")\n",
    "\n",
    "# Check for missing values efficiently with Polars\n",
    "print(f\"\\n• Missing Values Analysis:\")\n",
    "\n",
    "# Get null counts for all columns in one operation\n",
    "null_counts = df_integrated.null_count()\n",
    "total_records = df_integrated.height\n",
    "\n",
    "missing_summary = []\n",
    "for col_name in df_integrated.columns:\n",
    "    missing_count = null_counts[col_name][0]\n",
    "    if missing_count > 0:\n",
    "        missing_percentage = (missing_count / total_records) * 100\n",
    "        missing_summary.append((col_name, missing_count, missing_percentage))\n",
    "        print(f\"  {col_name}: {missing_count:,} ({missing_percentage:.2f}%)\")\n",
    "\n",
    "if not missing_summary:\n",
    "    print(\"  ✓ No missing values found in dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Duplicate Analysis\n",
    "\n",
    "Analyse the dataset for duplicate records to understand data integrity using Polars' efficient duplicate detection.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "• Duplicate Analysis:\n",
      "  Total records: 3,178,832\n",
      "  Unique records: 3,141,366\n",
      "  Duplicate records: 37,466\n"
     ]
    }
   ],
   "source": [
    "# Analyse duplicates efficiently\n",
    "print(f\"\\n• Duplicate Analysis:\")\n",
    "total_records = df_integrated.height\n",
    "unique_records = df_integrated.unique().height\n",
    "duplicate_count = total_records - unique_records\n",
    "\n",
    "print(f\"  Total records: {total_records:,}\")\n",
    "print(f\"  Unique records: {unique_records:,}\")\n",
    "print(f\"  Duplicate records: {duplicate_count:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Price Distribution Analysis\n",
    "\n",
    "Analyse price distribution to identify potential outliers and understand the pricing structure of H&M products using Polars' statistical functions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "• Price Distribution Analysis:\n",
      "  count: 3178832.00\n",
      "  mean: 0.03\n",
      "  std: 0.02\n",
      "  min: 0.00\n",
      "  25%: 0.02\n",
      "  50%: 0.03\n",
      "  75%: 0.03\n",
      "  max: 0.59\n"
     ]
    }
   ],
   "source": [
    "# Check for potential outliers in price data\n",
    "if 'price' in df_integrated.columns:\n",
    "    print(f\"\\n• Price Distribution Analysis:\")\n",
    "    \n",
    "    # Get comprehensive price statistics in one operation\n",
    "    price_stats = df_integrated.select([\n",
    "        pl.col('price').count().alias('count'),\n",
    "        pl.col('price').mean().alias('mean'),\n",
    "        pl.col('price').std().alias('std'),\n",
    "        pl.col('price').min().alias('min'),\n",
    "        pl.col('price').quantile(0.25).alias('25%'),\n",
    "        pl.col('price').quantile(0.5).alias('50%'),\n",
    "        pl.col('price').quantile(0.75).alias('75%'),\n",
    "        pl.col('price').max().alias('max')\n",
    "    ]).to_pandas().iloc[0]\n",
    "    \n",
    "    for stat_name, value in price_stats.items():\n",
    "        print(f\"  {stat_name}: {value:.2f}\")\n",
    "else:\n",
    "    print(f\"\\n• No price column found in dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 11. Performance Summary\n",
    "\n",
    "Display processing performance metrics of Polars' efficiency.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✓ Data Exploration Complete\n",
      "  - Transaction records: 3,178,832\n",
      "  - Integrated records: 3,178,832\n",
      "  - Unique customers: 822,211\n",
      "  - Unique articles: 86,988\n",
      "  - Total memory usage: 2183.1 MB\n",
      "\n",
      "============================================================\n",
      "DATA UNDERSTANDING COMPLETED SUCCESSFULLY\n",
      "============================================================\n",
      "Integrated dataset available as 'df_integrated'\n",
      "Quality report available as 'quality_report'\n"
     ]
    }
   ],
   "source": [
    "# Compile quality assessment results\n",
    "quality_report = {\n",
    "    'missing_summary': missing_summary,\n",
    "    'duplicate_count': duplicate_count,\n",
    "    'total_records': total_records\n",
    "}\n",
    "\n",
    "# Calculate total memory usage\n",
    "total_memory_mb = (\n",
    "    df_transactions.estimated_size('mb') + \n",
    "    df_customers.estimated_size('mb') + \n",
    "    df_articles.estimated_size('mb') + \n",
    "    df_integrated.estimated_size('mb')\n",
    ")\n",
    "\n",
    "print(f\"\\n✓ Data Exploration Complete\")\n",
    "print(f\"  - Transaction records: {transaction_count:,}\")\n",
    "print(f\"  - Integrated records: {integration_stats['total_records']:,}\")\n",
    "print(f\"  - Unique customers: {integration_stats['unique_customers']:,}\")\n",
    "print(f\"  - Unique articles: {integration_stats['unique_articles']:,}\")\n",
    "print(f\"  - Total memory usage: {total_memory_mb:.1f} MB\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"DATA UNDERSTANDING COMPLETED SUCCESSFULLY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"Integrated dataset available as 'df_integrated'\")\n",
    "print(f\"Quality report available as 'quality_report'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 12. Save Results\n",
    "\n",
    "Save the integrated dataset and results for use in preprocessing and further analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Integrated dataset saved as Parquet file\n",
      "✓ Integrated dataset saved as CSV file\n"
     ]
    }
   ],
   "source": [
    "output_dir = os.path.join(data_dir, 'processed')\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save as Parquet for efficient storage\n",
    "df_integrated.write_parquet(os.path.join(output_dir, 'hm_integrated_dataset.parquet'))\n",
    "print(f\"✓ Integrated dataset saved as Parquet file\")\n",
    "\n",
    "# Save integrated dataset as CSV for easier inspection\n",
    "df_integrated.write_csv(os.path.join(output_dir, 'hm_integrated_dataset.csv'))\n",
    "print(f\"✓ Integrated dataset saved as CSV file\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
