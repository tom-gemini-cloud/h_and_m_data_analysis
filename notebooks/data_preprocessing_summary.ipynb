{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# H&M Data Preprocessing Visualisations\n",
    "\n",
    "This notebook provides comprehensive visualisations for the H&M data preprocessing and cleaning pipeline. It demonstrates the impact of data cleaning operations including duplicate removal, missing value imputation, and outlier handling through before/after comparisons and quality improvement metrics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries and Setup\n",
    "\n",
    "Import necessary libraries for data processing and visualisation.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully\n",
      "Polars version: 0.20.31\n",
      "Matplotlib version: 3.8.3\n",
      "Seaborn version: 0.13.2\n"
     ]
    }
   ],
   "source": [
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "import plotly.figure_factory as ff\n",
    "import os\n",
    "import time\n",
    "from typing import Tuple, List, Dict, Optional\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Configure Polars\n",
    "try:\n",
    "    pl.Config.set_streaming_chunk_size(10000)\n",
    "    if hasattr(pl.Config, 'set_table_width'):\n",
    "        pl.Config.set_table_width(120)\n",
    "except AttributeError:\n",
    "    pass\n",
    "\n",
    "print(f\"Libraries imported successfully\")\n",
    "print(f\"Polars version: {pl.__version__}\")\n",
    "print(f\"Matplotlib version: {plt.matplotlib.__version__}\")\n",
    "print(f\"Seaborn version: {sns.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Original and Cleaned Datasets\n",
    "\n",
    "Load both the original integrated dataset and the cleaned dataset to perform before/after comparisons.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading datasets for preprocessing visualisation...\n",
      "‚úì Original dataset loaded: 3,178,832 records\n",
      "‚úì Cleaned dataset loaded: 3,141,398 records\n",
      "\n",
      "Dataset Comparison:\n",
      "‚Ä¢ Original memory usage: 1634.5 MB\n",
      "‚Ä¢ Cleaned memory usage: 1615.8 MB\n",
      "‚Ä¢ Memory change: -18.7 MB\n"
     ]
    }
   ],
   "source": [
    "# Load datasets for comparison\n",
    "data_dir = '../data'\n",
    "original_path = os.path.join(data_dir, 'processed', 'hm_integrated_dataset.parquet')\n",
    "cleaned_path = os.path.join(data_dir, 'processed', 'hm_customer_data_cleaned.parquet')\n",
    "\n",
    "print(\"Loading datasets for preprocessing visualisation...\")\n",
    "\n",
    "# Load original dataset\n",
    "if os.path.exists(original_path):\n",
    "    df_original = pl.read_parquet(original_path)\n",
    "    print(f\"‚úì Original dataset loaded: {df_original.height:,} records\")\n",
    "else:\n",
    "    print(\"Original integrated dataset not found. Creating from exploration...\")\n",
    "    # Fallback: create sample dataset to demonstrate preprocessing\n",
    "    # This would normally come from the exploration notebook\n",
    "    raise FileNotFoundError(f\"Please run the exploration notebook first: {original_path}\")\n",
    "\n",
    "# Load cleaned dataset\n",
    "if os.path.exists(cleaned_path):\n",
    "    df_cleaned = pl.read_parquet(cleaned_path)\n",
    "    print(f\"‚úì Cleaned dataset loaded: {df_cleaned.height:,} records\")\n",
    "else:\n",
    "    print(\"Cleaned dataset not found. Please run the preprocessing notebook first.\")\n",
    "    # For demonstration, we'll create the cleaned version during this notebook\n",
    "    print(\"Will demonstrate cleaning process step-by-step...\")\n",
    "    df_cleaned = None\n",
    "\n",
    "# Calculate memory usage\n",
    "original_memory = df_original.estimated_size('mb')\n",
    "cleaned_memory = df_cleaned.estimated_size('mb') if df_cleaned is not None else 0\n",
    "\n",
    "print(f\"\\nDataset Comparison:\")\n",
    "print(f\"‚Ä¢ Original memory usage: {original_memory:.1f} MB\")\n",
    "if df_cleaned is not None:\n",
    "    print(f\"‚Ä¢ Cleaned memory usage: {cleaned_memory:.1f} MB\")\n",
    "    print(f\"‚Ä¢ Memory change: {cleaned_memory - original_memory:+.1f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Quality Validation\n",
    "\n",
    "Comprehensive validation of the cleaned dataset quality and readiness for analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing final quality validation...\n",
      "‚úì Final quality validation completed\n",
      "\n",
      "============================================================\n",
      "FINAL QUALITY VALIDATION RESULTS\n",
      "============================================================\n",
      "\n",
      "Overall Quality Score: 100.0% - PASS\n",
      "\n",
      "Detailed Results:\n",
      "  ‚Ä¢ Completeness: 100.0% - PASS\n",
      "  ‚Ä¢ Uniqueness: 100.0% - PASS\n",
      "\n",
      "Dataset Readiness:\n",
      "  ‚úÖ Dataset is ready for further analysis and modelling\n"
     ]
    }
   ],
   "source": [
    "# Final quality validation\n",
    "print(\"Performing final quality validation...\")\n",
    "\n",
    "if df_cleaned is not None:\n",
    "    # Comprehensive quality checks\n",
    "    validation_results = {}\n",
    "\n",
    "    # Completeness check\n",
    "    total_cells = df_cleaned.height * len(df_cleaned.columns)\n",
    "    missing_cells = sum(df_cleaned.null_count().row(0))\n",
    "    completeness = ((total_cells - missing_cells) / total_cells) * 100\n",
    "    validation_results['completeness'] = {\n",
    "        'score': completeness,\n",
    "        'status': 'PASS' if completeness >= 95 else 'REVIEW' if completeness >= 90 else 'FAIL'\n",
    "    }\n",
    "\n",
    "    # Uniqueness check\n",
    "    uniqueness = (df_cleaned.unique().height / df_cleaned.height) * 100\n",
    "    validation_results['uniqueness'] = {\n",
    "        'score': uniqueness,\n",
    "        'status': 'PASS' if uniqueness >= 98 else 'REVIEW' if uniqueness >= 95 else 'FAIL'\n",
    "    }\n",
    "\n",
    "    # Overall quality score\n",
    "    overall_score = np.mean([v['score'] for v in validation_results.values()])\n",
    "    overall_status = 'PASS' if overall_score >= 95 else 'REVIEW' if overall_score >= 90 else 'FAIL'\n",
    "\n",
    "    print(f\"‚úì Final quality validation completed\")\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"FINAL QUALITY VALIDATION RESULTS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"\\nOverall Quality Score: {overall_score:.1f}% - {overall_status}\")\n",
    "    print(f\"\\nDetailed Results:\")\n",
    "    for dimension, result in validation_results.items():\n",
    "        print(f\"  ‚Ä¢ {dimension.title()}: {result['score']:.1f}% - {result['status']}\")\n",
    "\n",
    "    print(f\"\\nDataset Readiness:\")\n",
    "    if overall_status == 'PASS':\n",
    "        print(f\"  ‚úÖ Dataset is ready for further analysis and modelling\")\n",
    "    elif overall_status == 'REVIEW':\n",
    "        print(f\"  ‚ö†Ô∏è  Dataset quality is acceptable but may need attention in some areas\")\n",
    "    else:\n",
    "        print(f\"  ‚ùå Dataset requires additional cleaning before use\")\n",
    "\n",
    "else:\n",
    "    print(\"Cannot perform validation without cleaned dataset\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Next Steps\n",
    "\n",
    "Compile comprehensive summary of preprocessing improvements and recommendations for analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "H&M DATA PREPROCESSING - COMPREHENSIVE SUMMARY\n",
      "======================================================================\n",
      "\n",
      "üìä PREPROCESSING SUMMARY:\n",
      "  ‚Ä¢ Records processed: 3,178,832 ‚Üí 3,141,398 (-37,434)\n",
      "  ‚Ä¢ Duplicates removed: 37,434\n",
      "  ‚Ä¢ Missing value columns resolved: 6 ‚Üí 0\n",
      "  ‚Ä¢ Memory usage: 1634.5MB ‚Üí 1615.8MB (-18.7MB)\n",
      "\n",
      "‚úÖ PREPROCESSING PIPELINE COMPLETED SUCCESSFULLY\n",
      "‚úÖ DATASET READY FOR FURTHER ANALYTICS AND MODELLING\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# Comprehensive summary and recommendations\n",
    "print(\"=\" * 70)\n",
    "print(\"H&M DATA PREPROCESSING - COMPREHENSIVE SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "if df_cleaned is not None:\n",
    "    print(f\"\\nüìä PREPROCESSING SUMMARY:\")\n",
    "    print(f\"  ‚Ä¢ Records processed: {original_total:,} ‚Üí {cleaned_total:,} ({cleaned_total - original_total:+,})\")\n",
    "    print(f\"  ‚Ä¢ Duplicates removed: {original_metrics['duplicates']:,}\")\n",
    "    print(f\"  ‚Ä¢ Missing value columns resolved: {len(original_missing_df)} ‚Üí {len(cleaned_missing_df)}\")\n",
    "    print(f\"  ‚Ä¢ Memory usage: {original_memory:.1f}MB ‚Üí {df_cleaned.estimated_size('mb'):.1f}MB ({df_cleaned.estimated_size('mb') - original_memory:+.1f}MB)\")\n",
    "\n",
    "    print(f\"\\n‚úÖ PREPROCESSING PIPELINE COMPLETED SUCCESSFULLY\")\n",
    "    print(f\"‚úÖ DATASET READY FOR FURTHER ANALYTICS AND MODELLING\")\n",
    "\n",
    "else:\n",
    "    print(\"\\n‚ö†Ô∏è  Please run the preprocessing notebook to generate cleaned dataset\")\n",
    "\n",
    "print(f\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
