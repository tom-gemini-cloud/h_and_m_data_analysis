{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# H&M Product Recommendation Modelling\n",
    "\n",
    "This notebook implements multiple recommendation system approaches for predicting customer product preferences and purchase behaviour.\n",
    "\n",
    "## Modelling Approaches\n",
    "1. **Collaborative Filtering** - Matrix factorization and neighborhood-based methods\n",
    "2. **Content-Based Filtering** - Product and customer feature-based recommendations\n",
    "3. **Hybrid Models** - Combining multiple approaches\n",
    "4. **Purchase Prediction** - Binary classification for purchase likelihood\n",
    "\n",
    "## Business Objectives\n",
    "- Predict which products customers are likely to purchase\n",
    "- Recommend relevant products to increase engagement\n",
    "- Identify customer preferences and shopping patterns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('../')\n",
    "\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import pickle\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Machine learning libraries\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import TruncatedSVD, NMF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score, \n",
    "    roc_auc_score, classification_report, confusion_matrix\n",
    ")\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Set display options\n",
    "pl.Config.set_tbl_rows(10)\n",
    "pl.Config.set_tbl_cols(15)\n",
    "pd.set_option('display.max_columns', 20)\n",
    "pd.set_option('display.max_rows', 10)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries imported successfully\")\n",
    "print(f\"Current working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Training and Test Data\n",
    "\n",
    "Load the preprocessed training and test datasets created in the data preparation phase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training and test datasets\n",
    "print(\"Loading training and test datasets...\")\n",
    "\n",
    "train_df = pl.read_parquet(\"../data/modelling_data/train_data.parquet\")\n",
    "test_df = pl.read_parquet(\"../data/modelling_data/test_data.parquet\")\n",
    "\n",
    "print(f\"Training data shape: {train_df.shape}\")\n",
    "print(f\"Test data shape: {test_df.shape}\")\n",
    "print(f\"Total features: {train_df.shape[1]}\")\n",
    "\n",
    "# Check data consistency\n",
    "print(f\"\\nData consistency checks:\")\n",
    "print(f\"Training customers: {train_df['customer_id'].n_unique():,}\")\n",
    "print(f\"Test customers: {test_df['customer_id'].n_unique():,}\")\n",
    "print(f\"Training articles: {train_df['article_id'].n_unique():,}\")\n",
    "print(f\"Test articles: {test_df['article_id'].n_unique():,}\")\n",
    "\n",
    "# Check for customer overlap (should be 0)\n",
    "train_customers = set(train_df['customer_id'].unique())\n",
    "test_customers = set(test_df['customer_id'].unique())\n",
    "overlap = train_customers.intersection(test_customers)\n",
    "print(f\"Customer overlap: {len(overlap)} (should be 0)\")\n",
    "\n",
    "# Display sample data\n",
    "print(f\"\\nSample training data:\")\n",
    "display(train_df.head(3))\n",
    "\n",
    "print(f\"\\nColumn data types:\")\n",
    "for col, dtype in zip(train_df.columns, train_df.dtypes):\n",
    "    print(f\"{col}: {dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation for Recommendation Systems\n",
    "\n",
    "Prepare the data for different recommendation approaches by creating interaction matrices and feature sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to pandas for easier manipulation with sklearn\n",
    "train_pd = train_df.to_pandas()\n",
    "test_pd = test_df.to_pandas()\n",
    "\n",
    "print(\"Data converted to pandas for modeling\")\n",
    "\n",
    "# Create user-item interaction matrix for collaborative filtering\n",
    "print(\"\\nCreating user-item interaction matrices...\")\n",
    "\n",
    "# For simplicity, we'll use binary interactions (purchased = 1, not purchased = 0)\n",
    "# In a real scenario, you might use ratings, purchase frequency, or purchase amounts\n",
    "\n",
    "# Training interaction matrix\n",
    "train_interactions = train_pd.groupby(['customer_id', 'article_id']).size().reset_index(name='interaction_count')\n",
    "train_interactions['interaction'] = 1  # Binary interaction\n",
    "\n",
    "print(f\"Training interactions: {train_interactions.shape[0]:,}\")\n",
    "print(f\"Unique customer-article pairs in training: {train_interactions.shape[0]:,}\")\n",
    "\n",
    "# Test interactions\n",
    "test_interactions = test_pd.groupby(['customer_id', 'article_id']).size().reset_index(name='interaction_count')\n",
    "test_interactions['interaction'] = 1\n",
    "\n",
    "print(f\"Test interactions: {test_interactions.shape[0]:,}\")\n",
    "\n",
    "# Create pivot tables for matrix factorization\n",
    "print(\"\\nCreating interaction matrices...\")\n",
    "\n",
    "# Get all unique customers and articles from training data\n",
    "all_customers = sorted(train_pd['customer_id'].unique())\n",
    "all_articles = sorted(train_pd['article_id'].unique())\n",
    "\n",
    "print(f\"Total customers in training: {len(all_customers):,}\")\n",
    "print(f\"Total articles in training: {len(all_articles):,}\")\n",
    "\n",
    "# Create customer and article encoders\n",
    "customer_encoder = LabelEncoder()\n",
    "article_encoder = LabelEncoder()\n",
    "\n",
    "customer_encoder.fit(all_customers)\n",
    "article_encoder.fit(all_articles)\n",
    "\n",
    "# Encode interactions\n",
    "train_interactions['customer_idx'] = customer_encoder.transform(train_interactions['customer_id'])\n",
    "train_interactions['article_idx'] = article_encoder.transform(train_interactions['article_id'])\n",
    "\n",
    "print(\"Customer and article encoding completed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: Collaborative Filtering with Matrix Factorization\n",
    "\n",
    "Implement collaborative filtering using SVD (Singular Value Decomposition) for matrix factorization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "print(\"=== Collaborative Filtering with SVD ===\")\n",
    "\n",
    "# Create sparse interaction matrix\n",
    "n_customers = len(all_customers)\n",
    "n_articles = len(all_articles)\n",
    "\n",
    "print(f\"Creating {n_customers:,} x {n_articles:,} interaction matrix...\")\n",
    "\n",
    "# Create sparse matrix from interactions\n",
    "interaction_matrix = csr_matrix(\n",
    "    (train_interactions['interaction'], \n",
    "     (train_interactions['customer_idx'], train_interactions['article_idx'])),\n",
    "    shape=(n_customers, n_articles)\n",
    ")\n",
    "\n",
    "print(f\"Interaction matrix density: {interaction_matrix.nnz / (n_customers * n_articles) * 100:.4f}%\")\n",
    "print(f\"Non-zero elements: {interaction_matrix.nnz:,}\")\n",
    "\n",
    "# Apply SVD for matrix factorization\n",
    "print(\"\\nApplying SVD matrix factorization...\")\n",
    "\n",
    "# Use TruncatedSVD for sparse matrices\n",
    "n_components = min(50, min(n_customers, n_articles) - 1)  # Reduced for memory efficiency\n",
    "svd_model = TruncatedSVD(n_components=n_components, random_state=42)\n",
    "\n",
    "# Fit SVD on the interaction matrix\n",
    "customer_factors = svd_model.fit_transform(interaction_matrix)\n",
    "article_factors = svd_model.components_\n",
    "\n",
    "print(f\"SVD completed with {n_components} components\")\n",
    "print(f\"Explained variance ratio: {svd_model.explained_variance_ratio_.sum():.4f}\")\n",
    "print(f\"Customer factors shape: {customer_factors.shape}\")\n",
    "print(f\"Article factors shape: {article_factors.shape}\")\n",
    "\n",
    "# Function to get recommendations for a customer\n",
    "def get_svd_recommendations(customer_id, n_recommendations=10):\n",
    "    \"\"\"\n",
    "    Get SVD-based recommendations for a customer\n",
    "    \"\"\"\n",
    "    try:\n",
    "        customer_idx = customer_encoder.transform([customer_id])[0]\n",
    "        \n",
    "        # Get customer's latent factors\n",
    "        customer_vector = customer_factors[customer_idx]\n",
    "        \n",
    "        # Compute scores for all articles\n",
    "        scores = np.dot(customer_vector, article_factors)\n",
    "        \n",
    "        # Get articles customer has already interacted with\n",
    "        customer_articles = set(train_interactions[train_interactions['customer_id'] == customer_id]['article_id'])\n",
    "        \n",
    "        # Create recommendations excluding already purchased items\n",
    "        recommendations = []\n",
    "        article_scores = list(zip(all_articles, scores))\n",
    "        article_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        \n",
    "        for article_id, score in article_scores:\n",
    "            if article_id not in customer_articles and len(recommendations) < n_recommendations:\n",
    "                recommendations.append((article_id, score))\n",
    "        \n",
    "        return recommendations\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting recommendations for customer {customer_id}: {e}\")\n",
    "        return []\n",
    "\n",
    "# Test recommendations for a sample customer\n",
    "sample_customer = all_customers[0]\n",
    "sample_recommendations = get_svd_recommendations(sample_customer, 5)\n",
    "\n",
    "print(f\"\\nSample recommendations for customer {sample_customer}:\")\n",
    "for i, (article_id, score) in enumerate(sample_recommendations, 1):\n",
    "    print(f\"{i}. Article {article_id}: Score {score:.4f}\")\n",
    "\n",
    "print(\"\\nSVD Collaborative Filtering model ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2: Content-Based Filtering\n",
    "\n",
    "Implement content-based filtering using product features and customer preferences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"=== Content-Based Filtering ===\")\n\n# Extract product features for content-based filtering\nprint(\"Preparing product features...\")\n\n# Get unique articles with their features\narticle_features = train_pd[[\n    'article_id', 'product_type_name', 'product_group_name', \n    'graphical_appearance_name', 'colour_group_name', \n    'perceived_colour_value_name', 'perceived_colour_master_name',\n    'department_name', 'index_name', 'index_group_name',\n    'section_name', 'garment_group_name'\n]].drop_duplicates(subset=['article_id'])\n\nprint(f\"Articles with features: {article_features.shape[0]:,}\")\n\n# Convert categorical columns to string and handle missing values\ntext_columns = ['product_type_name', 'product_group_name', 'colour_group_name', \n                'department_name', 'section_name', 'garment_group_name']\n\nfor col in text_columns:\n    if col in article_features.columns:\n        # Convert to string and handle NaN values\n        article_features[col] = article_features[col].astype(str).replace('nan', 'unknown')\n\n# Create content features by combining text descriptions\narticle_features['content_features'] = (\n    article_features['product_type_name'] + ' ' +\n    article_features['product_group_name'] + ' ' +\n    article_features['colour_group_name'] + ' ' +\n    article_features['department_name'] + ' ' +\n    article_features['section_name'] + ' ' +\n    article_features['garment_group_name']\n)\n\n# Create TF-IDF vectors for content features\nprint(\"Creating TF-IDF vectors for product content...\")\n\ntfidf_vectorizer = TfidfVectorizer(\n    max_features=1000,  # Limit features for memory efficiency\n    stop_words='english',\n    ngram_range=(1, 2),\n    min_df=2\n)\n\ncontent_matrix = tfidf_vectorizer.fit_transform(article_features['content_features'])\nprint(f\"Content matrix shape: {content_matrix.shape}\")\n\n# Calculate content similarity matrix\nprint(\"Computing content similarity matrix...\")\ncontent_similarity = cosine_similarity(content_matrix)\nprint(f\"Content similarity matrix shape: {content_similarity.shape}\")\n\n# Create article index mapping\narticle_to_idx = {article_id: idx for idx, article_id in enumerate(article_features['article_id'])}\nidx_to_article = {idx: article_id for article_id, idx in article_to_idx.items()}\n\ndef get_content_based_recommendations(customer_id, n_recommendations=10):\n    \"\"\"\n    Get content-based recommendations for a customer based on their purchase history\n    \"\"\"\n    # Get customer's purchased articles\n    customer_articles = train_interactions[train_interactions['customer_id'] == customer_id]['article_id'].tolist()\n    \n    if not customer_articles:\n        return []\n    \n    # Calculate average similarity scores for articles similar to purchased ones\n    similarity_scores = np.zeros(len(article_features))\n    \n    for article_id in customer_articles:\n        if article_id in article_to_idx:\n            article_idx = article_to_idx[article_id]\n            similarity_scores += content_similarity[article_idx]\n    \n    if len(customer_articles) > 0:\n        similarity_scores /= len(customer_articles)\n    \n    # Get top recommendations excluding already purchased items\n    customer_articles_set = set(customer_articles)\n    recommendations = []\n    \n    # Sort by similarity score\n    sorted_indices = np.argsort(similarity_scores)[::-1]\n    \n    for idx in sorted_indices:\n        article_id = idx_to_article[idx]\n        if article_id not in customer_articles_set and len(recommendations) < n_recommendations:\n            recommendations.append((article_id, similarity_scores[idx]))\n    \n    return recommendations\n\n# Test content-based recommendations\nsample_content_recommendations = get_content_based_recommendations(sample_customer, 5)\n\nprint(f\"\\nSample content-based recommendations for customer {sample_customer}:\")\nfor i, (article_id, score) in enumerate(sample_content_recommendations, 1):\n    print(f\"{i}. Article {article_id}: Score {score:.4f}\")\n\nprint(\"\\nContent-Based Filtering model ready\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3: Purchase Prediction Classification\n",
    "\n",
    "Build classification models to predict whether a customer will purchase a specific product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Purchase Prediction Classification ===\")\n",
    "\n",
    "# Prepare features for classification\n",
    "print(\"Preparing features for purchase prediction...\")\n",
    "\n",
    "# Select numerical and categorical features\n",
    "numerical_features = [\n",
    "    'price', 'age', 'recency', 'frequency', 'monetary',\n",
    "    'purchase_diversity_score', 'price_sensitivity_index',\n",
    "    'colour_preference_entropy', 'style_consistency_score'\n",
    "]\n",
    "\n",
    "categorical_features = [\n",
    "    'club_member_status', 'fashion_news_frequency', 'sales_channel_id',\n",
    "    'product_type_name', 'colour_group_name', 'department_name'\n",
    "]\n",
    "\n",
    "# Prepare training data\n",
    "print(\"Preparing training features...\")\n",
    "\n",
    "# Create a balanced dataset by sampling negative examples\n",
    "# Positive examples: actual purchases\n",
    "positive_samples = train_pd.copy()\n",
    "positive_samples['purchased'] = 1\n",
    "\n",
    "print(f\"Positive samples (actual purchases): {len(positive_samples):,}\")\n",
    "\n",
    "# Create negative samples: customers who didn't purchase specific articles\n",
    "print(\"Creating negative samples...\")\n",
    "\n",
    "# Sample a subset for negative examples to balance the dataset\n",
    "n_negative_samples = min(len(positive_samples), 100000)  # Limit for memory\n",
    "\n",
    "# Get random customer-article pairs that don't exist in positive samples\n",
    "positive_pairs = set(zip(positive_samples['customer_id'], positive_samples['article_id']))\n",
    "\n",
    "negative_samples = []\n",
    "sample_customers = np.random.choice(all_customers, size=n_negative_samples, replace=True)\n",
    "sample_articles = np.random.choice(all_articles, size=n_negative_samples, replace=True)\n",
    "\n",
    "for customer_id, article_id in zip(sample_customers, sample_articles):\n",
    "    if (customer_id, article_id) not in positive_pairs:\n",
    "        negative_samples.append((customer_id, article_id))\n",
    "    if len(negative_samples) >= n_negative_samples:\n",
    "        break\n",
    "\n",
    "print(f\"Created {len(negative_samples):,} negative samples\")\n",
    "\n",
    "# Create negative samples dataframe\n",
    "negative_df_list = []\n",
    "for customer_id, article_id in negative_samples[:n_negative_samples//2]:  # Further reduce for memory\n",
    "    # Get customer features\n",
    "    customer_data = train_pd[train_pd['customer_id'] == customer_id].iloc[0]\n",
    "    # Get article features  \n",
    "    article_data = train_pd[train_pd['article_id'] == article_id].iloc[0]\n",
    "    \n",
    "    # Combine features\n",
    "    negative_row = customer_data.copy()\n",
    "    negative_row['article_id'] = article_id\n",
    "    # Update article-specific features\n",
    "    for col in ['product_type_name', 'product_group_name', 'colour_group_name', \n",
    "                'department_name', 'section_name', 'garment_group_name']:\n",
    "        if col in article_data:\n",
    "            negative_row[col] = article_data[col]\n",
    "    \n",
    "    negative_df_list.append(negative_row)\n",
    "\n",
    "negative_df = pd.DataFrame(negative_df_list)\n",
    "negative_df['purchased'] = 0\n",
    "\n",
    "print(f\"Negative samples dataframe: {negative_df.shape}\")\n",
    "\n",
    "# Combine positive and negative samples\n",
    "classification_data = pd.concat([\n",
    "    positive_samples.sample(n=len(negative_df), random_state=42),  # Sample to match negative samples\n",
    "    negative_df\n",
    "], ignore_index=True)\n",
    "\n",
    "print(f\"Combined classification dataset: {classification_data.shape}\")\n",
    "print(f\"Class distribution:\")\n",
    "print(classification_data['purchased'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare features for modeling\n",
    "print(\"Preparing features for classification models...\")\n",
    "\n",
    "# Handle missing values and encode categorical variables\n",
    "classification_features = classification_data[numerical_features + categorical_features].copy()\n",
    "\n",
    "# Fill missing values\n",
    "for col in numerical_features:\n",
    "    if col in classification_features.columns:\n",
    "        classification_features[col] = classification_features[col].fillna(classification_features[col].median())\n",
    "\n",
    "for col in categorical_features:\n",
    "    if col in classification_features.columns:\n",
    "        classification_features[col] = classification_features[col].fillna('Unknown')\n",
    "\n",
    "# Encode categorical variables\n",
    "print(\"Encoding categorical variables...\")\n",
    "encoded_features = classification_features.copy()\n",
    "\n",
    "label_encoders = {}\n",
    "for col in categorical_features:\n",
    "    if col in encoded_features.columns:\n",
    "        le = LabelEncoder()\n",
    "        encoded_features[col] = le.fit_transform(encoded_features[col].astype(str))\n",
    "        label_encoders[col] = le\n",
    "\n",
    "# Scale numerical features\n",
    "scaler = StandardScaler()\n",
    "numerical_cols = [col for col in numerical_features if col in encoded_features.columns]\n",
    "encoded_features[numerical_cols] = scaler.fit_transform(encoded_features[numerical_cols])\n",
    "\n",
    "# Prepare target variable\n",
    "y = classification_data['purchased']\n",
    "X = encoded_features\n",
    "\n",
    "print(f\"Final feature matrix shape: {X.shape}\")\n",
    "print(f\"Features: {list(X.columns)}\")\n",
    "\n",
    "# Split into train and validation sets\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "print(f\"Training set: {X_train.shape}\")\n",
    "print(f\"Validation set: {X_val.shape}\")\n",
    "print(f\"Training class distribution: {y_train.value_counts().to_dict()}\")\n",
    "print(f\"Validation class distribution: {y_val.value_counts().to_dict()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train multiple classification models\n",
    "print(\"Training classification models...\")\n",
    "\n",
    "models = {\n",
    "    'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=42, n_jobs=-1),\n",
    "    'Gradient Boosting': GradientBoostingClassifier(n_estimators=100, random_state=42)\n",
    "}\n",
    "\n",
    "trained_models = {}\n",
    "model_scores = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    print(f\"\\nTraining {name}...\")\n",
    "    \n",
    "    # Train model\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred = model.predict(X_val)\n",
    "    y_pred_proba = model.predict_proba(X_val)[:, 1]\n",
    "    \n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(y_val, y_pred)\n",
    "    precision = precision_score(y_val, y_pred)\n",
    "    recall = recall_score(y_val, y_pred)\n",
    "    f1 = f1_score(y_val, y_pred)\n",
    "    auc = roc_auc_score(y_val, y_pred_proba)\n",
    "    \n",
    "    # Store results\n",
    "    trained_models[name] = model\n",
    "    model_scores[name] = {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1_score': f1,\n",
    "        'auc_roc': auc,\n",
    "        'predictions': y_pred,\n",
    "        'probabilities': y_pred_proba\n",
    "    }\n",
    "    \n",
    "    print(f\"{name} Results:\")\n",
    "    print(f\"  Accuracy: {accuracy:.4f}\")\n",
    "    print(f\"  Precision: {precision:.4f}\")\n",
    "    print(f\"  Recall: {recall:.4f}\")\n",
    "    print(f\"  F1-score: {f1:.4f}\")\n",
    "    print(f\"  AUC-ROC: {auc:.4f}\")\n",
    "\n",
    "print(\"\\n=== Model Training Complete ===\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 4: Hybrid Recommendation System\n",
    "\n",
    "Combine collaborative filtering and content-based approaches for improved recommendations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Hybrid Recommendation System ===\")\n",
    "\n",
    "def get_hybrid_recommendations(customer_id, n_recommendations=10, cf_weight=0.6, cb_weight=0.4):\n",
    "    \"\"\"\n",
    "    Get hybrid recommendations combining collaborative filtering and content-based approaches\n",
    "    \n",
    "    Args:\n",
    "        customer_id: Customer ID\n",
    "        n_recommendations: Number of recommendations to return\n",
    "        cf_weight: Weight for collaborative filtering scores\n",
    "        cb_weight: Weight for content-based scores\n",
    "    \"\"\"\n",
    "    # Get recommendations from both approaches\n",
    "    cf_recommendations = get_svd_recommendations(customer_id, n_recommendations * 2)\n",
    "    cb_recommendations = get_content_based_recommendations(customer_id, n_recommendations * 2)\n",
    "    \n",
    "    # Convert to dictionaries for easier merging\n",
    "    cf_scores = {article_id: score for article_id, score in cf_recommendations}\n",
    "    cb_scores = {article_id: score for article_id, score in cb_recommendations}\n",
    "    \n",
    "    # Normalize scores to 0-1 range\n",
    "    if cf_scores:\n",
    "        cf_max = max(cf_scores.values())\n",
    "        cf_min = min(cf_scores.values())\n",
    "        if cf_max > cf_min:\n",
    "            cf_scores = {k: (v - cf_min) / (cf_max - cf_min) for k, v in cf_scores.items()}\n",
    "    \n",
    "    if cb_scores:\n",
    "        cb_max = max(cb_scores.values())\n",
    "        cb_min = min(cb_scores.values())\n",
    "        if cb_max > cb_min:\n",
    "            cb_scores = {k: (v - cb_min) / (cb_max - cb_min) for k, v in cb_scores.items()}\n",
    "    \n",
    "    # Combine scores\n",
    "    all_articles = set(cf_scores.keys()) | set(cb_scores.keys())\n",
    "    hybrid_scores = {}\n",
    "    \n",
    "    for article_id in all_articles:\n",
    "        cf_score = cf_scores.get(article_id, 0)\n",
    "        cb_score = cb_scores.get(article_id, 0)\n",
    "        hybrid_score = cf_weight * cf_score + cb_weight * cb_score\n",
    "        hybrid_scores[article_id] = hybrid_score\n",
    "    \n",
    "    # Sort and return top recommendations\n",
    "    sorted_recommendations = sorted(hybrid_scores.items(), key=lambda x: x[1], reverse=True)\n",
    "    return sorted_recommendations[:n_recommendations]\n",
    "\n",
    "# Test hybrid recommendations\n",
    "sample_hybrid_recommendations = get_hybrid_recommendations(sample_customer, 5)\n",
    "\n",
    "print(f\"Sample hybrid recommendations for customer {sample_customer}:\")\n",
    "for i, (article_id, score) in enumerate(sample_hybrid_recommendations, 1):\n",
    "    print(f\"{i}. Article {article_id}: Score {score:.4f}\")\n",
    "\n",
    "print(\"\\nHybrid Recommendation System ready\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Models and Preprocessing Objects\n",
    "\n",
    "Save all trained models and preprocessing objects for evaluation and future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create models directory\n",
    "models_dir = Path(\"../models\")\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"Saving models and preprocessing objects...\")\n",
    "\n",
    "# Save all models and objects\n",
    "model_artifacts = {\n",
    "    'svd_model': svd_model,\n",
    "    'customer_encoder': customer_encoder,\n",
    "    'article_encoder': article_encoder,\n",
    "    'customer_factors': customer_factors,\n",
    "    'article_factors': article_factors,\n",
    "    'tfidf_vectorizer': tfidf_vectorizer,\n",
    "    'content_similarity': content_similarity,\n",
    "    'article_to_idx': article_to_idx,\n",
    "    'idx_to_article': idx_to_article,\n",
    "    'article_features': article_features,\n",
    "    'trained_models': trained_models,\n",
    "    'label_encoders': label_encoders,\n",
    "    'scaler': scaler,\n",
    "    'model_scores': model_scores,\n",
    "    'train_interactions': train_interactions,\n",
    "    'all_customers': all_customers,\n",
    "    'all_articles': all_articles\n",
    "}\n",
    "\n",
    "# Save to pickle file\n",
    "with open(models_dir / 'recommendation_models.pkl', 'wb') as f:\n",
    "    pickle.dump(model_artifacts, f)\n",
    "\n",
    "print(f\"Models saved to: {models_dir / 'recommendation_models.pkl'}\")\n",
    "\n",
    "# Save model performance summary\n",
    "performance_summary = pd.DataFrame(model_scores).T\n",
    "performance_summary.to_csv(models_dir / 'model_performance_summary.csv')\n",
    "\n",
    "print(f\"Performance summary saved to: {models_dir / 'model_performance_summary.csv'}\")\n",
    "print(\"\\nModel Performance Summary:\")\n",
    "display(performance_summary[['accuracy', 'precision', 'recall', 'f1_score', 'auc_roc']])\n",
    "\n",
    "print(\"\\n=== Modelling Complete ===\")\n",
    "print(f\"Models trained: {list(trained_models.keys())}\")\n",
    "print(f\"Recommendation approaches: SVD Collaborative Filtering, Content-Based, Hybrid\")\n",
    "print(f\"Ready for evaluation in model_evaluation.ipynb\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}