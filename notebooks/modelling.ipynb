{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# H&M Product Recommendation Modelling\n",
    "\n",
    "This notebook implements multiple recommendation system approaches for predicting customer product preferences and purchase behaviour.\n",
    "\n",
    "## Modelling Approaches\n",
    "\n",
    "1. **Collaborative Filtering** - Matrix factorization and neighborhood-based methods\n",
    "2. **Content-Based Filtering** - Product and customer feature-based recommendations\n",
    "3. **Hybrid Models** - Combining multiple approaches\n",
    "4. **Purchase Prediction** - Binary classification for purchase likelihood\n",
    "\n",
    "## Business Objectives\n",
    "\n",
    "- Predict which products customers are likely to purchase\n",
    "- Recommend relevant products to increase engagement\n",
    "- Identify customer preferences and shopping patterns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('../')\n",
    "\n",
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import H&M data modelling modules\n",
    "from hnm_data_analysis.data_modelling import (\n",
    "    CollaborativeFilteringModel,\n",
    "    ContentBasedFilteringModel,\n",
    "    PurchasePredictionModel,\n",
    "    HybridRecommenderModel\n",
    ")\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Set display options\n",
    "pl.Config.set_tbl_rows(10)\n",
    "pl.Config.set_tbl_cols(15)\n",
    "pd.set_option('display.max_columns', 20)\n",
    "pd.set_option('display.max_rows', 10)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "\n",
    "print(\"Libraries and H&M modelling modules imported successfully\")\n",
    "print(f\"Current working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Training and Test Data\n",
    "\n",
    "Load the preprocessed training and test datasets created in the data preparation phase.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load training and test datasets\n",
    "print(\"Loading training and test datasets...\")\n",
    "\n",
    "train_df = pl.read_parquet(\"../data/modelling_data/train_data.parquet\")\n",
    "test_df = pl.read_parquet(\"../data/modelling_data/test_data.parquet\")\n",
    "\n",
    "print(f\"Training data shape: {train_df.shape}\")\n",
    "print(f\"Test data shape: {test_df.shape}\")\n",
    "print(f\"Total features: {train_df.shape[1]}\")\n",
    "\n",
    "# Check data consistency\n",
    "print(f\"\\nData consistency checks:\")\n",
    "print(f\"Training customers: {train_df['customer_id'].n_unique():,}\")\n",
    "print(f\"Test customers: {test_df['customer_id'].n_unique():,}\")\n",
    "print(f\"Training articles: {train_df['article_id'].n_unique():,}\")\n",
    "print(f\"Test articles: {test_df['article_id'].n_unique():,}\")\n",
    "\n",
    "# Check for customer overlap (should be 0)\n",
    "train_customers = set(train_df['customer_id'].unique())\n",
    "test_customers = set(test_df['customer_id'].unique())\n",
    "overlap = train_customers.intersection(test_customers)\n",
    "print(f\"Customer overlap: {len(overlap)} (should be 0)\")\n",
    "\n",
    "# Display sample data\n",
    "print(f\"\\nSample training data:\")\n",
    "display(train_df.head(3))\n",
    "\n",
    "print(f\"\\nColumn data types:\")\n",
    "for col, dtype in zip(train_df.columns, train_df.dtypes):\n",
    "    print(f\"{col}: {dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation for Recommendation Systems\n",
    "\n",
    "Prepare the data for different recommendation approaches by creating interaction matrices and feature sets.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to pandas for easier manipulation with sklearn\n",
    "print(\"Converting to pandas...\")\n",
    "train_pd = train_df.to_pandas()\n",
    "test_pd = test_df.to_pandas()\n",
    "print(\"Data converted to pandas for modeling\")\n",
    "\n",
    "# OPTIONAL: Use sampling for faster development/testing\n",
    "USE_SAMPLING = False  # Set to False for full dataset\n",
    "SAMPLE_SIZE = 100000  # Sample transactions for faster processing\n",
    "\n",
    "if USE_SAMPLING and len(train_pd) > SAMPLE_SIZE:\n",
    "    print(f\"\\nUsing sample of {SAMPLE_SIZE:,} transactions for faster processing...\")\n",
    "    train_pd = train_pd.sample(n=SAMPLE_SIZE, random_state=42).reset_index(drop=True)\n",
    "    print(f\"Sampled training data shape: {train_pd.shape}\")\n",
    "\n",
    "print(\"Data preparation complete - ready for modelling with H&M modules\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 1: Collaborative Filtering with Matrix Factorization\n",
    "\n",
    "Implement collaborative filtering using SVD (Singular Value Decomposition) for matrix factorization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Model 1: Collaborative Filtering ===\\n\")\n",
    "\n",
    "# Initialize and train collaborative filtering model\n",
    "cf_model = CollaborativeFilteringModel(n_components=50, random_state=42)\n",
    "cf_model.fit(train_pd)\n",
    "\n",
    "# Display model information\n",
    "model_info = cf_model.get_model_info()\n",
    "print(f\"\\nCollaborative Filtering Model Info:\")\n",
    "for key, value in model_info.items():\n",
    "    if key == 'matrix_density':\n",
    "        print(f\"  {key}: {value:.4f}%\")\n",
    "    elif isinstance(value, float):\n",
    "        print(f\"  {key}: {value:.4f}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value:,}\" if isinstance(value, int) else f\"  {key}: {value}\")\n",
    "\n",
    "# Test recommendations for a sample customer\n",
    "sample_customer = train_pd['customer_id'].iloc[0]\n",
    "sample_recommendations = cf_model.get_recommendations(sample_customer, 5)\n",
    "\n",
    "print(f\"\\nSample collaborative filtering recommendations for customer {sample_customer}:\")\n",
    "for i, (article_id, score) in enumerate(sample_recommendations, 1):\n",
    "    print(f\"{i}. Article {article_id}: Score {score:.4f}\")\n",
    "\n",
    "print(\"\\nCollaborative filtering model trained successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2: Content-Based Filtering\n",
    "\n",
    "Implement content-based filtering using product features and customer preferences.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Model 2: Content-Based Filtering ===\\n\")\n",
    "\n",
    "# Initialize and train content-based filtering model\n",
    "cb_model = ContentBasedFilteringModel(\n",
    "    max_features=1000,\n",
    "    ngram_range=(1, 2),\n",
    "    min_df=2\n",
    ")\n",
    "cb_model.fit(train_pd)\n",
    "\n",
    "# Display model information\n",
    "model_info = cb_model.get_model_info()\n",
    "print(f\"\\nContent-Based Filtering Model Info:\")\n",
    "for key, value in model_info.items():\n",
    "    if isinstance(value, int):\n",
    "        print(f\"  {key}: {value:,}\")\n",
    "    elif isinstance(value, tuple):\n",
    "        print(f\"  {key}: {value}\")\n",
    "    else:\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "# Test content-based recommendations\n",
    "sample_content_recommendations = cb_model.get_recommendations(sample_customer, 5)\n",
    "\n",
    "print(f\"\\nSample content-based recommendations for customer {sample_customer}:\")\n",
    "for i, (article_id, score) in enumerate(sample_content_recommendations, 1):\n",
    "    print(f\"{i}. Article {article_id}: Score {score:.4f}\")\n",
    "\n",
    "print(\"\\nContent-based filtering model trained successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 3: Purchase Prediction Classification\n",
    "\n",
    "Build classification models to predict whether a customer will purchase a specific product.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Model 3: Purchase Prediction ===\\n\")\n",
    "\n",
    "# Initialize and train purchase prediction model\n",
    "pp_model = PurchasePredictionModel(test_size=0.2, random_state=42)\n",
    "pp_model.fit(train_pd)\n",
    "\n",
    "# Display model performance\n",
    "model_scores = pp_model.get_model_scores()\n",
    "print(f\"\\nPurchase Prediction Model Performance:\")\n",
    "\n",
    "for model_name, scores in model_scores.items():\n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"  Accuracy:  {scores['accuracy']:.4f}\")\n",
    "    print(f\"  Precision: {scores['precision']:.4f}\")\n",
    "    print(f\"  Recall:    {scores['recall']:.4f}\")\n",
    "    print(f\"  F1-Score:  {scores['f1_score']:.4f}\")\n",
    "    print(f\"  AUC-ROC:   {scores['auc_roc']:.4f}\")\n",
    "\n",
    "# Get best performing model\n",
    "best_model_name = max(model_scores.keys(), key=lambda x: model_scores[x]['f1_score'])\n",
    "print(f\"\\nBest performing model: {best_model_name} (F1-Score: {model_scores[best_model_name]['f1_score']:.4f})\")\n",
    "\n",
    "# Test purchase prediction for sample customer-article pairs\n",
    "print(f\"\\nSample purchase predictions:\")\n",
    "sample_articles = train_pd['article_id'].unique()[:5]\n",
    "for article_id in sample_articles:\n",
    "    prob = pp_model.predict_purchase_probability(sample_customer, article_id)\n",
    "    print(f\"Customer {sample_customer} - Article {article_id}: {prob:.4f}\")\n",
    "\n",
    "print(\"\\nPurchase prediction model trained successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 4: Hybrid Recommendation System\n",
    "\n",
    "Combine collaborative filtering and content-based approaches for improved recommendations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=== Model 4: Hybrid Recommendation System ===\\n\")\n",
    "\n",
    "# Initialize and train hybrid recommender model\n",
    "hybrid_model = HybridRecommenderModel(\n",
    "    cf_weight=0.4,\n",
    "    cb_weight=0.4, \n",
    "    pp_weight=0.2\n",
    ")\n",
    "\n",
    "# Train the hybrid model with all components\n",
    "hybrid_model.fit(\n",
    "    train_pd,\n",
    "    cf_params={'n_components': 50},\n",
    "    cb_params={'max_features': 1000},\n",
    "    pp_params={'test_size': 0.2}\n",
    ")\n",
    "\n",
    "# Display model information\n",
    "model_info = hybrid_model.get_model_info()\n",
    "print(f\"\\nHybrid Recommender Model Info:\")\n",
    "print(f\"  Component Models: {model_info['component_models']}\")\n",
    "print(f\"  Model Weights: CF={model_info['cf_weight']:.1f}, CB={model_info['cb_weight']:.1f}, PP={model_info['pp_weight']:.1f}\")\n",
    "print(f\"  Fitted: {model_info['fitted']}\")\n",
    "\n",
    "# Test hybrid recommendations\n",
    "sample_hybrid_recommendations = hybrid_model.get_recommendations(sample_customer, 5)\n",
    "\n",
    "print(f\"\\nSample hybrid recommendations for customer {sample_customer}:\")\n",
    "for i, (article_id, score) in enumerate(sample_hybrid_recommendations, 1):\n",
    "    print(f\"{i}. Article {article_id}: Score {score:.4f}\")\n",
    "\n",
    "print(\"\\nHybrid recommendation system trained successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Models and Preprocessing Objects\n",
    "\n",
    "Save all trained models and preprocessing objects for evaluation and future use.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create models directory\n",
    "models_dir = Path(\"../models\")\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"Saving trained models...\")\n",
    "\n",
    "# Save individual models\n",
    "print(\"Saving collaborative filtering model...\")\n",
    "cf_model.save_model(models_dir / 'collaborative_filtering_model.pkl')\n",
    "\n",
    "print(\"Saving content-based filtering model...\")\n",
    "cb_model.save_model(models_dir / 'content_based_filtering_model.pkl')\n",
    "\n",
    "print(\"Saving purchase prediction model...\")\n",
    "pp_model.save_model(models_dir / 'purchase_prediction_model.pkl')\n",
    "\n",
    "print(\"Saving hybrid recommender model...\")\n",
    "hybrid_model.save_model(models_dir / 'hybrid_recommender_model.pkl')\n",
    "\n",
    "# Create and save performance summary\n",
    "print(\"\\nCreating performance summary...\")\n",
    "\n",
    "# Combine all model information\n",
    "performance_data = {\n",
    "    'Collaborative Filtering': cf_model.get_model_info(),\n",
    "    'Content-Based Filtering': cb_model.get_model_info(), \n",
    "    'Purchase Prediction': {\n",
    "        'best_model': max(pp_model.get_model_scores().keys(), \n",
    "                         key=lambda x: pp_model.get_model_scores()[x]['f1_score']),\n",
    "        'best_f1_score': max(pp_model.get_model_scores().values(), \n",
    "                           key=lambda x: x['f1_score'])['f1_score']\n",
    "    },\n",
    "    'Hybrid Recommender': hybrid_model.get_model_info()\n",
    "}\n",
    "\n",
    "# Save performance summary as JSON for easier reading\n",
    "import json\n",
    "with open(models_dir / 'model_summary.json', 'w') as f:\n",
    "    # Convert numpy types to native Python types for JSON serialization\n",
    "    json_data = {}\n",
    "    for model_name, info in performance_data.items():\n",
    "        json_data[model_name] = {}\n",
    "        for key, value in info.items():\n",
    "            if isinstance(value, np.integer):\n",
    "                json_data[model_name][key] = int(value)\n",
    "            elif isinstance(value, np.floating):\n",
    "                json_data[model_name][key] = float(value)\n",
    "            else:\n",
    "                json_data[model_name][key] = value\n",
    "    \n",
    "    json.dump(json_data, f, indent=2)\n",
    "\n",
    "print(f\"Model summary saved to: {models_dir / 'model_summary.json'}\")\n",
    "\n",
    "print(\"\\n=== Modelling Complete ===\\n\")\n",
    "print(\"Trained Models:\")\n",
    "print(\"  1. Collaborative Filtering (SVD-based matrix factorization)\")\n",
    "print(\"  2. Content-Based Filtering (TF-IDF product similarity)\")\n",
    "print(\"  3. Purchase Prediction (Multiple classification algorithms)\")\n",
    "print(\"  4. Hybrid Recommender (Combines all approaches)\")\n",
    "\n",
    "print(f\"\\nAll models saved to: {models_dir}/\")\n",
    "print(\"Ready for evaluation and deployment!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
