{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering Experiments with Categorical Features\n",
    "\n",
    "This notebook clusters articles using categorical features from the final dataset, excluding article_id and bert_cluster columns.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure project root is the working directory so relative paths resolve\n",
    "import os\n",
    "if os.path.basename(os.getcwd()) == 'notebooks':\n",
    "    os.chdir('..')\n",
    "print('CWD:', os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import polars as pl\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import joblib\n",
    "from pathlib import Path\n",
    "\n",
    "# Import the ArticleClusterer module\n",
    "from hnm_data_analysis.clustering.article_clustering import ArticleClusterer, ClusteringConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the final dataset\n",
    "data_path = 'data/features/final/articles_features_final.parquet'\n",
    "df = pl.read_parquet(data_path)\n",
    "print(f'Loaded data: {df.shape[0]:,} rows x {df.shape[1]} columns')\n",
    "print(f'Columns: {df.columns}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features for clustering from all articles - exclude the columns that are not features\n",
    "exclude_cols = ['article_id', 'bert_cluster', 'product_type_name', 'perceived_colour_master_name', 'index_name', 'index_group_name', 'detail_desc']\n",
    "#exclude_cols = ['article_id', 'bert_cluster', 'detail_desc']\n",
    "feature_cols = [col for col in df.columns if col not in exclude_cols]\n",
    "print(f'Feature columns ({len(feature_cols)}): {feature_cols}')\n",
    "print(f'Total articles for clustering: {df.shape[0]:,}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to pandas for sklearn compatibility (using all articles)\n",
    "df_features = df.select(feature_cols).to_pandas()\n",
    "article_ids = df.select('article_id').to_pandas()['article_id'].values\n",
    "\n",
    "print(f'Feature matrix shape: {df_features.shape}')\n",
    "print(f'Article IDs: {len(article_ids)}')\n",
    "print(f'\\nFeature data types:')\n",
    "print(df_features.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess categorical features (all columns are categorical now)\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# All feature columns are categorical (the dataset now only contains categorical columns)\n",
    "categorical_cols = feature_cols\n",
    "print(f'Categorical columns ({len(categorical_cols)}): {categorical_cols}')\n",
    "\n",
    "# Create one-hot encoder for categorical features\n",
    "encoder = OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore')\n",
    "\n",
    "# Fit and transform the features\n",
    "X_processed = encoder.fit_transform(df_features)\n",
    "print(f'\\nProcessed feature matrix shape: {X_processed.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get feature names after preprocessing\n",
    "feature_names = encoder.get_feature_names_out(categorical_cols)\n",
    "\n",
    "print(f'Total features after encoding: {len(feature_names)}')\n",
    "print(f'First 10 feature names: {feature_names[:10]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed categorical features for ArticleClusterer\n",
    "# First apply PCA for dimensionality reduction (similar to BERT notebook)\n",
    "n_components = min(30, X_processed.shape[1])  # Use up to 30 components or max available\n",
    "pca = PCA(n_components=n_components, random_state=42)\n",
    "X_pca = pca.fit_transform(X_processed)\n",
    "\n",
    "print(f'PCA reduced features to {X_pca.shape[1]} components')\n",
    "print(f'Explained variance ratio (first 10): {pca.explained_variance_ratio_[:10]}')\n",
    "print(f'Total explained variance: {pca.explained_variance_ratio_.sum():.3f}')\n",
    "\n",
    "# Create categorical features dataset compatible with ArticleClusterer\n",
    "categorical_features_dir = Path('data/features/categorical')\n",
    "categorical_features_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Save PCA features as parquet with article_id and feature columns\n",
    "feature_column_names = [f'pca_{i:03d}' for i in range(X_pca.shape[1])]\n",
    "categorical_features_df = pl.DataFrame({\n",
    "    'article_id': article_ids,\n",
    "    **{name: X_pca[:, i] for i, name in enumerate(feature_column_names)}\n",
    "})\n",
    "\n",
    "categorical_features_path = categorical_features_dir / 'pca_categorical_features.parquet'\n",
    "categorical_features_df.write_parquet(categorical_features_path)\n",
    "print(f'Saved categorical features to: {categorical_features_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ArticleClusterer with categorical features\n",
    "clusterer = ArticleClusterer(\n",
    "    features_path=str(categorical_features_path),\n",
    "    articles_metadata_path='data/features/final/articles_features_final.parquet'\n",
    ")\n",
    "\n",
    "# Load features and metadata\n",
    "features, article_ids_loaded = clusterer.load_features()\n",
    "clusterer.load_articles_metadata()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create output directory\n",
    "out_dir = Path('results/categorical_clustering')\n",
    "out_dir.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "\n",
    "# Find optimal k using different methods with ArticleClusterer\n",
    "k_min, k_max = 2, 50\n",
    "print('Finding optimal k using elbow method...')\n",
    "opt_k_elbow, scores_elbow = clusterer.find_optimal_k(k_range=(k_min, k_max), method='elbow')\n",
    "clusterer.plot_k_selection(scores_elbow, method='elbow', optimal_k=opt_k_elbow,\n",
    "                          save_path=str(out_dir / f'optimal_k_elbow_{k_min}_{k_max}.png'))\n",
    "\n",
    "print('\\nFinding optimal k using silhouette method...')\n",
    "opt_k_sil, scores_sil = clusterer.find_optimal_k(k_range=(k_min, k_max), method='silhouette')\n",
    "clusterer.plot_k_selection(scores_sil, method='silhouette', optimal_k=opt_k_sil,\n",
    "                          save_path=str(out_dir / f'optimal_k_silhouette_{k_min}_{k_max}.png'))\n",
    "\n",
    "print(f'\\nOptimal k recommendations:')\n",
    "print(f'Elbow: {opt_k_elbow}')\n",
    "print(f'Silhouette: {opt_k_sil}')\n",
    "\n",
    "# Use silhouette as primary recommendation\n",
    "recommended_k = opt_k_sil\n",
    "print(f'\\nUsing k={recommended_k} (silhouette method)')\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the manual calculation of additional metrics since ArticleClusterer handles this\n",
    "# Perform final clustering with ArticleClusterer\n",
    "recommended_k = 50\n",
    "print(f'Performing final clustering with k={recommended_k}')\n",
    "config = ClusteringConfig(algorithm='kmeans', n_clusters=recommended_k)\n",
    "results = clusterer.cluster(config)\n",
    "\n",
    "print(f'Final clustering results:')\n",
    "print(f'Number of clusters: {results.n_clusters}')\n",
    "print(f'Silhouette Score: {results.silhouette:.4f}')\n",
    "print(f'Calinski-Harabasz Index: {results.calinski_harabasz:.4f}')\n",
    "print(f'Davies-Bouldin Index: {results.davies_bouldin:.4f}')\n",
    "\n",
    "# Show cluster distribution\n",
    "unique, counts = np.unique(results.labels, return_counts=True)\n",
    "print(f'\\nCluster distribution:')\n",
    "for cluster_id, count in zip(unique, counts):\n",
    "    print(f'Cluster {cluster_id}: {count} items ({count/len(results.labels)*100:.1f}%)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove duplicate clustering - ArticleClusterer already performed it in previous cell\n",
    "# Just reference the existing results\n",
    "cluster_labels = results.labels\n",
    "recommended_k = results.n_clusters\n",
    "silhouette = results.silhouette\n",
    "calinski_harabasz = results.calinski_harabasz\n",
    "davies_bouldin = results.davies_bouldin\n",
    "\n",
    "print(f'Using clustering results from ArticleClusterer:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use ArticleClusterer's built-in visualization methods\n",
    "clusterer.visualise_clusters(method='pca', save_path=str(out_dir / 'clusters_pca.png'))\n",
    "clusterer.visualise_clusters(method='tsne', save_path=str(out_dir / 'clusters_tsne.png'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use ArticleClusterer's interpretation functionality\n",
    "cluster_interpretations = clusterer.interpret_clusters()\n",
    "\n",
    "# Display sample articles from different clusters\n",
    "print('Sample cluster interpretations:')\n",
    "for cluster_id in list(cluster_interpretations.keys())[:3]:  # Show first 3 clusters\n",
    "    interpretation = cluster_interpretations[cluster_id]\n",
    "    print(f'\\nCluster {cluster_id}:')\n",
    "    print(f'  Size: {interpretation[\"size\"]} articles ({interpretation[\"percentage\"]:.1f}%)')\n",
    "    \n",
    "    # Show top categories for this cluster\n",
    "    for key, values in interpretation.items():\n",
    "        if key.startswith('top_') and isinstance(values, dict):\n",
    "            category_name = key.replace('top_', '').replace('_', ' ').title()\n",
    "            print(f'  {category_name}: {dict(list(values.items())[:3])}')  # Show top 3 values\n",
    "\n",
    "# Also display sample articles using the original function but with clusterer's data\n",
    "def display_cluster_samples(clusterer_obj, labels, n_clusters_sample=3, n_articles_per_cluster=3):\n",
    "    unique_clusters = np.unique(labels)\n",
    "    sampled_clusters = np.random.choice(unique_clusters, \n",
    "                                      size=min(n_clusters_sample, len(unique_clusters)), \n",
    "                                      replace=False)\n",
    "    \n",
    "    print(f'\\nDetailed samples from clusters: {sampled_clusters}')\n",
    "    \n",
    "    for cluster_id in sampled_clusters:\n",
    "        cluster_indices = np.where(labels == cluster_id)[0]\n",
    "        if len(cluster_indices) == 0:\n",
    "            continue\n",
    "            \n",
    "        sample_indices = np.random.choice(cluster_indices, \n",
    "                                        size=min(n_articles_per_cluster, len(cluster_indices)), \n",
    "                                        replace=False)\n",
    "        sampled_article_ids = [clusterer_obj.article_ids[i] for i in sample_indices]\n",
    "        \n",
    "        print(f'\\nCluster {cluster_id} — {len(cluster_indices)} items; showing {len(sampled_article_ids)} samples')\n",
    "        \n",
    "        # Display sample articles from this cluster\n",
    "        if clusterer_obj.articles_metadata is not None:\n",
    "            sample_df = clusterer_obj.articles_metadata.filter(pl.col('article_id').is_in(sampled_article_ids))\n",
    "            display_cols = ['article_id', 'product_group_name', 'product_type_name', \n",
    "                           'department_name', 'garment_group_name', 'colour_group_name',\n",
    "                           'graphical_appearance_name', 'detail_desc', 'bert_cluster']\n",
    "            \n",
    "            # Only select columns that exist in the dataframe\n",
    "            existing_cols = [col for col in display_cols if col in sample_df.columns]\n",
    "            print(sample_df.select(existing_cols).to_pandas())\n",
    "\n",
    "# Display sample clusters\n",
    "display_cluster_samples(clusterer, cluster_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use ArticleClusterer's save functionality\n",
    "clusterer.save_results(str(out_dir))\n",
    "\n",
    "# Also save preprocessing artifacts for reproducibility\n",
    "joblib.dump(encoder, out_dir / 'encoder.joblib')\n",
    "print(f'Saved encoder to: {out_dir / \"encoder.joblib\"}')\n",
    "\n",
    "joblib.dump(pca, out_dir / 'pca_model.joblib')\n",
    "print(f'Saved PCA model to: {out_dir / \"pca_model.joblib\"}')\n",
    "\n",
    "# Save additional metadata about the categorical preprocessing\n",
    "import json\n",
    "preprocessing_metadata = {\n",
    "    'preprocessing_method': 'one_hot_encoding_plus_pca',\n",
    "    'n_features_original': int(X_processed.shape[1]),\n",
    "    'n_features_pca': int(X_pca.shape[1]),\n",
    "    'n_articles': int(len(article_ids)),\n",
    "    'explained_variance_ratio': float(pca.explained_variance_ratio_.sum()),\n",
    "    'feature_columns_used': feature_cols,\n",
    "    'categorical_columns': categorical_cols,\n",
    "    'categorical_features_file': str(categorical_features_path)\n",
    "}\n",
    "\n",
    "with open(out_dir / 'preprocessing_metadata.json', 'w') as f:\n",
    "    json.dump(preprocessing_metadata, f, indent=2)\n",
    "print(f'Saved preprocessing metadata to: {out_dir / \"preprocessing_metadata.json\"}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create final dataset with both BERT and categorical clusters\n",
    "print('Creating final dataset with categorical cluster labels...')\n",
    "\n",
    "# Get categorical cluster labels from ArticleClusterer results\n",
    "cat_labels_pl = pl.DataFrame({\n",
    "    'article_id': clusterer.article_ids,\n",
    "    'categorical_cluster': results.labels\n",
    "})\n",
    "\n",
    "# Join with original dataset (now includes ALL articles with categorical clusters)\n",
    "final_dataset = df.join(cat_labels_pl, on='article_id', how='left')\n",
    "\n",
    "print(f'Final dataset shape: {final_dataset.shape}')\n",
    "print(f'Articles with categorical clusters: {final_dataset.filter(pl.col(\"categorical_cluster\").is_not_null()).shape[0]:,}')\n",
    "print(f'Articles with BERT clusters: {final_dataset.filter(pl.col(\"bert_cluster\").is_not_null()).shape[0]:,}')\n",
    "\n",
    "# Show overlap between clustering methods\n",
    "both_clusters = final_dataset.filter(\n",
    "    (pl.col(\"categorical_cluster\").is_not_null()) & \n",
    "    (pl.col(\"bert_cluster\").is_not_null())\n",
    ").shape[0]\n",
    "print(f'Articles with both cluster types: {both_clusters:,}')\n",
    "\n",
    "# Save the enhanced dataset\n",
    "output_path = Path('data/features/final/articles_features_with_clusters.parquet')\n",
    "final_dataset.write_parquet(output_path)\n",
    "print(f'Saved enhanced dataset to: {output_path}')\n",
    "\n",
    "print(f'\\nDataset summary:')\n",
    "print('BERT clusters:', final_dataset.select('bert_cluster').filter(pl.col('bert_cluster').is_not_null()).shape[0])\n",
    "print('Categorical clusters:', final_dataset.select('categorical_cluster').filter(pl.col('categorical_cluster').is_not_null()).shape[0])\n",
    "print('\\nCluster value ranges:')\n",
    "print('BERT cluster range:', final_dataset.select(pl.col('bert_cluster').min().alias('min'), pl.col('bert_cluster').max().alias('max')))\n",
    "print('Categorical cluster range:', final_dataset.select(pl.col('categorical_cluster').min().alias('min'), pl.col('categorical_cluster').max().alias('max')))\n",
    "\n",
    "# Generate data report for the final enhanced dataset\n",
    "from hnm_data_analysis.data_understanding.data_report_generator import generate_data_report\n",
    "print(f'\\nGenerating data report for enhanced dataset...')\n",
    "report_path = generate_data_report(str(output_path))\n",
    "print(f'Data report saved to: {report_path}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hnm_data_analysis.data_understanding.data_report_generator import generate_data_report\n",
    "# Generate data report for data/features/final/articles_features_with_clusters.parquet\n",
    "print(generate_data_report(\"data/features/final/articles_features_with_clusters.parquet\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
