{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVD-Only Clustering Analysis\n",
    "\n",
    "This notebook performs clustering analysis on H&M articles using **only** SVD-reduced TF-IDF vectors (excluding categorical features). It utilises the `ArticleClusterer` module to:\n",
    "\n",
    "1. Load SVD features from the combined article features file\n",
    "2. Perform K-means clustering with optimal k selection\n",
    "3. Visualise clusters using PCA and t-SNE\n",
    "4. Interpret cluster characteristics\n",
    "\n",
    "## Setup and Imports\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import warnings\n",
    "sys.path.append('../')  # Add project root to path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import polars as pl\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Import our clustering module\n",
    "from hnm_data_analysis.clustering.article_clustering import (\n",
    "    ArticleClusterer, \n",
    "    ClusteringConfig, \n",
    "    ClusteringResults\n",
    ")\n",
    "\n",
    "# Configure plotting\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "%matplotlib inline\n",
    "\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "Load only the SVD-reduced TF-IDF vectors from the combined article features file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file paths\n",
    "features_path = r\"C:\\Users\\tom\\coding_projects\\data_analytics_projects\\h_and_m_data_analysis\\data\\features\\combined_article_features.parquet\"\n",
    "articles_metadata_path = r\"C:\\Users\\tom\\coding_projects\\data_analytics_projects\\h_and_m_data_analysis\\data\\processed\\articles_last_3_months.parquet\"\n",
    "\n",
    "# Check if files exist\n",
    "print(f\"Combined features file exists: {os.path.exists(features_path)}\")\n",
    "print(f\"Articles metadata file exists: {os.path.exists(articles_metadata_path)}\")\n",
    "\n",
    "print(\"\\nNote: Using SVD features only (categorical features excluded)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and filter to SVD features only\n",
    "print(\"Loading and filtering to SVD features only...\")\n",
    "features_df = pl.read_parquet(features_path)\n",
    "\n",
    "# Identify SVD features\n",
    "svd_cols = [c for c in features_df.columns if c.startswith('svd_')]\n",
    "categorical_cols = [c for c in features_df.columns if not c.startswith('svd_') and c != 'article_id']\n",
    "\n",
    "print(f\"Total columns in file: {len(features_df.columns)}\")\n",
    "print(f\"SVD features: {len(svd_cols)} (using these for clustering)\")\n",
    "print(f\"Categorical features: {len(categorical_cols)} (excluded from clustering)\")\n",
    "\n",
    "# Create SVD-only dataset\n",
    "svd_features_df = features_df.select(['article_id'] + sorted(svd_cols))\n",
    "print(f\"\\nSVD-only dataset shape: {svd_features_df.shape}\")\n",
    "\n",
    "# Save temporary SVD-only file\n",
    "temp_svd_path = \"../data/temp/svd_only_features.parquet\"\n",
    "os.makedirs(\"../data/temp\", exist_ok=True)\n",
    "svd_features_df.write_parquet(temp_svd_path)\n",
    "print(f\"Saved SVD-only features to: {temp_svd_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialise the clusterer with SVD-only features\n",
    "clusterer = ArticleClusterer(\n",
    "    features_path=temp_svd_path,\n",
    "    articles_metadata_path=articles_metadata_path\n",
    ")\n",
    "\n",
    "print(\"Article Clusterer initialised with SVD-only features!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the SVD feature data\n",
    "features, article_ids = clusterer.load_features()\n",
    "print(f\"\\nLoaded features shape: {features.shape}\")\n",
    "print(f\"Number of articles: {len(article_ids):,}\")\n",
    "print(f\"SVD dimensions: {features.shape[1]:,}\")\n",
    "\n",
    "# Display basic feature statistics\n",
    "print(f\"\\nSVD Feature Statistics:\")\n",
    "print(f\"Mean: {features.mean():.4f}\")\n",
    "print(f\"Std: {features.std():.4f}\")\n",
    "print(f\"Min: {features.min():.4f}\")\n",
    "print(f\"Max: {features.max():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load articles metadata for cluster interpretation\n",
    "metadata = clusterer.load_articles_metadata()\n",
    "\n",
    "if metadata is not None:\n",
    "    print(f\"\\nMetadata shape: {metadata.shape}\")\n",
    "    print(\"Available columns for cluster interpretation:\")\n",
    "    for col in metadata.columns:\n",
    "        print(f\"  - {col}\")\n",
    "else:\n",
    "    print(\"\\nMetadata not available for cluster interpretation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD Feature Analysis\n",
    "\n",
    "Examine the SVD-reduced TF-IDF vectors to understand their characteristics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse SVD feature distribution\n",
    "print(f\"SVD Features Analysis:\")\n",
    "print(f\"Total SVD components: {len(svd_cols)}\")\n",
    "print(f\"SVD component range: {min(svd_cols)} to {max(svd_cols)}\")\n",
    "\n",
    "# Plot SVD feature variance\n",
    "feature_variances = np.var(features, axis=0)\n",
    "plt.figure(figsize=(12, 5))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(feature_variances)\n",
    "plt.title('SVD Component Variances')\n",
    "plt.xlabel('SVD Component')\n",
    "plt.ylabel('Variance')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.hist(features.flatten(), bins=50, alpha=0.7, density=True)\n",
    "plt.title('Distribution of SVD Feature Values')\n",
    "plt.xlabel('Feature Value')\n",
    "plt.ylabel('Density')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nTop 10 SVD components by variance:\")\n",
    "top_variance_indices = np.argsort(feature_variances)[-10:][::-1]\n",
    "for i, idx in enumerate(top_variance_indices):\n",
    "    print(f\"  {i+1}. SVD component {idx}: variance = {feature_variances[idx]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimal K Selection\n",
    "\n",
    "Find the optimal number of clusters using both elbow method and silhouette analysis on SVD features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find optimal k using elbow method\n",
    "print(\"Finding optimal k using elbow method...\")\n",
    "optimal_k_elbow, elbow_scores = clusterer.find_optimal_k(\n",
    "    k_range=(2, 15), \n",
    "    algorithm=\"kmeans\", \n",
    "    method=\"elbow\"\n",
    ")\n",
    "\n",
    "print(f\"\\nOptimal k (elbow method): {optimal_k_elbow}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find optimal k using silhouette analysis\n",
    "print(\"Finding optimal k using silhouette analysis...\")\n",
    "optimal_k_silhouette, silhouette_scores = clusterer.find_optimal_k(\n",
    "    k_range=(2, 15), \n",
    "    algorithm=\"kmeans\", \n",
    "    method=\"silhouette\"\n",
    ")\n",
    "\n",
    "print(f\"\\nOptimal k (silhouette method): {optimal_k_silhouette}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot k selection curves for SVD features\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Elbow plot\n",
    "k_values = sorted(elbow_scores.keys())\n",
    "inertia_values = [elbow_scores[k] for k in k_values]\n",
    "ax1.plot(k_values, inertia_values, 'bo-', linewidth=2, markersize=8)\n",
    "ax1.axvline(x=optimal_k_elbow, color='red', linestyle='--', \n",
    "           label=f'Optimal k = {optimal_k_elbow}')\n",
    "ax1.set_xlabel('Number of Clusters (k)')\n",
    "ax1.set_ylabel('Inertia')\n",
    "ax1.set_title('Elbow Method for Optimal k (SVD Features Only)')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Silhouette plot\n",
    "silhouette_values = [silhouette_scores[k] for k in k_values]\n",
    "ax2.plot(k_values, silhouette_values, 'go-', linewidth=2, markersize=8)\n",
    "ax2.axvline(x=optimal_k_silhouette, color='red', linestyle='--', \n",
    "           label=f'Optimal k = {optimal_k_silhouette}')\n",
    "ax2.set_xlabel('Number of Clusters (k)')\n",
    "ax2.set_ylabel('Silhouette Score')\n",
    "ax2.set_title('Silhouette Analysis for Optimal k (SVD Features Only)')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Choose final k (prefer silhouette method)\n",
    "final_k = optimal_k_silhouette\n",
    "print(f\"\\nSelected k = {final_k} for final SVD-based clustering\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means Clustering on SVD Features\n",
    "\n",
    "Perform K-means clustering using only the SVD-reduced TF-IDF vectors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure clustering\n",
    "config = ClusteringConfig(\n",
    "    algorithm=\"kmeans\",\n",
    "    n_clusters=final_k,\n",
    "    random_state=42,\n",
    "    kmeans_n_init=20  # More initialisations for better results\n",
    ")\n",
    "\n",
    "# Perform clustering\n",
    "print(f\"Performing K-means clustering on SVD features with k = {final_k}...\")\n",
    "results = clusterer.cluster(config)\n",
    "\n",
    "print(f\"\\nSVD-Based Clustering Results:\")\n",
    "print(f\"Algorithm: {results.algorithm}\")\n",
    "print(f\"Number of clusters: {results.n_clusters}\")\n",
    "print(f\"Feature dimensions: {results.feature_shape[1]} (SVD components only)\")\n",
    "print(f\"Silhouette score: {results.silhouette:.4f}\")\n",
    "print(f\"Calinski-Harabasz index: {results.calinski_harabasz:.2f}\")\n",
    "print(f\"Davies-Bouldin index: {results.davies_bouldin:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse cluster sizes\n",
    "unique_labels, counts = np.unique(results.labels, return_counts=True)\n",
    "cluster_sizes = dict(zip(unique_labels, counts))\n",
    "\n",
    "print(\"Cluster size distribution (SVD-based):\")\n",
    "for cluster_id in sorted(cluster_sizes.keys()):\n",
    "    size = cluster_sizes[cluster_id]\n",
    "    percentage = (size / len(results.labels)) * 100\n",
    "    print(f\"Cluster {cluster_id}: {size:,} articles ({percentage:.1f}%)\")\n",
    "\n",
    "# Plot cluster size distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "clusters = sorted(cluster_sizes.keys())\n",
    "sizes = [cluster_sizes[c] for c in clusters]\n",
    "bars = plt.bar(clusters, sizes, alpha=0.7)\n",
    "plt.xlabel('Cluster ID')\n",
    "plt.ylabel('Number of Articles')\n",
    "plt.title('Cluster Size Distribution (SVD Features Only)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, size in zip(bars, sizes):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + max(sizes)*0.01, \n",
    "             f'{size:,}', ha='center', va='bottom')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster Visualization\n",
    "\n",
    "Visualise the SVD-based clusters using both PCA and t-SNE dimensionality reduction.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA visualization of SVD-based clusters\n",
    "print(\"Creating PCA visualisation of SVD-based clusters...\")\n",
    "clusterer.visualise_clusters(method=\"pca\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t-SNE visualization (this may take a while for large datasets)\n",
    "print(\"Creating t-SNE visualisation of SVD-based clusters...\")\n",
    "print(\"Note: t-SNE may take several minutes for large datasets\")\n",
    "clusterer.visualise_clusters(method=\"tsne\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cluster Interpretation\n",
    "\n",
    "Analyse SVD-based cluster characteristics using the article metadata.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interpret SVD-based clusters\n",
    "print(\"Interpreting SVD-based clusters...\")\n",
    "cluster_summaries = clusterer.interpret_clusters(top_features=10)\n",
    "\n",
    "# Display cluster summaries\n",
    "for cluster_id in sorted(cluster_summaries.keys()):\n",
    "    summary = cluster_summaries[cluster_id]\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"SVD-BASED CLUSTER {cluster_id}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Size: {summary['size']:,} articles ({summary['percentage']:.1f}%)\")\n",
    "    \n",
    "    # Show top categories for each cluster\n",
    "    categorical_fields = [\n",
    "        'top_product_group_name', 'top_product_type_name', \n",
    "        'top_colour_group_name', 'top_department_name', 'top_garment_group_name'\n",
    "    ]\n",
    "    \n",
    "    for field in categorical_fields:\n",
    "        if field in summary and summary[field]:\n",
    "            field_name = field.replace('top_', '').replace('_', ' ').title()\n",
    "            print(f\"\\n{field_name}:\")\n",
    "            for category, count in list(summary[field].items())[:3]:  # Top 3\n",
    "                print(f\"  {category}: {count} articles\")\n",
    "    \n",
    "    # Show sample article IDs\n",
    "    print(f\"\\nSample Article IDs: {summary['article_ids'][:5]}\")\n",
    "    \n",
    "    print(f\"\\nNote: Clusters based purely on text similarity (SVD of TF-IDF vectors)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SVD Feature Importance Analysis\n",
    "\n",
    "Analyse which SVD components are most important for distinguishing between clusters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse SVD feature importance in clusters\n",
    "if hasattr(clusterer.clustering_model, 'cluster_centers_'):\n",
    "    cluster_centers = clusterer.clustering_model.cluster_centers_\n",
    "    \n",
    "    # Calculate feature importance as the range across cluster centers\n",
    "    feature_importance = np.ptp(cluster_centers, axis=0)  # peak-to-peak (max - min)\n",
    "    \n",
    "    # Get top important SVD components\n",
    "    top_features_idx = np.argsort(feature_importance)[-20:][::-1]  # Top 20\n",
    "    \n",
    "    print(f\"Top 20 Most Important SVD Components for Clustering:\")\n",
    "    for i, idx in enumerate(top_features_idx):\n",
    "        print(f\"  {i+1:2d}. SVD component {idx:3d}: importance = {feature_importance[idx]:.4f}\")\n",
    "    \n",
    "    # Plot heatmap of cluster centers for top features\n",
    "    plt.figure(figsize=(15, 8))\n",
    "    \n",
    "    # Use top 30 features for visualization\n",
    "    top_30_idx = np.argsort(feature_importance)[-30:][::-1]\n",
    "    centers_important = cluster_centers[:, top_30_idx]\n",
    "    feature_labels = [f'SVD_{i:03d}' for i in top_30_idx]\n",
    "    \n",
    "    sns.heatmap(centers_important, \n",
    "                xticklabels=feature_labels,\n",
    "                yticklabels=[f'Cluster {i}' for i in range(cluster_centers.shape[0])],\n",
    "                cmap='RdBu_r', center=0, cbar_kws={'label': 'SVD Component Value'})\n",
    "    plt.title('Cluster Centers: Top 30 Most Important SVD Components')\n",
    "    plt.xlabel('SVD Components')\n",
    "    plt.ylabel('Clusters')\n",
    "    plt.xticks(rotation=45)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\nCluster centers shape: {cluster_centers.shape}\")\n",
    "    print(f\"All {cluster_centers.shape[1]} SVD components used in clustering\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate inter-cluster distances in SVD space\n",
    "if hasattr(clusterer.clustering_model, 'cluster_centers_'):\n",
    "    from scipy.spatial.distance import pdist, squareform\n",
    "    \n",
    "    # Calculate pairwise distances between cluster centers in SVD space\n",
    "    distances = pdist(cluster_centers, metric='euclidean')\n",
    "    distance_matrix = squareform(distances)\n",
    "    \n",
    "    # Plot distance matrix\n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(distance_matrix,\n",
    "                xticklabels=[f'Cluster {i}' for i in range(len(cluster_centers))],\n",
    "                yticklabels=[f'Cluster {i}' for i in range(len(cluster_centers))],\n",
    "                annot=True, fmt='.2f', cmap='viridis',\n",
    "                cbar_kws={'label': 'Euclidean Distance in SVD Space'})\n",
    "    plt.title('Inter-Cluster Distance Matrix (SVD Space)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Find most similar and most different cluster pairs\n",
    "    np.fill_diagonal(distance_matrix, np.inf)  # Ignore diagonal\n",
    "    min_dist_idx = np.unravel_index(np.argmin(distance_matrix), distance_matrix.shape)\n",
    "    max_dist_idx = np.unravel_index(np.argmax(distance_matrix), distance_matrix.shape)\n",
    "    \n",
    "    print(f\"\\nMost similar clusters in SVD space: {min_dist_idx[0]} and {min_dist_idx[1]} (distance: {distance_matrix[min_dist_idx]:.2f})\")\n",
    "    print(f\"Most different clusters in SVD space: {max_dist_idx[0]} and {max_dist_idx[1]} (distance: {distance_matrix[max_dist_idx]:.2f})\")\n",
    "    print(f\"\\nNote: Distances computed in {cluster_centers.shape[1]}-dimensional SVD space\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Insights\n",
    "\n",
    "Key findings from the SVD-based clustering analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary statistics for SVD-based clustering\n",
    "print(\"SVD-BASED CLUSTERING ANALYSIS SUMMARY\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Dataset: {len(article_ids):,} articles\")\n",
    "print(f\"Features: {features.shape[1]:,} SVD components (TF-IDF reduced)\")\n",
    "print(f\"Algorithm: K-means with k={final_k}\")\n",
    "print(f\"Clustering basis: Text similarity only (no categorical features)\")\n",
    "print(f\"\\nQuality Metrics:\")\n",
    "print(f\"  Silhouette Score: {results.silhouette:.4f} (higher is better, range: -1 to 1)\")\n",
    "print(f\"  Calinski-Harabasz Index: {results.calinski_harabasz:.2f} (higher is better)\")\n",
    "print(f\"  Davies-Bouldin Index: {results.davies_bouldin:.4f} (lower is better)\")\n",
    "\n",
    "print(f\"\\nCluster Size Statistics:\")\n",
    "sizes = [cluster_sizes[c] for c in sorted(cluster_sizes.keys())]\n",
    "print(f\"  Largest cluster: {max(sizes):,} articles ({max(sizes)/len(article_ids)*100:.1f}%)\")\n",
    "print(f\"  Smallest cluster: {min(sizes):,} articles ({min(sizes)/len(article_ids)*100:.1f}%)\")\n",
    "print(f\"  Average cluster size: {np.mean(sizes):.0f} articles\")\n",
    "print(f\"  Cluster size std: {np.std(sizes):.0f} articles\")\n",
    "\n",
    "# Assess cluster balance\n",
    "size_cv = np.std(sizes) / np.mean(sizes)  # Coefficient of variation\n",
    "if size_cv < 0.5:\n",
    "    balance_assessment = \"Well-balanced\"\n",
    "elif size_cv < 1.0:\n",
    "    balance_assessment = \"Moderately balanced\"\n",
    "else:\n",
    "    balance_assessment = \"Imbalanced\"\n",
    "\n",
    "print(f\"  Cluster balance: {balance_assessment} (CV: {size_cv:.2f})\")\n",
    "\n",
    "print(f\"\\nKey Insights:\")\n",
    "print(f\"  • Clusters represent articles with similar text descriptions\")\n",
    "print(f\"  • SVD dimensionality reduction preserves semantic relationships\")\n",
    "print(f\"  • Product categorisation patterns emerge from text similarity alone\")\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results\n",
    "\n",
    "Save the SVD-based clustering results for further analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create results directory\n",
    "output_dir = \"../results/clustering/svd_only_clustering\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Save clustering results\n",
    "print(\"Saving SVD-based clustering results...\")\n",
    "clusterer.save_results(output_dir)\n",
    "\n",
    "print(f\"\\nResults saved to: {output_dir}\")\n",
    "print(\"Files created:\")\n",
    "for file in os.listdir(output_dir):\n",
    "    print(f\"  - {file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a summary DataFrame for easy analysis\n",
    "cluster_labels_df = pl.DataFrame({\n",
    "    \"article_id\": article_ids,\n",
    "    \"cluster_label\": results.labels,\n",
    "    \"clustering_method\": [\"SVD_only\"] * len(article_ids)\n",
    "})\n",
    "\n",
    "# Save as both CSV and Parquet\n",
    "cluster_labels_df.write_csv(os.path.join(output_dir, \"svd_article_clusters.csv\"))\n",
    "cluster_labels_df.write_parquet(os.path.join(output_dir, \"svd_article_clusters.parquet\"))\n",
    "\n",
    "print(\"\\nSVD-based cluster labels saved as CSV and Parquet files\")\n",
    "print(f\"Shape: {cluster_labels_df.shape}\")\n",
    "print(cluster_labels_df.head())\n",
    "\n",
    "# Clean up temporary file\n",
    "if os.path.exists(temp_svd_path):\n",
    "    os.remove(temp_svd_path)\n",
    "    print(f\"\\nCleaned up temporary file: {temp_svd_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
